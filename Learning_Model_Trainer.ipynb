{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier)\n",
    "from sklearn import (metrics, cross_validation, linear_model, preprocessing)\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def cv_loop_acc(X, y, model, N):\n",
    "    mean_acc = 0.0\n",
    "    for i in range(0,N,1):\n",
    "        X_train, X_cv, y_train, y_cv = cross_validation.train_test_split(\n",
    "                                       X, y, test_size=1.0/float(N), \n",
    "                                       random_state = i*SEED)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_cv)\n",
    "        acc = metrics.accuracy_score(y_cv, preds)\n",
    "        #print acc\n",
    "        mean_acc += acc\n",
    "    return mean_acc/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def cv_loop_auc(X, y, model, N):\n",
    "    mean_auc = 0.0\n",
    "    for i in range(0,N,1):\n",
    "        X_train, X_cv, y_train, y_cv = cross_validation.train_test_split(\n",
    "                                       X, y, test_size=1.0/float(N), \n",
    "                                       random_state = i*SEED)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_cv)[:,1]\n",
    "        auc = metrics.roc_auc_score(y_cv, preds)\n",
    "        #print auc\n",
    "        mean_auc += auc\n",
    "    return mean_auc/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def latest_day_loop_acc(X_train,y_train,X_test,y_test,model):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, preds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_csv(day_trade):\n",
    "    data_up = []\n",
    "    data_down = []\n",
    "    path = '/home/bigdatas16/SGX-OrderBook-Tick-Data-Trading-Strategy-/Train_Test_Data/ML_data_2014'\n",
    "    for j,i in enumerate(day_trade):\n",
    "        for k in range(0,len(i),1):\n",
    "            path_up = path + '_' + str(j+1) + '_' + str(i[k]) + '_' + 'UP' + '.csv'\n",
    "            path_down = path + '_' + str(j+1) + '_' + str(i[k]) + '_' + 'DOWN' + '.csv'\n",
    "            data_up.append(pd.read_csv(path_up))\n",
    "            data_down.append(pd.read_csv(path_down))\n",
    "            #print path_down\n",
    "    return data_up,data_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_trade = [[2,3,6,7,8,10,13,14,15,16,17,20,21,22,23,24,27,28,29,30],\\\n",
    "             [7,10,11,12,13,17,18,19,21,24,25,26,27],\\\n",
    "             [3,4,5,6,7,10,11,13,14,17,18,19,20,24,25,26,27,31]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_2014_up, data_2014_down = read_csv(day_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.697888888889\n",
      "0.649666666667\n",
      "0.646666666667\n",
      "0.487333333333\n",
      "0.650555555556\n",
      "0.517222222222\n",
      "0.547333333333\n",
      "0.555444444444\n",
      "0.525777777778\n",
      "0.473444444444\n",
      "0.544888888889\n",
      "0.505111111111\n",
      "0.541111111111\n",
      "0.434111111111\n",
      "0.538111111111\n",
      "0.428666666667\n",
      "0.503777777778\n",
      "0.486222222222\n",
      "0.550222222222\n",
      "0.511666666667\n",
      "0.571222222222\n",
      "0.511444444444\n",
      "0.372555555556\n",
      "0.467777777778\n",
      "0.524888888889\n",
      "0.617777777778\n",
      "0.504\n",
      "0.570777777778\n",
      "0.446222222222\n",
      "0.543888888889\n",
      "0.5\n",
      "0.471222222222\n",
      "0.500555555556\n",
      "0.601222222222\n",
      "0.630777777778\n",
      "0.445666666667\n",
      "0.584444444444\n",
      "0.482333333333\n",
      "0.503555555556\n",
      "0.431\n",
      "0.486222222222\n",
      "0.660666666667\n",
      "0.520111111111\n",
      "0.523111111111\n",
      "0.479555555556\n",
      "0.537777777778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.52791304347826085, 0.068121882942348999)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_five_day = []\n",
    "for i in range(1,len(day_trade[0])+len(day_trade[1])+len(day_trade[2])-4,1):\n",
    "    data_train = pd.concat([data_2014_up[i-1],data_2014_up[i],data_2014_up[i+1],\\\n",
    "                           data_2014_up[i+2]],axis = 0).reset_index(drop=True)\n",
    "    data_train = data_train\n",
    "    X_train = data_train.drop([\"0\"],axis=1)\n",
    "    y_train = data_train['0']\n",
    "    data_test = pd.concat([data_2014_up[i+3]],axis = 0).reset_index(drop=True)\n",
    "    data_test = data_test\n",
    "    X_test = data_test.drop([\"0\"],axis=1)\n",
    "    y_test = data_test['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    acc = latest_day_loop_acc(X_train,y_train,X_test,y_test,model)\n",
    "    print acc\n",
    "    mean_five_day.append(acc)\n",
    "np.mean(mean_five_day),np.std(mean_five_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.53753427895981087, 0.067884145649198563)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_four_day = []\n",
    "for i in range(1,len(day_trade[0])+len(day_trade[1])+len(day_trade[2])-3,1):\n",
    "    data_train = pd.concat([data_2014_up[i-1],data_2014_up[i],data_2014_up[i+1]],\\\n",
    "                           axis = 0).reset_index(drop=True)\n",
    "    data_train = data_train\n",
    "    X_train = data_train.drop([\"0\"],axis=1)\n",
    "    y_train = data_train['0']\n",
    "    data_test = pd.concat([data_2014_up[i+2]],axis = 0).reset_index(drop=True)\n",
    "    data_test = data_test\n",
    "    X_test = data_test.drop([\"0\"],axis=1)\n",
    "    y_test = data_test['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    acc = latest_day_loop_acc(X_train,y_train,X_test,y_test,model)\n",
    "    print acc\n",
    "    mean_four_day.append(acc)\n",
    "np.mean(mean_four_day),np.std(mean_four_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.660111111111\n",
      "0.59\n",
      "0.707222222222\n",
      "0.594333333333\n",
      "0.644111111111\n",
      "0.467111111111\n",
      "0.628555555556\n",
      "0.506333333333\n",
      "0.552777777778\n",
      "0.499111111111\n",
      "0.564888888889\n",
      "0.508888888889\n",
      "0.468333333333\n",
      "0.513444444444\n",
      "0.515444444444\n",
      "0.409333333333\n",
      "0.479444444444\n",
      "0.525222222222\n",
      "0.558333333333\n",
      "0.489444444444\n",
      "0.579444444444\n",
      "0.556222222222\n",
      "0.560444444444\n",
      "0.498333333333\n",
      "0.472111111111\n",
      "0.304\n",
      "0.562444444444\n",
      "0.525222222222\n",
      "0.528222222222\n",
      "0.528666666667\n",
      "0.586222222222\n",
      "0.596222222222\n",
      "0.500666666667\n",
      "0.569888888889\n",
      "0.514777777778\n",
      "0.562555555556\n",
      "0.529333333333\n",
      "0.443777777778\n",
      "0.594333333333\n",
      "0.574888888889\n",
      "0.503666666667\n",
      "0.549555555556\n",
      "0.576222222222\n",
      "0.492888888889\n",
      "0.518222222222\n",
      "0.531777777778\n",
      "0.551\n",
      "0.547444444444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.53627083333333336, 0.064448474914871962)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_three_day = []\n",
    "for i in range(1,len(day_trade[0])+len(day_trade[1])+len(day_trade[2])-2,1):\n",
    "    data_train = pd.concat([data_2014_up[i-1],data_2014_up[i]],\\\n",
    "                           axis = 0).reset_index(drop=True)\n",
    "    data_train = data_train\n",
    "    X_train = data_train.drop([\"0\"],axis=1)\n",
    "    y_train = data_train['0']\n",
    "    data_test = pd.concat([data_2014_up[i+1]],axis = 0).reset_index(drop=True)\n",
    "    data_test = data_test\n",
    "    X_test = data_test.drop([\"0\"],axis=1)\n",
    "    y_test = data_test['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    acc = latest_day_loop_acc(X_train,y_train,X_test,y_test,model)\n",
    "    print acc\n",
    "    mean_three_day.append(acc)\n",
    "np.mean(mean_three_day),np.std(mean_three_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.649111111111\n",
      "0.613888888889\n",
      "0.580888888889\n",
      "0.632222222222\n",
      "0.595444444444\n",
      "0.652333333333\n",
      "0.466\n",
      "0.609222222222\n",
      "0.587111111111\n",
      "0.560222222222\n",
      "0.508\n",
      "0.561444444444\n",
      "0.481\n",
      "0.546333333333\n",
      "0.432888888889\n",
      "0.546666666667\n",
      "0.463555555556\n",
      "0.482333333333\n",
      "0.452777777778\n",
      "0.527222222222\n",
      "0.463666666667\n",
      "0.744888888889\n",
      "0.550888888889\n",
      "0.482333333333\n",
      "0.461111111111\n",
      "0.570444444444\n",
      "0.502222222222\n",
      "0.456333333333\n",
      "0.629222222222\n",
      "0.5\n",
      "0.400333333333\n",
      "0.598111111111\n",
      "0.551333333333\n",
      "0.539666666667\n",
      "0.531666666667\n",
      "0.481111111111\n",
      "0.574111111111\n",
      "0.508777777778\n",
      "0.432333333333\n",
      "0.576\n",
      "0.452\n",
      "0.513888888889\n",
      "0.541\n",
      "0.522555555556\n",
      "0.523111111111\n",
      "0.536111111111\n",
      "0.561888888889\n",
      "0.498111111111\n",
      "0.578111111111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.53530612244897957, 0.066883338491784516)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_two_day = []\n",
    "for i in range(1,len(day_trade[0])+len(day_trade[1])+len(day_trade[2])-1,1):\n",
    "    data_train = pd.concat([data_2014_up[i-1]],axis = 0).reset_index(drop=True)\n",
    "    data_train = data_train\n",
    "    X_train = data_train.drop([\"0\"],axis=1)\n",
    "    y_train = data_train['0']\n",
    "    data_test = pd.concat([data_2014_up[i]],axis = 0).reset_index(drop=True)\n",
    "    data_test = data_test\n",
    "    X_test = data_test.drop([\"0\"],axis=1)\n",
    "    y_test = data_test['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    acc = latest_day_loop_acc(X_train,y_train,X_test,y_test,model)\n",
    "    print acc\n",
    "    mean_two_day.append(acc)\n",
    "np.mean(mean_two_day),np.std(mean_two_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170    1.0\n",
       "171    1.0\n",
       "172    1.0\n",
       "173    1.0\n",
       "174    1.0\n",
       "175    1.0\n",
       "176    1.0\n",
       "177    1.0\n",
       "178    1.0\n",
       "179    1.0\n",
       "180    1.0\n",
       "181    1.0\n",
       "182    1.0\n",
       "183    1.0\n",
       "184    1.0\n",
       "185    1.0\n",
       "186    1.0\n",
       "187    1.0\n",
       "188    1.0\n",
       "189    1.0\n",
       "190    1.0\n",
       "191    1.0\n",
       "192    1.0\n",
       "193    1.0\n",
       "194    1.0\n",
       "195    1.0\n",
       "196    1.0\n",
       "197    1.0\n",
       "198    1.0\n",
       "199    1.0\n",
       "      ... \n",
       "440    1.0\n",
       "441    1.0\n",
       "442    1.0\n",
       "443    1.0\n",
       "444    1.0\n",
       "445    1.0\n",
       "446    1.0\n",
       "447    1.0\n",
       "448    1.0\n",
       "449    1.0\n",
       "450    1.0\n",
       "451    1.0\n",
       "452    1.0\n",
       "453    1.0\n",
       "454    1.0\n",
       "455    1.0\n",
       "456    1.0\n",
       "457    1.0\n",
       "458    1.0\n",
       "459    1.0\n",
       "460    1.0\n",
       "461    1.0\n",
       "462    1.0\n",
       "463    1.0\n",
       "464    1.0\n",
       "465    1.0\n",
       "466    1.0\n",
       "467    1.0\n",
       "468    1.0\n",
       "469    1.0\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1.0\n",
      "10\n",
      "1.0\n",
      "20\n",
      "1.0\n",
      "30\n",
      "1.0\n",
      "40\n",
      "1.0\n",
      "50\n",
      "1.0\n",
      "60\n",
      "1.0\n",
      "70\n",
      "1.0\n",
      "80\n",
      "1.0\n",
      "90\n",
      "1.0\n",
      "100\n",
      "1.0\n",
      "110\n",
      "1.0\n",
      "120\n",
      "1.0\n",
      "130\n",
      "1.0\n",
      "140\n",
      "1.0\n",
      "150\n",
      "1.0\n",
      "160\n",
      "1.0\n",
      "170\n",
      "1.0\n",
      "180\n",
      "1.0\n",
      "190\n",
      "1.0\n",
      "200\n",
      "1.0\n",
      "210\n",
      "1.0\n",
      "220\n",
      "1.0\n",
      "230\n",
      "1.0\n",
      "240\n",
      "1.0\n",
      "250\n",
      "1.0\n",
      "260\n",
      "1.0\n",
      "270\n",
      "1.0\n",
      "280\n",
      "1.0\n",
      "290\n",
      "1.0\n",
      "300\n",
      "1.0\n",
      "310\n",
      "1.0\n",
      "320\n",
      "1.0\n",
      "330\n",
      "1.0\n",
      "340\n",
      "1.0\n",
      "350\n",
      "1.0\n",
      "360\n",
      "1.0\n",
      "370\n",
      "1.0\n",
      "380\n",
      "1.0\n",
      "390\n",
      "1.0\n",
      "400\n",
      "1.0\n",
      "410\n",
      "0.6\n",
      "420\n",
      "1.0\n",
      "430\n",
      "1.0\n",
      "440\n",
      "1.0\n",
      "450\n",
      "1.0\n",
      "460\n",
      "0.0\n",
      "470\n",
      "0.0\n",
      "480\n",
      "0.4\n",
      "490\n",
      "1.0\n",
      "500\n",
      "1.0\n",
      "510\n",
      "1.0\n",
      "520\n",
      "1.0\n",
      "530\n",
      "1.0\n",
      "540\n",
      "0.0\n",
      "550\n",
      "0.2\n",
      "560\n",
      "0.8\n",
      "570\n",
      "0.0\n",
      "580\n",
      "1.0\n",
      "590\n",
      "1.0\n",
      "600\n",
      "1.0\n",
      "610\n",
      "0.4\n",
      "620\n",
      "0.4\n",
      "630\n",
      "1.0\n",
      "640\n",
      "1.0\n",
      "650\n",
      "1.0\n",
      "660\n",
      "1.0\n",
      "670\n",
      "1.0\n",
      "680\n",
      "1.0\n",
      "690\n",
      "1.0\n",
      "700\n",
      "1.0\n",
      "710\n",
      "1.0\n",
      "720\n",
      "1.0\n",
      "730\n",
      "1.0\n",
      "740\n",
      "1.0\n",
      "750\n",
      "1.0\n",
      "760\n",
      "1.0\n",
      "770\n",
      "1.0\n",
      "780\n",
      "1.0\n",
      "790\n",
      "1.0\n",
      "800\n",
      "1.0\n",
      "810\n",
      "1.0\n",
      "820\n",
      "1.0\n",
      "830\n",
      "1.0\n",
      "840\n",
      "1.0\n",
      "850\n",
      "1.0\n",
      "860\n",
      "1.0\n",
      "870\n",
      "1.0\n",
      "880\n",
      "1.0\n",
      "890\n",
      "1.0\n",
      "900\n",
      "1.0\n",
      "910\n",
      "1.0\n",
      "920\n",
      "1.0\n",
      "930\n",
      "1.0\n",
      "940\n",
      "1.0\n",
      "950\n",
      "1.0\n",
      "960\n",
      "0.2\n",
      "970\n",
      "0.0\n",
      "980\n",
      "0.2\n",
      "990\n",
      "0.0\n",
      "1000\n",
      "0.0\n",
      "1010\n",
      "0.0\n",
      "1020\n",
      "0.0\n",
      "1030\n",
      "0.0\n",
      "1040\n",
      "0.0\n",
      "1050\n",
      "0.4\n",
      "1060\n",
      "0.0\n",
      "1070\n",
      "0.0\n",
      "1080\n",
      "0.0\n",
      "1090\n",
      "0.4\n",
      "1100\n",
      "1.0\n",
      "1110\n",
      "1.0\n",
      "1120\n",
      "0.0\n",
      "1130\n",
      "0.0\n",
      "1140\n",
      "0.0\n",
      "1150\n",
      "0.0\n",
      "1160\n",
      "0.0\n",
      "1170\n",
      "0.0\n",
      "1180\n",
      "0.0\n",
      "1190\n",
      "0.0\n",
      "1200\n",
      "0.0\n",
      "1210\n",
      "0.0\n",
      "1220\n",
      "0.0\n",
      "1230\n",
      "0.2\n",
      "1240\n",
      "0.8\n",
      "1250\n",
      "0.0\n",
      "1260\n",
      "0.8\n",
      "1270\n",
      "1.0\n",
      "1280\n",
      "1.0\n",
      "1290\n",
      "1.0\n",
      "1300\n",
      "1.0\n",
      "1310\n",
      "1.0\n",
      "1320\n",
      "1.0\n",
      "1330\n",
      "1.0\n",
      "1340\n",
      "1.0\n",
      "1350\n",
      "1.0\n",
      "1360\n",
      "1.0\n",
      "1370\n",
      "1.0\n",
      "1380\n",
      "0.2\n",
      "1390\n",
      "1.0\n",
      "1400\n",
      "0.0\n",
      "1410\n",
      "1.0\n",
      "1420\n",
      "1.0\n",
      "1430\n",
      "1.0\n",
      "1440\n",
      "0.6\n",
      "1450\n",
      "1.0\n",
      "1460\n",
      "0.4\n",
      "1470\n",
      "1.0\n",
      "1480\n",
      "0.8\n",
      "1490\n",
      "0.4\n",
      "1500\n",
      "0.6\n",
      "1510\n",
      "1.0\n",
      "1520\n",
      "0.6\n",
      "1530\n",
      "1.0\n",
      "1540\n",
      "1.0\n",
      "1550\n",
      "1.0\n",
      "1560\n",
      "1.0\n",
      "1570\n",
      "1.0\n",
      "1580\n",
      "1.0\n",
      "1590\n",
      "1.0\n",
      "1600\n",
      "1.0\n",
      "1610\n",
      "1.0\n",
      "1620\n",
      "1.0\n",
      "1630\n",
      "0.8\n",
      "1640\n",
      "0.0\n",
      "1650\n",
      "1.0\n",
      "1660\n",
      "1.0\n",
      "1670\n",
      "1.0\n",
      "1680\n",
      "1.0\n",
      "1690\n",
      "0.0\n",
      "1700\n",
      "0.6\n",
      "1710\n",
      "1.0\n",
      "1720\n",
      "1.0\n",
      "1730\n",
      "1.0\n",
      "1740\n",
      "1.0\n",
      "1750\n",
      "1.0\n",
      "1760\n",
      "1.0\n",
      "1770\n",
      "1.0\n",
      "1780\n",
      "1.0\n",
      "1790\n",
      "1.0\n",
      "1800\n",
      "1.0\n",
      "1810\n",
      "1.0\n",
      "1820\n",
      "1.0\n",
      "1830\n",
      "1.0\n",
      "1840\n",
      "1.0\n",
      "1850\n",
      "1.0\n",
      "1860\n",
      "1.0\n",
      "1870\n",
      "1.0\n",
      "1880\n",
      "1.0\n",
      "1890\n",
      "1.0\n",
      "1900\n",
      "1.0\n",
      "1910\n",
      "1.0\n",
      "1920\n",
      "1.0\n",
      "1930\n",
      "1.0\n",
      "1940\n",
      "1.0\n",
      "1950\n",
      "1.0\n",
      "1960\n",
      "1.0\n",
      "1970\n",
      "1.0\n",
      "1980\n",
      "1.0\n",
      "1990\n",
      "1.0\n",
      "2000\n",
      "1.0\n",
      "2010\n",
      "1.0\n",
      "2020\n",
      "1.0\n",
      "2030\n",
      "1.0\n",
      "2040\n",
      "1.0\n",
      "2050\n",
      "1.0\n",
      "2060\n",
      "1.0\n",
      "2070\n",
      "1.0\n",
      "2080\n",
      "1.0\n",
      "2090\n",
      "1.0\n",
      "2100\n",
      "1.0\n",
      "2110\n",
      "1.0\n",
      "2120\n",
      "1.0\n",
      "2130\n",
      "1.0\n",
      "2140\n",
      "1.0\n",
      "2150\n",
      "1.0\n",
      "2160\n",
      "1.0\n",
      "2170\n",
      "1.0\n",
      "2180\n",
      "1.0\n",
      "2190\n",
      "1.0\n",
      "2200\n",
      "1.0\n",
      "2210\n",
      "1.0\n",
      "2220\n",
      "1.0\n",
      "2230\n",
      "1.0\n",
      "2240\n",
      "1.0\n",
      "2250\n",
      "1.0\n",
      "2260\n",
      "1.0\n",
      "2270\n",
      "1.0\n",
      "2280\n",
      "1.0\n",
      "2290\n",
      "1.0\n",
      "2300\n",
      "1.0\n",
      "2310\n",
      "1.0\n",
      "2320\n",
      "1.0\n",
      "2330\n",
      "1.0\n",
      "2340\n",
      "1.0\n",
      "2350\n",
      "1.0\n",
      "2360\n",
      "1.0\n",
      "2370\n",
      "1.0\n",
      "2380\n",
      "1.0\n",
      "2390\n",
      "1.0\n",
      "2400\n",
      "1.0\n",
      "2410\n",
      "0.6\n",
      "2420\n",
      "1.0\n",
      "2430\n",
      "0.8\n",
      "2440\n",
      "0.8\n",
      "2450\n",
      "1.0\n",
      "2460\n",
      "1.0\n",
      "2470\n",
      "0.6\n",
      "2480\n",
      "1.0\n",
      "2490\n",
      "1.0\n",
      "2500\n",
      "1.0\n",
      "2510\n",
      "1.0\n",
      "2520\n",
      "1.0\n",
      "2530\n",
      "0.4\n",
      "2540\n",
      "1.0\n",
      "2550\n",
      "1.0\n",
      "2560\n",
      "0.2\n",
      "2570\n",
      "1.0\n",
      "2580\n",
      "1.0\n",
      "2590\n",
      "1.0\n",
      "2600\n",
      "1.0\n",
      "2610\n",
      "0.0\n",
      "2620\n",
      "0.4\n",
      "2630\n",
      "1.0\n",
      "2640\n",
      "0.8\n",
      "2650\n",
      "0.0\n",
      "2660\n",
      "0.0\n",
      "2670\n",
      "1.0\n",
      "2680\n",
      "0.6\n",
      "2690\n",
      "1.0\n",
      "2700\n",
      "1.0\n",
      "2710\n",
      "1.0\n",
      "2720\n",
      "1.0\n",
      "2730\n",
      "0.6\n",
      "2740\n",
      "1.0\n",
      "2750\n",
      "1.0\n",
      "2760\n",
      "1.0\n",
      "2770\n",
      "1.0\n",
      "2780\n",
      "1.0\n",
      "2790\n",
      "1.0\n",
      "2800\n",
      "1.0\n",
      "2810\n",
      "1.0\n",
      "2820\n",
      "1.0\n",
      "2830\n",
      "1.0\n",
      "2840\n",
      "1.0\n",
      "2850\n",
      "1.0\n",
      "2860\n",
      "1.0\n",
      "2870\n",
      "0.0\n",
      "2880\n",
      "0.4\n",
      "2890\n",
      "1.0\n",
      "2900\n",
      "0.2\n",
      "2910\n",
      "0.2\n",
      "2920\n",
      "0.0\n",
      "2930\n",
      "0.0\n",
      "2940\n",
      "0.0\n",
      "2950\n",
      "0.0\n",
      "2960\n",
      "0.0\n",
      "2970\n",
      "0.6\n",
      "2980\n",
      "0.0\n",
      "2990\n",
      "0.0\n",
      "3000\n",
      "0.0\n",
      "3010\n",
      "0.2\n",
      "3020\n",
      "0.0\n",
      "3030\n",
      "0.6\n",
      "3040\n",
      "1.0\n",
      "3050\n",
      "1.0\n",
      "3060\n",
      "0.4\n",
      "3070\n",
      "0.0\n",
      "3080\n",
      "0.0\n",
      "3090\n",
      "0.0\n",
      "3100\n",
      "0.0\n",
      "3110\n",
      "0.0\n",
      "3120\n",
      "0.4\n",
      "3130\n",
      "0.0\n",
      "3140\n",
      "0.0\n",
      "3150\n",
      "0.0\n",
      "3160\n",
      "0.0\n",
      "3170\n",
      "0.0\n",
      "3180\n",
      "0.0\n",
      "3190\n",
      "0.8\n",
      "3200\n",
      "0.2\n",
      "3210\n",
      "1.0\n",
      "3220\n",
      "1.0\n",
      "3230\n",
      "1.0\n",
      "3240\n",
      "1.0\n",
      "3250\n",
      "0.0\n",
      "3260\n",
      "0.0\n",
      "3270\n",
      "0.0\n",
      "3280\n",
      "0.0\n",
      "3290\n",
      "0.0\n",
      "3300\n",
      "0.0\n",
      "3310\n",
      "0.0\n",
      "3320\n",
      "0.0\n",
      "3330\n",
      "0.0\n",
      "3340\n",
      "0.0\n",
      "3350\n",
      "0.0\n",
      "3360\n",
      "0.0\n",
      "3370\n",
      "0.0\n",
      "3380\n",
      "0.0\n",
      "3390\n",
      "0.0\n",
      "3400\n",
      "0.0\n",
      "3410\n",
      "0.0\n",
      "3420\n",
      "0.0\n",
      "3430\n",
      "0.0\n",
      "3440\n",
      "0.0\n",
      "3450\n",
      "0.0\n",
      "3460\n",
      "0.0\n",
      "3470\n",
      "0.0\n",
      "3480\n",
      "0.0\n",
      "3490\n",
      "0.0\n",
      "3500\n",
      "0.2\n",
      "3510\n",
      "0.0\n",
      "3520\n",
      "1.0\n",
      "3530\n",
      "1.0\n",
      "3540\n",
      "1.0\n",
      "3550\n",
      "0.2\n",
      "3560\n",
      "1.0\n",
      "3570\n",
      "1.0\n",
      "3580\n",
      "1.0\n",
      "3590\n",
      "1.0\n",
      "3600\n",
      "1.0\n",
      "3610\n",
      "1.0\n",
      "3620\n",
      "1.0\n",
      "3630\n",
      "1.0\n",
      "3640\n",
      "1.0\n",
      "3650\n",
      "1.0\n",
      "3660\n",
      "1.0\n",
      "3670\n",
      "1.0\n",
      "3680\n",
      "1.0\n",
      "3690\n",
      "1.0\n",
      "3700\n",
      "1.0\n",
      "3710\n",
      "1.0\n",
      "3720\n",
      "1.0\n",
      "3730\n",
      "1.0\n",
      "3740\n",
      "1.0\n",
      "3750\n",
      "1.0\n",
      "3760\n",
      "1.0\n",
      "3770\n",
      "1.0\n",
      "3780\n",
      "1.0\n",
      "3790\n",
      "1.0\n",
      "3800\n",
      "1.0\n",
      "3810\n",
      "1.0\n",
      "3820\n",
      "1.0\n",
      "3830\n",
      "1.0\n",
      "3840\n",
      "1.0\n",
      "3850\n",
      "1.0\n",
      "3860\n",
      "1.0\n",
      "3870\n",
      "1.0\n",
      "3880\n",
      "1.0\n",
      "3890\n",
      "1.0\n",
      "3900\n",
      "1.0\n",
      "3910\n",
      "1.0\n",
      "3920\n",
      "1.0\n",
      "3930\n",
      "1.0\n",
      "3940\n",
      "1.0\n",
      "3950\n",
      "1.0\n",
      "3960\n",
      "1.0\n",
      "3970\n",
      "1.0\n",
      "3980\n",
      "1.0\n",
      "3990\n",
      "1.0\n",
      "4000\n",
      "1.0\n",
      "4010\n",
      "1.0\n",
      "4020\n",
      "1.0\n",
      "4030\n",
      "1.0\n",
      "4040\n",
      "1.0\n",
      "4050\n",
      "1.0\n",
      "4060\n",
      "1.0\n",
      "4070\n",
      "1.0\n",
      "4080\n",
      "1.0\n",
      "4090\n",
      "1.0\n",
      "4100\n",
      "1.0\n",
      "4110\n",
      "1.0\n",
      "4120\n",
      "1.0\n",
      "4130\n",
      "1.0\n",
      "4140\n",
      "1.0\n",
      "4150\n",
      "1.0\n",
      "4160\n",
      "1.0\n",
      "4170\n",
      "1.0\n",
      "4180\n",
      "1.0\n",
      "4190\n",
      "1.0\n",
      "4200\n",
      "1.0\n",
      "4210\n",
      "1.0\n",
      "4220\n",
      "1.0\n",
      "4230\n",
      "1.0\n",
      "4240\n",
      "1.0\n",
      "4250\n",
      "1.0\n",
      "4260\n",
      "1.0\n",
      "4270\n",
      "1.0\n",
      "4280\n",
      "1.0\n",
      "4290\n",
      "1.0\n",
      "4300\n",
      "1.0\n",
      "4310\n",
      "1.0\n",
      "4320\n",
      "1.0\n",
      "4330\n",
      "1.0\n",
      "4340\n",
      "1.0\n",
      "4350\n",
      "1.0\n",
      "4360\n",
      "1.0\n",
      "4370\n",
      "1.0\n",
      "4380\n",
      "1.0\n",
      "4390\n",
      "1.0\n",
      "4400\n",
      "1.0\n",
      "4410\n",
      "1.0\n",
      "4420\n",
      "1.0\n",
      "4430\n",
      "1.0\n",
      "4440\n",
      "1.0\n",
      "4450\n",
      "1.0\n",
      "4460\n",
      "1.0\n",
      "4470\n",
      "1.0\n",
      "4480\n",
      "1.0\n",
      "4490\n",
      "1.0\n",
      "4500\n",
      "1.0\n",
      "4510\n",
      "1.0\n",
      "4520\n",
      "1.0\n",
      "4530\n",
      "1.0\n",
      "4540\n",
      "1.0\n",
      "4550\n",
      "1.0\n",
      "4560\n",
      "1.0\n",
      "4570\n",
      "1.0\n",
      "4580\n",
      "0.2\n",
      "4590\n",
      "0.6\n",
      "4600\n",
      "1.0\n",
      "4610\n",
      "1.0\n",
      "4620\n",
      "0.6\n",
      "4630\n",
      "0.4\n",
      "4640\n",
      "0.8\n",
      "4650\n",
      "0.8\n",
      "4660\n",
      "1.0\n",
      "4670\n",
      "1.0\n",
      "4680\n",
      "0.4\n",
      "4690\n",
      "0.6\n",
      "4700\n",
      "0.0\n",
      "4710\n",
      "1.0\n",
      "4720\n",
      "1.0\n",
      "4730\n",
      "1.0\n",
      "4740\n",
      "1.0\n",
      "4750\n",
      "1.0\n",
      "4760\n",
      "1.0\n",
      "4770\n",
      "1.0\n",
      "4780\n",
      "1.0\n",
      "4790\n",
      "1.0\n",
      "4800\n",
      "1.0\n",
      "4810\n",
      "1.0\n",
      "4820\n",
      "1.0\n",
      "4830\n",
      "1.0\n",
      "4840\n",
      "1.0\n",
      "4850\n",
      "1.0\n",
      "4860\n",
      "0.2\n",
      "4870\n",
      "0.0\n",
      "4880\n",
      "1.0\n",
      "4890\n",
      "0.8\n",
      "4900\n",
      "1.0\n",
      "4910\n",
      "1.0\n",
      "4920\n",
      "1.0\n",
      "4930\n",
      "0.8\n",
      "4940\n",
      "1.0\n",
      "4950\n",
      "1.0\n",
      "4960\n",
      "0.2\n",
      "4970\n",
      "0.2\n",
      "4980\n",
      "0.6\n",
      "4990\n",
      "1.0\n",
      "5000\n",
      "1.0\n",
      "5010\n",
      "1.0\n",
      "5020\n",
      "1.0\n",
      "5030\n",
      "1.0\n",
      "5040\n",
      "1.0\n",
      "5050\n",
      "0.6\n",
      "5060\n",
      "1.0\n",
      "5070\n",
      "1.0\n",
      "5080\n",
      "0.0\n",
      "5090\n",
      "1.0\n",
      "5100\n",
      "1.0\n",
      "5110\n",
      "0.0\n",
      "5120\n",
      "0.0\n",
      "5130\n",
      "0.6\n",
      "5140\n",
      "1.0\n",
      "5150\n",
      "1.0\n",
      "5160\n",
      "1.0\n",
      "5170\n",
      "1.0\n",
      "5180\n",
      "1.0\n",
      "5190\n",
      "1.0\n",
      "5200\n",
      "1.0\n",
      "5210\n",
      "1.0\n",
      "5220\n",
      "1.0\n",
      "5230\n",
      "1.0\n",
      "5240\n",
      "1.0\n",
      "5250\n",
      "1.0\n",
      "5260\n",
      "1.0\n",
      "5270\n",
      "1.0\n",
      "5280\n",
      "1.0\n",
      "5290\n",
      "1.0\n",
      "5300\n",
      "1.0\n",
      "5310\n",
      "1.0\n",
      "5320\n",
      "1.0\n",
      "5330\n",
      "1.0\n",
      "5340\n",
      "1.0\n",
      "5350\n",
      "1.0\n",
      "5360\n",
      "1.0\n",
      "5370\n",
      "1.0\n",
      "5380\n",
      "1.0\n",
      "5390\n",
      "1.0\n",
      "5400\n",
      "1.0\n",
      "5410\n",
      "1.0\n",
      "5420\n",
      "1.0\n",
      "5430\n",
      "1.0\n",
      "5440\n",
      "1.0\n",
      "5450\n",
      "1.0\n",
      "5460\n",
      "1.0\n",
      "5470\n",
      "1.0\n",
      "5480\n",
      "1.0\n",
      "5490\n",
      "1.0\n",
      "5500\n",
      "1.0\n",
      "5510\n",
      "1.0\n",
      "5520\n",
      "1.0\n",
      "5530\n",
      "1.0\n",
      "5540\n",
      "1.0\n",
      "5550\n",
      "1.0\n",
      "5560\n",
      "1.0\n",
      "5570\n",
      "1.0\n",
      "5580\n",
      "1.0\n",
      "5590\n",
      "1.0\n",
      "5600\n",
      "1.0\n",
      "5610\n",
      "1.0\n",
      "5620\n",
      "1.0\n",
      "5630\n",
      "1.0\n",
      "5640\n",
      "1.0\n",
      "5650\n",
      "1.0\n",
      "5660\n",
      "1.0\n",
      "5670\n",
      "1.0\n",
      "5680\n",
      "1.0\n",
      "5690\n",
      "1.0\n",
      "5700\n",
      "1.0\n",
      "5710\n",
      "0.0\n",
      "5720\n",
      "0.0\n",
      "5730\n",
      "0.0\n",
      "5740\n",
      "0.0\n",
      "5750\n",
      "0.0\n",
      "5760\n",
      "0.0\n",
      "5770\n",
      "0.0\n",
      "5780\n",
      "0.0\n",
      "5790\n",
      "0.0\n",
      "5800\n",
      "0.4\n",
      "5810\n",
      "0.0\n",
      "5820\n",
      "0.0\n",
      "5830\n",
      "0.0\n",
      "5840\n",
      "0.0\n",
      "5850\n",
      "0.0\n",
      "5860\n",
      "0.0\n",
      "5870\n",
      "0.0\n",
      "5880\n",
      "1.0\n",
      "5890\n",
      "1.0\n",
      "5900\n",
      "1.0\n",
      "5910\n",
      "1.0\n",
      "5920\n",
      "1.0\n",
      "5930\n",
      "1.0\n",
      "5940\n",
      "1.0\n",
      "5950\n",
      "0.8\n",
      "5960\n",
      "1.0\n",
      "5970\n",
      "1.0\n",
      "5980\n",
      "1.0\n",
      "5990\n",
      "1.0\n",
      "6000\n",
      "1.0\n",
      "6010\n",
      "1.0\n",
      "6020\n",
      "1.0\n",
      "6030\n",
      "1.0\n",
      "6040\n",
      "1.0\n",
      "6050\n",
      "1.0\n",
      "6060\n",
      "1.0\n",
      "6070\n",
      "1.0\n",
      "6080\n",
      "1.0\n",
      "6090\n",
      "1.0\n",
      "6100\n",
      "1.0\n",
      "6110\n",
      "1.0\n",
      "6120\n",
      "1.0\n",
      "6130\n",
      "1.0\n",
      "6140\n",
      "0.2\n",
      "6150\n",
      "0.0\n",
      "6160\n",
      "0.0\n",
      "6170\n",
      "0.0\n",
      "6180\n",
      "0.4\n",
      "6190\n",
      "1.0\n",
      "6200\n",
      "0.0\n",
      "6210\n",
      "0.2\n",
      "6220\n",
      "0.6\n",
      "6230\n",
      "0.0\n",
      "6240\n",
      "0.0\n",
      "6250\n",
      "0.6\n",
      "6260\n",
      "1.0\n",
      "6270\n",
      "1.0\n",
      "6280\n",
      "1.0\n",
      "6290\n",
      "1.0\n",
      "6300\n",
      "1.0\n",
      "6310\n",
      "1.0\n",
      "6320\n",
      "1.0\n",
      "6330\n",
      "1.0\n",
      "6340\n",
      "1.0\n",
      "6350\n",
      "1.0\n",
      "6360\n",
      "1.0\n",
      "6370\n",
      "1.0\n",
      "6380\n",
      "1.0\n",
      "6390\n",
      "1.0\n",
      "6400\n",
      "0.8\n",
      "6410\n",
      "1.0\n",
      "6420\n",
      "1.0\n",
      "6430\n",
      "1.0\n",
      "6440\n",
      "1.0\n",
      "6450\n",
      "1.0\n",
      "6460\n",
      "1.0\n",
      "6470\n",
      "1.0\n",
      "6480\n",
      "1.0\n",
      "6490\n",
      "0.6\n",
      "6500\n",
      "1.0\n",
      "6510\n",
      "1.0\n",
      "6520\n",
      "1.0\n",
      "6530\n",
      "0.8\n",
      "6540\n",
      "1.0\n",
      "6550\n",
      "1.0\n",
      "6560\n",
      "0.4\n",
      "6570\n",
      "1.0\n",
      "6580\n",
      "1.0\n",
      "6590\n",
      "1.0\n",
      "6600\n",
      "1.0\n",
      "6610\n",
      "1.0\n",
      "6620\n",
      "1.0\n",
      "6630\n",
      "1.0\n",
      "6640\n",
      "1.0\n",
      "6650\n",
      "1.0\n",
      "6660\n",
      "0.2\n",
      "6670\n",
      "1.0\n",
      "6680\n",
      "0.0\n",
      "6690\n",
      "1.0\n",
      "6700\n",
      "0.0\n",
      "6710\n",
      "1.0\n",
      "6720\n",
      "1.0\n",
      "6730\n",
      "1.0\n",
      "6740\n",
      "1.0\n",
      "6750\n",
      "1.0\n",
      "6760\n",
      "1.0\n",
      "6770\n",
      "1.0\n",
      "6780\n",
      "1.0\n",
      "6790\n",
      "1.0\n",
      "6800\n",
      "1.0\n",
      "6810\n",
      "1.0\n",
      "6820\n",
      "1.0\n",
      "6830\n",
      "1.0\n",
      "6840\n",
      "1.0\n",
      "6850\n",
      "1.0\n",
      "6860\n",
      "0.8\n",
      "6870\n",
      "0.0\n",
      "6880\n",
      "1.0\n",
      "6890\n",
      "0.0\n",
      "6900\n",
      "0.6\n",
      "6910\n",
      "1.0\n",
      "6920\n",
      "1.0\n",
      "6930\n",
      "1.0\n",
      "6940\n",
      "1.0\n",
      "6950\n",
      "1.0\n",
      "6960\n",
      "1.0\n",
      "6970\n",
      "1.0\n",
      "6980\n",
      "1.0\n",
      "6990\n",
      "1.0\n",
      "7000\n",
      "1.0\n",
      "7010\n",
      "1.0\n",
      "7020\n",
      "1.0\n",
      "7030\n",
      "1.0\n",
      "7040\n",
      "1.0\n",
      "7050\n",
      "1.0\n",
      "7060\n",
      "1.0\n",
      "7070\n",
      "1.0\n",
      "7080\n",
      "1.0\n",
      "7090\n",
      "1.0\n",
      "7100\n",
      "1.0\n",
      "7110\n",
      "1.0\n",
      "7120\n",
      "1.0\n",
      "7130\n",
      "1.0\n",
      "7140\n",
      "1.0\n",
      "7150\n",
      "1.0\n",
      "7160\n",
      "1.0\n",
      "7170\n",
      "1.0\n",
      "7180\n",
      "1.0\n",
      "7190\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.79555555555555568, 0.37680266309247368)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_min_day = []\n",
    "i = 0\n",
    "for i in range(0,7200,10):\n",
    "    print i\n",
    "    data_train = data_2014_up[0][i:i+1800]\n",
    "    X_train = data_train.drop([\"0\"],axis=1)\n",
    "    y_train = data_train['0']\n",
    "    data_test = data_2014_up[0][i+1800:i+1800+5]\n",
    "    X_test = data_test.drop([\"0\"],axis=1)\n",
    "    y_test = data_test['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    acc = latest_day_loop_acc(X_train,y_train,X_test,y_test,model)\n",
    "    print acc\n",
    "    mean_min_day.append(acc)\n",
    "np.mean(mean_min_day),np.std(mean_min_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.4\n",
      "10\n",
      "0.5\n",
      "20\n",
      "0.5\n",
      "30\n",
      "1.0\n",
      "40\n",
      "1.0\n",
      "50\n",
      "0.9\n",
      "60\n",
      "1.0\n",
      "70\n",
      "0.5\n",
      "80\n",
      "0.9\n",
      "90\n",
      "1.0\n",
      "100\n",
      "1.0\n",
      "110\n",
      "0.5\n",
      "120\n",
      "0.8\n",
      "130\n",
      "1.0\n",
      "140\n",
      "1.0\n",
      "150\n",
      "1.0\n",
      "160\n",
      "1.0\n",
      "170\n",
      "1.0\n",
      "180\n",
      "1.0\n",
      "190\n",
      "1.0\n",
      "200\n",
      "1.0\n",
      "210\n",
      "1.0\n",
      "220\n",
      "1.0\n",
      "230\n",
      "1.0\n",
      "240\n",
      "1.0\n",
      "250\n",
      "1.0\n",
      "260\n",
      "1.0\n",
      "270\n",
      "1.0\n",
      "280\n",
      "1.0\n",
      "290\n",
      "1.0\n",
      "300\n",
      "1.0\n",
      "310\n",
      "1.0\n",
      "320\n",
      "1.0\n",
      "330\n",
      "1.0\n",
      "340\n",
      "1.0\n",
      "350\n",
      "1.0\n",
      "360\n",
      "1.0\n",
      "370\n",
      "1.0\n",
      "380\n",
      "1.0\n",
      "390\n",
      "1.0\n",
      "400\n",
      "1.0\n",
      "410\n",
      "1.0\n",
      "420\n",
      "1.0\n",
      "430\n",
      "1.0\n",
      "440\n",
      "1.0\n",
      "450\n",
      "1.0\n",
      "460\n",
      "1.0\n",
      "470\n",
      "1.0\n",
      "480\n",
      "1.0\n",
      "490\n",
      "1.0\n",
      "500\n",
      "0.4\n",
      "510\n",
      "1.0\n",
      "520\n",
      "1.0\n",
      "530\n",
      "1.0\n",
      "540\n",
      "1.0\n",
      "550\n",
      "1.0\n",
      "560\n",
      "1.0\n",
      "570\n",
      "0.8\n",
      "580\n",
      "1.0\n",
      "590\n",
      "1.0\n",
      "600\n",
      "1.0\n",
      "610\n",
      "0.0\n",
      "620\n",
      "0.0\n",
      "630\n",
      "0.0\n",
      "640\n",
      "1.0\n",
      "650\n",
      "0.5\n",
      "660\n",
      "0.1\n",
      "670\n",
      "0.8\n",
      "680\n",
      "0.3\n",
      "690\n",
      "0.9\n",
      "700\n",
      "0.7\n",
      "710\n",
      "0.7\n",
      "720\n",
      "0.5\n",
      "730\n",
      "0.3\n",
      "740\n",
      "0.8\n",
      "750\n",
      "0.2\n",
      "760\n",
      "0.0\n",
      "770\n",
      "0.4\n",
      "780\n",
      "0.8\n",
      "790\n",
      "0.4\n",
      "800\n",
      "0.4\n",
      "810\n",
      "1.0\n",
      "820\n",
      "1.0\n",
      "830\n",
      "1.0\n",
      "840\n",
      "1.0\n",
      "850\n",
      "1.0\n",
      "860\n",
      "1.0\n",
      "870\n",
      "1.0\n",
      "880\n",
      "1.0\n",
      "890\n",
      "1.0\n",
      "900\n",
      "0.9\n",
      "910\n",
      "0.3\n",
      "920\n",
      "1.0\n",
      "930\n",
      "1.0\n",
      "940\n",
      "1.0\n",
      "950\n",
      "1.0\n",
      "960\n",
      "1.0\n",
      "970\n",
      "1.0\n",
      "980\n",
      "1.0\n",
      "990\n",
      "1.0\n",
      "1000\n",
      "0.9\n",
      "1010\n",
      "1.0\n",
      "1020\n",
      "1.0\n",
      "1030\n",
      "1.0\n",
      "1040\n",
      "1.0\n",
      "1050\n",
      "1.0\n",
      "1060\n",
      "1.0\n",
      "1070\n",
      "0.0\n",
      "1080\n",
      "0.4\n",
      "1090\n",
      "1.0\n",
      "1100\n",
      "0.5\n",
      "1110\n",
      "0.2\n",
      "1120\n",
      "0.0\n",
      "1130\n",
      "0.5\n",
      "1140\n",
      "0.7\n",
      "1150\n",
      "0.0\n",
      "1160\n",
      "0.0\n",
      "1170\n",
      "0.3\n",
      "1180\n",
      "0.0\n",
      "1190\n",
      "0.0\n",
      "1200\n",
      "0.0\n",
      "1210\n",
      "0.0\n",
      "1220\n",
      "0.0\n",
      "1230\n",
      "0.8\n",
      "1240\n",
      "1.0\n",
      "1250\n",
      "1.0\n",
      "1260\n",
      "0.2\n",
      "1270\n",
      "0.0\n",
      "1280\n",
      "0.0\n",
      "1290\n",
      "0.0\n",
      "1300\n",
      "0.0\n",
      "1310\n",
      "0.0\n",
      "1320\n",
      "0.0\n",
      "1330\n",
      "0.0\n",
      "1340\n",
      "0.0\n",
      "1350\n",
      "0.0\n",
      "1360\n",
      "0.0\n",
      "1370\n",
      "0.0\n",
      "1380\n",
      "0.0\n",
      "1390\n",
      "0.0\n",
      "1400\n",
      "0.0\n",
      "1410\n",
      "0.3\n",
      "1420\n",
      "1.0\n",
      "1430\n",
      "0.9\n",
      "1440\n",
      "0.4\n",
      "1450\n",
      "0.0\n",
      "1460\n",
      "0.0\n",
      "1470\n",
      "0.0\n",
      "1480\n",
      "0.0\n",
      "1490\n",
      "0.0\n",
      "1500\n",
      "0.0\n",
      "1510\n",
      "0.0\n",
      "1520\n",
      "0.0\n",
      "1530\n",
      "0.0\n",
      "1540\n",
      "0.0\n",
      "1550\n",
      "0.0\n",
      "1560\n",
      "0.0\n",
      "1570\n",
      "0.0\n",
      "1580\n",
      "0.0\n",
      "1590\n",
      "0.0\n",
      "1600\n",
      "0.0\n",
      "1610\n",
      "0.0\n",
      "1620\n",
      "0.0\n",
      "1630\n",
      "0.1\n",
      "1640\n",
      "0.0\n",
      "1650\n",
      "0.0\n",
      "1660\n",
      "0.0\n",
      "1670\n",
      "0.0\n",
      "1680\n",
      "0.0\n",
      "1690\n",
      "0.0\n",
      "1700\n",
      "0.8\n",
      "1710\n",
      "1.0\n",
      "1720\n",
      "1.0\n",
      "1730\n",
      "1.0\n",
      "1740\n",
      "1.0\n",
      "1750\n",
      "0.2\n",
      "1760\n",
      "1.0\n",
      "1770\n",
      "1.0\n",
      "1780\n",
      "1.0\n",
      "1790\n",
      "1.0\n",
      "1800\n",
      "1.0\n",
      "1810\n",
      "1.0\n",
      "1820\n",
      "1.0\n",
      "1830\n",
      "1.0\n",
      "1840\n",
      "1.0\n",
      "1850\n",
      "1.0\n",
      "1860\n",
      "1.0\n",
      "1870\n",
      "1.0\n",
      "1880\n",
      "1.0\n",
      "1890\n",
      "1.0\n",
      "1900\n",
      "1.0\n",
      "1910\n",
      "1.0\n",
      "1920\n",
      "1.0\n",
      "1930\n",
      "1.0\n",
      "1940\n",
      "1.0\n",
      "1950\n",
      "1.0\n",
      "1960\n",
      "1.0\n",
      "1970\n",
      "1.0\n",
      "1980\n",
      "1.0\n",
      "1990\n",
      "1.0\n",
      "2000\n",
      "1.0\n",
      "2010\n",
      "1.0\n",
      "2020\n",
      "1.0\n",
      "2030\n",
      "1.0\n",
      "2040\n",
      "1.0\n",
      "2050\n",
      "1.0\n",
      "2060\n",
      "1.0\n",
      "2070\n",
      "1.0\n",
      "2080\n",
      "1.0\n",
      "2090\n",
      "1.0\n",
      "2100\n",
      "1.0\n",
      "2110\n",
      "1.0\n",
      "2120\n",
      "1.0\n",
      "2130\n",
      "1.0\n",
      "2140\n",
      "1.0\n",
      "2150\n",
      "1.0\n",
      "2160\n",
      "1.0\n",
      "2170\n",
      "1.0\n",
      "2180\n",
      "1.0\n",
      "2190\n",
      "1.0\n",
      "2200\n",
      "1.0\n",
      "2210\n",
      "1.0\n",
      "2220\n",
      "1.0\n",
      "2230\n",
      "1.0\n",
      "2240\n",
      "1.0\n",
      "2250\n",
      "1.0\n",
      "2260\n",
      "1.0\n",
      "2270\n",
      "1.0\n",
      "2280\n",
      "1.0\n",
      "2290\n",
      "1.0\n",
      "2300\n",
      "1.0\n",
      "2310\n",
      "1.0\n",
      "2320\n",
      "1.0\n",
      "2330\n",
      "1.0\n",
      "2340\n",
      "1.0\n",
      "2350\n",
      "1.0\n",
      "2360\n",
      "1.0\n",
      "2370\n",
      "1.0\n",
      "2380\n",
      "1.0\n",
      "2390\n",
      "1.0\n",
      "2400\n",
      "1.0\n",
      "2410\n",
      "1.0\n",
      "2420\n",
      "1.0\n",
      "2430\n",
      "1.0\n",
      "2440\n",
      "1.0\n",
      "2450\n",
      "1.0\n",
      "2460\n",
      "1.0\n",
      "2470\n",
      "1.0\n",
      "2480\n",
      "1.0\n",
      "2490\n",
      "1.0\n",
      "2500\n",
      "1.0\n",
      "2510\n",
      "1.0\n",
      "2520\n",
      "1.0\n",
      "2530\n",
      "1.0\n",
      "2540\n",
      "1.0\n",
      "2550\n",
      "1.0\n",
      "2560\n",
      "1.0\n",
      "2570\n",
      "1.0\n",
      "2580\n",
      "1.0\n",
      "2590\n",
      "1.0\n",
      "2600\n",
      "1.0\n",
      "2610\n",
      "1.0\n",
      "2620\n",
      "1.0\n",
      "2630\n",
      "1.0\n",
      "2640\n",
      "1.0\n",
      "2650\n",
      "1.0\n",
      "2660\n",
      "1.0\n",
      "2670\n",
      "1.0\n",
      "2680\n",
      "1.0\n",
      "2690\n",
      "1.0\n",
      "2700\n",
      "1.0\n",
      "2710\n",
      "1.0\n",
      "2720\n",
      "1.0\n",
      "2730\n",
      "1.0\n",
      "2740\n",
      "1.0\n",
      "2750\n",
      "1.0\n",
      "2760\n",
      "1.0\n",
      "2770\n",
      "1.0\n",
      "2780\n",
      "0.1\n",
      "2790\n",
      "0.0\n",
      "2800\n",
      "0.0\n",
      "2810\n",
      "0.0\n",
      "2820\n",
      "0.0\n",
      "2830\n",
      "0.0\n",
      "2840\n",
      "0.0\n",
      "2850\n",
      "0.2\n",
      "2860\n",
      "0.3\n",
      "2870\n",
      "0.1\n",
      "2880\n",
      "0.0\n",
      "2890\n",
      "0.1\n",
      "2900\n",
      "0.0\n",
      "2910\n",
      "0.0\n",
      "2920\n",
      "0.0\n",
      "2930\n",
      "0.0\n",
      "2940\n",
      "0.0\n",
      "2950\n",
      "0.0\n",
      "2960\n",
      "0.0\n",
      "2970\n",
      "0.0\n",
      "2980\n",
      "0.0\n",
      "2990\n",
      "0.4\n",
      "3000\n",
      "0.1\n",
      "3010\n",
      "0.0\n",
      "3020\n",
      "0.0\n",
      "3030\n",
      "0.0\n",
      "3040\n",
      "0.0\n",
      "3050\n",
      "0.0\n",
      "3060\n",
      "0.0\n",
      "3070\n",
      "0.0\n",
      "3080\n",
      "0.0\n",
      "3090\n",
      "0.0\n",
      "3100\n",
      "0.0\n",
      "3110\n",
      "0.0\n",
      "3120\n",
      "0.0\n",
      "3130\n",
      "0.0\n",
      "3140\n",
      "0.0\n",
      "3150\n",
      "0.0\n",
      "3160\n",
      "0.0\n",
      "3170\n",
      "0.0\n",
      "3180\n",
      "0.0\n",
      "3190\n",
      "0.0\n",
      "3200\n",
      "0.0\n",
      "3210\n",
      "0.5\n",
      "3220\n",
      "0.2\n",
      "3230\n",
      "0.1\n",
      "3240\n",
      "0.0\n",
      "3250\n",
      "0.0\n",
      "3260\n",
      "0.0\n",
      "3270\n",
      "0.0\n",
      "3280\n",
      "0.0\n",
      "3290\n",
      "0.2\n",
      "3300\n",
      "0.0\n",
      "3310\n",
      "1.0\n",
      "3320\n",
      "1.0\n",
      "3330\n",
      "0.7\n",
      "3340\n",
      "1.0\n",
      "3350\n",
      "0.9\n",
      "3360\n",
      "0.0\n",
      "3370\n",
      "1.0\n",
      "3380\n",
      "1.0\n",
      "3390\n",
      "1.0\n",
      "3400\n",
      "1.0\n",
      "3410\n",
      "1.0\n",
      "3420\n",
      "1.0\n",
      "3430\n",
      "1.0\n",
      "3440\n",
      "1.0\n",
      "3450\n",
      "1.0\n",
      "3460\n",
      "1.0\n",
      "3470\n",
      "1.0\n",
      "3480\n",
      "1.0\n",
      "3490\n",
      "1.0\n",
      "3500\n",
      "1.0\n",
      "3510\n",
      "1.0\n",
      "3520\n",
      "0.9\n",
      "3530\n",
      "1.0\n",
      "3540\n",
      "1.0\n",
      "3550\n",
      "1.0\n",
      "3560\n",
      "1.0\n",
      "3570\n",
      "1.0\n",
      "3580\n",
      "1.0\n",
      "3590\n",
      "1.0\n",
      "3600\n",
      "1.0\n",
      "3610\n",
      "1.0\n",
      "3620\n",
      "1.0\n",
      "3630\n",
      "1.0\n",
      "3640\n",
      "1.0\n",
      "3650\n",
      "1.0\n",
      "3660\n",
      "1.0\n",
      "3670\n",
      "1.0\n",
      "3680\n",
      "1.0\n",
      "3690\n",
      "1.0\n",
      "3700\n",
      "1.0\n",
      "3710\n",
      "1.0\n",
      "3720\n",
      "1.0\n",
      "3730\n",
      "1.0\n",
      "3740\n",
      "1.0\n",
      "3750\n",
      "1.0\n",
      "3760\n",
      "1.0\n",
      "3770\n",
      "1.0\n",
      "3780\n",
      "1.0\n",
      "3790\n",
      "1.0\n",
      "3800\n",
      "1.0\n",
      "3810\n",
      "1.0\n",
      "3820\n",
      "1.0\n",
      "3830\n",
      "1.0\n",
      "3840\n",
      "1.0\n",
      "3850\n",
      "1.0\n",
      "3860\n",
      "1.0\n",
      "3870\n",
      "1.0\n",
      "3880\n",
      "1.0\n",
      "3890\n",
      "1.0\n",
      "3900\n",
      "1.0\n",
      "3910\n",
      "0.0\n",
      "3920\n",
      "0.0\n",
      "3930\n",
      "0.0\n",
      "3940\n",
      "0.0\n",
      "3950\n",
      "0.0\n",
      "3960\n",
      "0.0\n",
      "3970\n",
      "0.0\n",
      "3980\n",
      "0.0\n",
      "3990\n",
      "0.0\n",
      "4000\n",
      "0.0\n",
      "4010\n",
      "0.0\n",
      "4020\n",
      "0.0\n",
      "4030\n",
      "0.0\n",
      "4040\n",
      "0.0\n",
      "4050\n",
      "0.0\n",
      "4060\n",
      "0.0\n",
      "4070\n",
      "0.5\n",
      "4080\n",
      "1.0\n",
      "4090\n",
      "1.0\n",
      "4100\n",
      "1.0\n",
      "4110\n",
      "1.0\n",
      "4120\n",
      "1.0\n",
      "4130\n",
      "1.0\n",
      "4140\n",
      "0.9\n",
      "4150\n",
      "0.9\n",
      "4160\n",
      "1.0\n",
      "4170\n",
      "1.0\n",
      "4180\n",
      "1.0\n",
      "4190\n",
      "1.0\n",
      "4200\n",
      "1.0\n",
      "4210\n",
      "1.0\n",
      "4220\n",
      "1.0\n",
      "4230\n",
      "1.0\n",
      "4240\n",
      "1.0\n",
      "4250\n",
      "1.0\n",
      "4260\n",
      "1.0\n",
      "4270\n",
      "1.0\n",
      "4280\n",
      "1.0\n",
      "4290\n",
      "1.0\n",
      "4300\n",
      "0.6\n",
      "4310\n",
      "0.4\n",
      "4320\n",
      "0.0\n",
      "4330\n",
      "1.0\n",
      "4340\n",
      "0.1\n",
      "4350\n",
      "0.0\n",
      "4360\n",
      "0.2\n",
      "4370\n",
      "0.8\n",
      "4380\n",
      "0.3\n",
      "4390\n",
      "0.6\n",
      "4400\n",
      "0.4\n",
      "4410\n",
      "0.6\n",
      "4420\n",
      "0.2\n",
      "4430\n",
      "0.0\n",
      "4440\n",
      "0.0\n",
      "4450\n",
      "0.0\n",
      "4460\n",
      "0.7\n",
      "4470\n",
      "0.4\n",
      "4480\n",
      "1.0\n",
      "4490\n",
      "1.0\n",
      "4500\n",
      "1.0\n",
      "4510\n",
      "1.0\n",
      "4520\n",
      "1.0\n",
      "4530\n",
      "0.9\n",
      "4540\n",
      "0.6\n",
      "4550\n",
      "1.0\n",
      "4560\n",
      "1.0\n",
      "4570\n",
      "1.0\n",
      "4580\n",
      "1.0\n",
      "4590\n",
      "1.0\n",
      "4600\n",
      "0.8\n",
      "4610\n",
      "1.0\n",
      "4620\n",
      "1.0\n",
      "4630\n",
      "0.9\n",
      "4640\n",
      "1.0\n",
      "4650\n",
      "0.9\n",
      "4660\n",
      "0.4\n",
      "4670\n",
      "0.3\n",
      "4680\n",
      "0.0\n",
      "4690\n",
      "0.9\n",
      "4700\n",
      "1.0\n",
      "4710\n",
      "1.0\n",
      "4720\n",
      "1.0\n",
      "4730\n",
      "1.0\n",
      "4740\n",
      "1.0\n",
      "4750\n",
      "1.0\n",
      "4760\n",
      "1.0\n",
      "4770\n",
      "1.0\n",
      "4780\n",
      "1.0\n",
      "4790\n",
      "1.0\n",
      "4800\n",
      "1.0\n",
      "4810\n",
      "1.0\n",
      "4820\n",
      "1.0\n",
      "4830\n",
      "1.0\n",
      "4840\n",
      "1.0\n",
      "4850\n",
      "1.0\n",
      "4860\n",
      "0.1\n",
      "4870\n",
      "0.0\n",
      "4880\n",
      "0.0\n",
      "4890\n",
      "0.3\n",
      "4900\n",
      "0.1\n",
      "4910\n",
      "1.0\n",
      "4920\n",
      "1.0\n",
      "4930\n",
      "1.0\n",
      "4940\n",
      "1.0\n",
      "4950\n",
      "1.0\n",
      "4960\n",
      "1.0\n",
      "4970\n",
      "1.0\n",
      "4980\n",
      "1.0\n",
      "4990\n",
      "1.0\n",
      "5000\n",
      "1.0\n",
      "5010\n",
      "1.0\n",
      "5020\n",
      "0.5\n",
      "5030\n",
      "0.5\n",
      "5040\n",
      "0.0\n",
      "5050\n",
      "0.0\n",
      "5060\n",
      "0.0\n",
      "5070\n",
      "0.1\n",
      "5080\n",
      "0.1\n",
      "5090\n",
      "0.5\n",
      "5100\n",
      "0.0\n",
      "5110\n",
      "0.6\n",
      "5120\n",
      "1.0\n",
      "5130\n",
      "0.4\n",
      "5140\n",
      "0.2\n",
      "5150\n",
      "0.6\n",
      "5160\n",
      "0.5\n",
      "5170\n",
      "1.0\n",
      "5180\n",
      "1.0\n",
      "5190\n",
      "1.0\n",
      "5200\n",
      "0.3\n",
      "5210\n",
      "1.0\n",
      "5220\n",
      "1.0\n",
      "5230\n",
      "0.8\n",
      "5240\n",
      "1.0\n",
      "5250\n",
      "1.0\n",
      "5260\n",
      "1.0\n",
      "5270\n",
      "1.0\n",
      "5280\n",
      "1.0\n",
      "5290\n",
      "1.0\n",
      "5300\n",
      "1.0\n",
      "5310\n",
      "1.0\n",
      "5320\n",
      "1.0\n",
      "5330\n",
      "1.0\n",
      "5340\n",
      "1.0\n",
      "5350\n",
      "0.9\n",
      "5360\n",
      "0.8\n",
      "5370\n",
      "0.4\n",
      "5380\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.68311688311688323, 0.42951490011187549)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_min_day = []\n",
    "i = 0\n",
    "for i in range(0,9000-3600-10,10):\n",
    "    print i\n",
    "    data_train = data_2014_up[0][i:i+3600]\n",
    "    X_train = data_train.drop([\"0\"],axis=1)\n",
    "    y_train = data_train['0']\n",
    "    data_test = data_2014_up[0][i+3600:i+3600+10]\n",
    "    X_test = data_test.drop([\"0\"],axis=1)\n",
    "    y_test = data_test['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    acc = latest_day_loop_acc(X_train,y_train,X_test,y_test,model)\n",
    "    print acc\n",
    "    mean_min_day.append(acc)\n",
    "np.mean(mean_min_day),np.std(mean_min_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1.0\n",
      "10\n",
      "1.0\n",
      "20\n",
      "1.0\n",
      "30\n",
      "1.0\n",
      "40\n",
      "1.0\n",
      "50\n",
      "1.0\n",
      "60\n",
      "1.0\n",
      "70\n",
      "1.0\n",
      "80\n",
      "1.0\n",
      "90\n",
      "1.0\n",
      "100\n",
      "1.0\n",
      "110\n",
      "1.0\n",
      "120\n",
      "1.0\n",
      "130\n",
      "1.0\n",
      "140\n",
      "1.0\n",
      "150\n",
      "1.0\n",
      "160\n",
      "1.0\n",
      "170\n",
      "1.0\n",
      "180\n",
      "1.0\n",
      "190\n",
      "1.0\n",
      "200\n",
      "1.0\n",
      "210\n",
      "1.0\n",
      "220\n",
      "1.0\n",
      "230\n",
      "1.0\n",
      "240\n",
      "1.0\n",
      "250\n",
      "1.0\n",
      "260\n",
      "1.0\n",
      "270\n",
      "1.0\n",
      "280\n",
      "1.0\n",
      "290\n",
      "1.0\n",
      "300\n",
      "1.0\n",
      "310\n",
      "1.0\n",
      "320\n",
      "1.0\n",
      "330\n",
      "1.0\n",
      "340\n",
      "1.0\n",
      "350\n",
      "1.0\n",
      "360\n",
      "1.0\n",
      "370\n",
      "1.0\n",
      "380\n",
      "1.0\n",
      "390\n",
      "1.0\n",
      "400\n",
      "1.0\n",
      "410\n",
      "0.8\n",
      "420\n",
      "1.0\n",
      "430\n",
      "1.0\n",
      "440\n",
      "1.0\n",
      "450\n",
      "0.8\n",
      "460\n",
      "0.0\n",
      "470\n",
      "0.0\n",
      "480\n",
      "0.7\n",
      "490\n",
      "1.0\n",
      "500\n",
      "1.0\n",
      "510\n",
      "1.0\n",
      "520\n",
      "1.0\n",
      "530\n",
      "0.6\n",
      "540\n",
      "0.0\n",
      "550\n",
      "0.6\n",
      "560\n",
      "0.5\n",
      "570\n",
      "0.2\n",
      "580\n",
      "1.0\n",
      "590\n",
      "0.6\n",
      "600\n",
      "1.0\n",
      "610\n",
      "0.2\n",
      "620\n",
      "0.2\n",
      "630\n",
      "1.0\n",
      "640\n",
      "1.0\n",
      "650\n",
      "1.0\n",
      "660\n",
      "1.0\n",
      "670\n",
      "1.0\n",
      "680\n",
      "1.0\n",
      "690\n",
      "1.0\n",
      "700\n",
      "1.0\n",
      "710\n",
      "1.0\n",
      "720\n",
      "1.0\n",
      "730\n",
      "1.0\n",
      "740\n",
      "1.0\n",
      "750\n",
      "1.0\n",
      "760\n",
      "1.0\n",
      "770\n",
      "1.0\n",
      "780\n",
      "1.0\n",
      "790\n",
      "1.0\n",
      "800\n",
      "1.0\n",
      "810\n",
      "1.0\n",
      "820\n",
      "1.0\n",
      "830\n",
      "1.0\n",
      "840\n",
      "1.0\n",
      "850\n",
      "1.0\n",
      "860\n",
      "1.0\n",
      "870\n",
      "1.0\n",
      "880\n",
      "1.0\n",
      "890\n",
      "1.0\n",
      "900\n",
      "1.0\n",
      "910\n",
      "1.0\n",
      "920\n",
      "1.0\n",
      "930\n",
      "1.0\n",
      "940\n",
      "1.0\n",
      "950\n",
      "1.0\n",
      "960\n",
      "0.2\n",
      "970\n",
      "0.5\n",
      "980\n",
      "0.1\n",
      "990\n",
      "0.0\n",
      "1000\n",
      "0.0\n",
      "1010\n",
      "0.0\n",
      "1020\n",
      "0.0\n",
      "1030\n",
      "0.0\n",
      "1040\n",
      "0.1\n",
      "1050\n",
      "0.2\n",
      "1060\n",
      "0.0\n",
      "1070\n",
      "0.0\n",
      "1080\n",
      "0.0\n",
      "1090\n",
      "0.7\n",
      "1100\n",
      "1.0\n",
      "1110\n",
      "0.6\n",
      "1120\n",
      "0.0\n",
      "1130\n",
      "0.0\n",
      "1140\n",
      "0.0\n",
      "1150\n",
      "0.0\n",
      "1160\n",
      "0.0\n",
      "1170\n",
      "0.0\n",
      "1180\n",
      "0.0\n",
      "1190\n",
      "0.0\n",
      "1200\n",
      "0.0\n",
      "1210\n",
      "0.0\n",
      "1220\n",
      "0.0\n",
      "1230\n",
      "0.6\n",
      "1240\n",
      "0.4\n",
      "1250\n",
      "0.0\n",
      "1260\n",
      "0.9\n",
      "1270\n",
      "1.0\n",
      "1280\n",
      "1.0\n",
      "1290\n",
      "0.8\n",
      "1300\n",
      "1.0\n",
      "1310\n",
      "1.0\n",
      "1320\n",
      "1.0\n",
      "1330\n",
      "1.0\n",
      "1340\n",
      "1.0\n",
      "1350\n",
      "1.0\n",
      "1360\n",
      "1.0\n",
      "1370\n",
      "1.0\n",
      "1380\n",
      "0.4\n",
      "1390\n",
      "1.0\n",
      "1400\n",
      "0.2\n",
      "1410\n",
      "1.0\n",
      "1420\n",
      "1.0\n",
      "1430\n",
      "1.0\n",
      "1440\n",
      "0.7\n",
      "1450\n",
      "0.8\n",
      "1460\n",
      "0.7\n",
      "1470\n",
      "0.8\n",
      "1480\n",
      "0.6\n",
      "1490\n",
      "0.2\n",
      "1500\n",
      "0.3\n",
      "1510\n",
      "1.0\n",
      "1520\n",
      "0.7\n",
      "1530\n",
      "1.0\n",
      "1540\n",
      "0.9\n",
      "1550\n",
      "1.0\n",
      "1560\n",
      "0.7\n",
      "1570\n",
      "1.0\n",
      "1580\n",
      "1.0\n",
      "1590\n",
      "1.0\n",
      "1600\n",
      "1.0\n",
      "1610\n",
      "1.0\n",
      "1620\n",
      "1.0\n",
      "1630\n",
      "0.8\n",
      "1640\n",
      "0.5\n",
      "1650\n",
      "1.0\n",
      "1660\n",
      "1.0\n",
      "1670\n",
      "1.0\n",
      "1680\n",
      "0.7\n",
      "1690\n",
      "0.0\n",
      "1700\n",
      "0.8\n",
      "1710\n",
      "1.0\n",
      "1720\n",
      "1.0\n",
      "1730\n",
      "1.0\n",
      "1740\n",
      "1.0\n",
      "1750\n",
      "1.0\n",
      "1760\n",
      "1.0\n",
      "1770\n",
      "1.0\n",
      "1780\n",
      "1.0\n",
      "1790\n",
      "1.0\n",
      "1800\n",
      "1.0\n",
      "1810\n",
      "1.0\n",
      "1820\n",
      "1.0\n",
      "1830\n",
      "1.0\n",
      "1840\n",
      "1.0\n",
      "1850\n",
      "1.0\n",
      "1860\n",
      "1.0\n",
      "1870\n",
      "1.0\n",
      "1880\n",
      "1.0\n",
      "1890\n",
      "1.0\n",
      "1900\n",
      "1.0\n",
      "1910\n",
      "1.0\n",
      "1920\n",
      "1.0\n",
      "1930\n",
      "1.0\n",
      "1940\n",
      "1.0\n",
      "1950\n",
      "1.0\n",
      "1960\n",
      "1.0\n",
      "1970\n",
      "1.0\n",
      "1980\n",
      "1.0\n",
      "1990\n",
      "1.0\n",
      "2000\n",
      "1.0\n",
      "2010\n",
      "1.0\n",
      "2020\n",
      "1.0\n",
      "2030\n",
      "1.0\n",
      "2040\n",
      "1.0\n",
      "2050\n",
      "1.0\n",
      "2060\n",
      "1.0\n",
      "2070\n",
      "1.0\n",
      "2080\n",
      "1.0\n",
      "2090\n",
      "1.0\n",
      "2100\n",
      "1.0\n",
      "2110\n",
      "1.0\n",
      "2120\n",
      "1.0\n",
      "2130\n",
      "1.0\n",
      "2140\n",
      "1.0\n",
      "2150\n",
      "1.0\n",
      "2160\n",
      "1.0\n",
      "2170\n",
      "1.0\n",
      "2180\n",
      "1.0\n",
      "2190\n",
      "1.0\n",
      "2200\n",
      "1.0\n",
      "2210\n",
      "1.0\n",
      "2220\n",
      "1.0\n",
      "2230\n",
      "1.0\n",
      "2240\n",
      "1.0\n",
      "2250\n",
      "1.0\n",
      "2260\n",
      "1.0\n",
      "2270\n",
      "1.0\n",
      "2280\n",
      "1.0\n",
      "2290\n",
      "1.0\n",
      "2300\n",
      "1.0\n",
      "2310\n",
      "1.0\n",
      "2320\n",
      "1.0\n",
      "2330\n",
      "1.0\n",
      "2340\n",
      "1.0\n",
      "2350\n",
      "1.0\n",
      "2360\n",
      "1.0\n",
      "2370\n",
      "1.0\n",
      "2380\n",
      "1.0\n",
      "2390\n",
      "1.0\n",
      "2400\n",
      "1.0\n",
      "2410\n",
      "0.5\n",
      "2420\n",
      "1.0\n",
      "2430\n",
      "0.9\n",
      "2440\n",
      "0.9\n",
      "2450\n",
      "0.8\n",
      "2460\n",
      "0.8\n",
      "2470\n",
      "0.8\n",
      "2480\n",
      "1.0\n",
      "2490\n",
      "1.0\n",
      "2500\n",
      "0.8\n",
      "2510\n",
      "1.0\n",
      "2520\n",
      "1.0\n",
      "2530\n",
      "0.7\n",
      "2540\n",
      "1.0\n",
      "2550\n",
      "1.0\n",
      "2560\n",
      "0.6\n",
      "2570\n",
      "0.9\n",
      "2580\n",
      "1.0\n",
      "2590\n",
      "1.0\n",
      "2600\n",
      "0.8\n",
      "2610\n",
      "0.2\n",
      "2620\n",
      "0.5\n",
      "2630\n",
      "1.0\n",
      "2640\n",
      "0.5\n",
      "2650\n",
      "0.0\n",
      "2660\n",
      "0.0\n",
      "2670\n",
      "1.0\n",
      "2680\n",
      "0.8\n",
      "2690\n",
      "1.0\n",
      "2700\n",
      "1.0\n",
      "2710\n",
      "1.0\n",
      "2720\n",
      "1.0\n",
      "2730\n",
      "0.6\n",
      "2740\n",
      "1.0\n",
      "2750\n",
      "1.0\n",
      "2760\n",
      "1.0\n",
      "2770\n",
      "1.0\n",
      "2780\n",
      "1.0\n",
      "2790\n",
      "1.0\n",
      "2800\n",
      "0.9\n",
      "2810\n",
      "1.0\n",
      "2820\n",
      "1.0\n",
      "2830\n",
      "1.0\n",
      "2840\n",
      "1.0\n",
      "2850\n",
      "1.0\n",
      "2860\n",
      "1.0\n",
      "2870\n",
      "0.0\n",
      "2880\n",
      "0.4\n",
      "2890\n",
      "1.0\n",
      "2900\n",
      "0.1\n",
      "2910\n",
      "0.1\n",
      "2920\n",
      "0.0\n",
      "2930\n",
      "0.0\n",
      "2940\n",
      "0.0\n",
      "2950\n",
      "0.0\n",
      "2960\n",
      "0.0\n",
      "2970\n",
      "0.3\n",
      "2980\n",
      "0.0\n",
      "2990\n",
      "0.0\n",
      "3000\n",
      "0.0\n",
      "3010\n",
      "0.1\n",
      "3020\n",
      "0.0\n",
      "3030\n",
      "0.8\n",
      "3040\n",
      "1.0\n",
      "3050\n",
      "1.0\n",
      "3060\n",
      "0.2\n",
      "3070\n",
      "0.0\n",
      "3080\n",
      "0.0\n",
      "3090\n",
      "0.0\n",
      "3100\n",
      "0.0\n",
      "3110\n",
      "0.0\n",
      "3120\n",
      "0.2\n",
      "3130\n",
      "0.2\n",
      "3140\n",
      "0.0\n",
      "3150\n",
      "0.0\n",
      "3160\n",
      "0.0\n",
      "3170\n",
      "0.0\n",
      "3180\n",
      "0.0\n",
      "3190\n",
      "0.4\n",
      "3200\n",
      "0.2\n",
      "3210\n",
      "1.0\n",
      "3220\n",
      "1.0\n",
      "3230\n",
      "1.0\n",
      "3240\n",
      "0.9\n",
      "3250\n",
      "0.0\n",
      "3260\n",
      "0.0\n",
      "3270\n",
      "0.0\n",
      "3280\n",
      "0.0\n",
      "3290\n",
      "0.0\n",
      "3300\n",
      "0.0\n",
      "3310\n",
      "0.0\n",
      "3320\n",
      "0.0\n",
      "3330\n",
      "0.0\n",
      "3340\n",
      "0.0\n",
      "3350\n",
      "0.0\n",
      "3360\n",
      "0.0\n",
      "3370\n",
      "0.0\n",
      "3380\n",
      "0.0\n",
      "3390\n",
      "0.0\n",
      "3400\n",
      "0.0\n",
      "3410\n",
      "0.0\n",
      "3420\n",
      "0.0\n",
      "3430\n",
      "0.1\n",
      "3440\n",
      "0.0\n",
      "3450\n",
      "0.0\n",
      "3460\n",
      "0.0\n",
      "3470\n",
      "0.0\n",
      "3480\n",
      "0.0\n",
      "3490\n",
      "0.0\n",
      "3500\n",
      "0.1\n",
      "3510\n",
      "0.0\n",
      "3520\n",
      "1.0\n",
      "3530\n",
      "1.0\n",
      "3540\n",
      "1.0\n",
      "3550\n",
      "0.2\n",
      "3560\n",
      "1.0\n",
      "3570\n",
      "1.0\n",
      "3580\n",
      "1.0\n",
      "3590\n",
      "1.0\n",
      "3600\n",
      "1.0\n",
      "3610\n",
      "1.0\n",
      "3620\n",
      "1.0\n",
      "3630\n",
      "1.0\n",
      "3640\n",
      "1.0\n",
      "3650\n",
      "1.0\n",
      "3660\n",
      "1.0\n",
      "3670\n",
      "1.0\n",
      "3680\n",
      "1.0\n",
      "3690\n",
      "1.0\n",
      "3700\n",
      "1.0\n",
      "3710\n",
      "1.0\n",
      "3720\n",
      "1.0\n",
      "3730\n",
      "1.0\n",
      "3740\n",
      "1.0\n",
      "3750\n",
      "1.0\n",
      "3760\n",
      "1.0\n",
      "3770\n",
      "1.0\n",
      "3780\n",
      "1.0\n",
      "3790\n",
      "1.0\n",
      "3800\n",
      "1.0\n",
      "3810\n",
      "1.0\n",
      "3820\n",
      "1.0\n",
      "3830\n",
      "1.0\n",
      "3840\n",
      "1.0\n",
      "3850\n",
      "1.0\n",
      "3860\n",
      "1.0\n",
      "3870\n",
      "1.0\n",
      "3880\n",
      "1.0\n",
      "3890\n",
      "1.0\n",
      "3900\n",
      "1.0\n",
      "3910\n",
      "1.0\n",
      "3920\n",
      "1.0\n",
      "3930\n",
      "1.0\n",
      "3940\n",
      "1.0\n",
      "3950\n",
      "1.0\n",
      "3960\n",
      "1.0\n",
      "3970\n",
      "1.0\n",
      "3980\n",
      "1.0\n",
      "3990\n",
      "1.0\n",
      "4000\n",
      "1.0\n",
      "4010\n",
      "1.0\n",
      "4020\n",
      "1.0\n",
      "4030\n",
      "1.0\n",
      "4040\n",
      "1.0\n",
      "4050\n",
      "1.0\n",
      "4060\n",
      "1.0\n",
      "4070\n",
      "1.0\n",
      "4080\n",
      "1.0\n",
      "4090\n",
      "1.0\n",
      "4100\n",
      "1.0\n",
      "4110\n",
      "1.0\n",
      "4120\n",
      "1.0\n",
      "4130\n",
      "1.0\n",
      "4140\n",
      "1.0\n",
      "4150\n",
      "1.0\n",
      "4160\n",
      "1.0\n",
      "4170\n",
      "1.0\n",
      "4180\n",
      "1.0\n",
      "4190\n",
      "1.0\n",
      "4200\n",
      "1.0\n",
      "4210\n",
      "1.0\n",
      "4220\n",
      "1.0\n",
      "4230\n",
      "1.0\n",
      "4240\n",
      "1.0\n",
      "4250\n",
      "1.0\n",
      "4260\n",
      "1.0\n",
      "4270\n",
      "1.0\n",
      "4280\n",
      "1.0\n",
      "4290\n",
      "1.0\n",
      "4300\n",
      "1.0\n",
      "4310\n",
      "1.0\n",
      "4320\n",
      "1.0\n",
      "4330\n",
      "1.0\n",
      "4340\n",
      "1.0\n",
      "4350\n",
      "1.0\n",
      "4360\n",
      "1.0\n",
      "4370\n",
      "1.0\n",
      "4380\n",
      "1.0\n",
      "4390\n",
      "1.0\n",
      "4400\n",
      "1.0\n",
      "4410\n",
      "1.0\n",
      "4420\n",
      "1.0\n",
      "4430\n",
      "1.0\n",
      "4440\n",
      "1.0\n",
      "4450\n",
      "1.0\n",
      "4460\n",
      "1.0\n",
      "4470\n",
      "1.0\n",
      "4480\n",
      "1.0\n",
      "4490\n",
      "1.0\n",
      "4500\n",
      "1.0\n",
      "4510\n",
      "1.0\n",
      "4520\n",
      "1.0\n",
      "4530\n",
      "1.0\n",
      "4540\n",
      "1.0\n",
      "4550\n",
      "1.0\n",
      "4560\n",
      "1.0\n",
      "4570\n",
      "1.0\n",
      "4580\n",
      "0.3\n",
      "4590\n",
      "0.6\n",
      "4600\n",
      "1.0\n",
      "4610\n",
      "0.9\n",
      "4620\n",
      "0.3\n",
      "4630\n",
      "0.3\n",
      "4640\n",
      "0.9\n",
      "4650\n",
      "0.6\n",
      "4660\n",
      "0.9\n",
      "4670\n",
      "1.0\n",
      "4680\n",
      "0.6\n",
      "4690\n",
      "0.5\n",
      "4700\n",
      "0.4\n",
      "4710\n",
      "1.0\n",
      "4720\n",
      "0.7\n",
      "4730\n",
      "1.0\n",
      "4740\n",
      "1.0\n",
      "4750\n",
      "1.0\n",
      "4760\n",
      "0.6\n",
      "4770\n",
      "1.0\n",
      "4780\n",
      "1.0\n",
      "4790\n",
      "1.0\n",
      "4800\n",
      "1.0\n",
      "4810\n",
      "1.0\n",
      "4820\n",
      "1.0\n",
      "4830\n",
      "1.0\n",
      "4840\n",
      "1.0\n",
      "4850\n",
      "0.5\n",
      "4860\n",
      "0.1\n",
      "4870\n",
      "0.3\n",
      "4880\n",
      "1.0\n",
      "4890\n",
      "0.9\n",
      "4900\n",
      "0.7\n",
      "4910\n",
      "1.0\n",
      "4920\n",
      "0.7\n",
      "4930\n",
      "0.9\n",
      "4940\n",
      "1.0\n",
      "4950\n",
      "0.5\n",
      "4960\n",
      "0.1\n",
      "4970\n",
      "0.1\n",
      "4980\n",
      "0.4\n",
      "4990\n",
      "0.8\n",
      "5000\n",
      "1.0\n",
      "5010\n",
      "1.0\n",
      "5020\n",
      "0.8\n",
      "5030\n",
      "1.0\n",
      "5040\n",
      "1.0\n",
      "5050\n",
      "0.8\n",
      "5060\n",
      "0.8\n",
      "5070\n",
      "0.8\n",
      "5080\n",
      "0.0\n",
      "5090\n",
      "1.0\n",
      "5100\n",
      "1.0\n",
      "5110\n",
      "0.0\n",
      "5120\n",
      "0.2\n",
      "5130\n",
      "0.8\n",
      "5140\n",
      "1.0\n",
      "5150\n",
      "1.0\n",
      "5160\n",
      "1.0\n",
      "5170\n",
      "1.0\n",
      "5180\n",
      "1.0\n",
      "5190\n",
      "1.0\n",
      "5200\n",
      "1.0\n",
      "5210\n",
      "1.0\n",
      "5220\n",
      "1.0\n",
      "5230\n",
      "1.0\n",
      "5240\n",
      "1.0\n",
      "5250\n",
      "1.0\n",
      "5260\n",
      "1.0\n",
      "5270\n",
      "1.0\n",
      "5280\n",
      "1.0\n",
      "5290\n",
      "1.0\n",
      "5300\n",
      "1.0\n",
      "5310\n",
      "1.0\n",
      "5320\n",
      "1.0\n",
      "5330\n",
      "1.0\n",
      "5340\n",
      "1.0\n",
      "5350\n",
      "1.0\n",
      "5360\n",
      "1.0\n",
      "5370\n",
      "1.0\n",
      "5380\n",
      "1.0\n",
      "5390\n",
      "1.0\n",
      "5400\n",
      "1.0\n",
      "5410\n",
      "1.0\n",
      "5420\n",
      "1.0\n",
      "5430\n",
      "1.0\n",
      "5440\n",
      "1.0\n",
      "5450\n",
      "1.0\n",
      "5460\n",
      "1.0\n",
      "5470\n",
      "1.0\n",
      "5480\n",
      "1.0\n",
      "5490\n",
      "1.0\n",
      "5500\n",
      "1.0\n",
      "5510\n",
      "1.0\n",
      "5520\n",
      "1.0\n",
      "5530\n",
      "1.0\n",
      "5540\n",
      "1.0\n",
      "5550\n",
      "1.0\n",
      "5560\n",
      "1.0\n",
      "5570\n",
      "1.0\n",
      "5580\n",
      "1.0\n",
      "5590\n",
      "1.0\n",
      "5600\n",
      "1.0\n",
      "5610\n",
      "1.0\n",
      "5620\n",
      "1.0\n",
      "5630\n",
      "1.0\n",
      "5640\n",
      "1.0\n",
      "5650\n",
      "1.0\n",
      "5660\n",
      "1.0\n",
      "5670\n",
      "1.0\n",
      "5680\n",
      "1.0\n",
      "5690\n",
      "1.0\n",
      "5700\n",
      "1.0\n",
      "5710\n",
      "0.0\n",
      "5720\n",
      "0.0\n",
      "5730\n",
      "0.0\n",
      "5740\n",
      "0.0\n",
      "5750\n",
      "0.0\n",
      "5760\n",
      "0.0\n",
      "5770\n",
      "0.0\n",
      "5780\n",
      "0.0\n",
      "5790\n",
      "0.0\n",
      "5800\n",
      "0.2\n",
      "5810\n",
      "0.0\n",
      "5820\n",
      "0.0\n",
      "5830\n",
      "0.0\n",
      "5840\n",
      "0.0\n",
      "5850\n",
      "0.0\n",
      "5860\n",
      "0.0\n",
      "5870\n",
      "0.5\n",
      "5880\n",
      "1.0\n",
      "5890\n",
      "1.0\n",
      "5900\n",
      "1.0\n",
      "5910\n",
      "1.0\n",
      "5920\n",
      "1.0\n",
      "5930\n",
      "1.0\n",
      "5940\n",
      "0.9\n",
      "5950\n",
      "0.9\n",
      "5960\n",
      "1.0\n",
      "5970\n",
      "1.0\n",
      "5980\n",
      "1.0\n",
      "5990\n",
      "1.0\n",
      "6000\n",
      "1.0\n",
      "6010\n",
      "1.0\n",
      "6020\n",
      "1.0\n",
      "6030\n",
      "1.0\n",
      "6040\n",
      "1.0\n",
      "6050\n",
      "1.0\n",
      "6060\n",
      "1.0\n",
      "6070\n",
      "1.0\n",
      "6080\n",
      "1.0\n",
      "6090\n",
      "1.0\n",
      "6100\n",
      "0.6\n",
      "6110\n",
      "0.7\n",
      "6120\n",
      "1.0\n",
      "6130\n",
      "1.0\n",
      "6140\n",
      "0.1\n",
      "6150\n",
      "0.0\n",
      "6160\n",
      "0.0\n",
      "6170\n",
      "0.0\n",
      "6180\n",
      "0.5\n",
      "6190\n",
      "0.6\n",
      "6200\n",
      "0.0\n",
      "6210\n",
      "0.1\n",
      "6220\n",
      "0.3\n",
      "6230\n",
      "0.0\n",
      "6240\n",
      "0.0\n",
      "6250\n",
      "0.8\n",
      "6260\n",
      "1.0\n",
      "6270\n",
      "1.0\n",
      "6280\n",
      "1.0\n",
      "6290\n",
      "1.0\n",
      "6300\n",
      "1.0\n",
      "6310\n",
      "1.0\n",
      "6320\n",
      "1.0\n",
      "6330\n",
      "1.0\n",
      "6340\n",
      "1.0\n",
      "6350\n",
      "1.0\n",
      "6360\n",
      "1.0\n",
      "6370\n",
      "1.0\n",
      "6380\n",
      "1.0\n",
      "6390\n",
      "1.0\n",
      "6400\n",
      "0.9\n",
      "6410\n",
      "1.0\n",
      "6420\n",
      "1.0\n",
      "6430\n",
      "1.0\n",
      "6440\n",
      "1.0\n",
      "6450\n",
      "1.0\n",
      "6460\n",
      "1.0\n",
      "6470\n",
      "1.0\n",
      "6480\n",
      "0.8\n",
      "6490\n",
      "0.8\n",
      "6500\n",
      "0.9\n",
      "6510\n",
      "0.9\n",
      "6520\n",
      "1.0\n",
      "6530\n",
      "0.5\n",
      "6540\n",
      "1.0\n",
      "6550\n",
      "1.0\n",
      "6560\n",
      "0.7\n",
      "6570\n",
      "1.0\n",
      "6580\n",
      "1.0\n",
      "6590\n",
      "1.0\n",
      "6600\n",
      "0.9\n",
      "6610\n",
      "1.0\n",
      "6620\n",
      "1.0\n",
      "6630\n",
      "0.7\n",
      "6640\n",
      "0.9\n",
      "6650\n",
      "0.6\n",
      "6660\n",
      "0.3\n",
      "6670\n",
      "0.6\n",
      "6680\n",
      "0.4\n",
      "6690\n",
      "0.6\n",
      "6700\n",
      "0.0\n",
      "6710\n",
      "1.0\n",
      "6720\n",
      "1.0\n",
      "6730\n",
      "1.0\n",
      "6740\n",
      "1.0\n",
      "6750\n",
      "1.0\n",
      "6760\n",
      "1.0\n",
      "6770\n",
      "1.0\n",
      "6780\n",
      "1.0\n",
      "6790\n",
      "1.0\n",
      "6800\n",
      "1.0\n",
      "6810\n",
      "1.0\n",
      "6820\n",
      "0.5\n",
      "6830\n",
      "1.0\n",
      "6840\n",
      "1.0\n",
      "6850\n",
      "1.0\n",
      "6860\n",
      "0.4\n",
      "6870\n",
      "0.5\n",
      "6880\n",
      "0.9\n",
      "6890\n",
      "0.0\n",
      "6900\n",
      "0.4\n",
      "6910\n",
      "1.0\n",
      "6920\n",
      "1.0\n",
      "6930\n",
      "1.0\n",
      "6940\n",
      "1.0\n",
      "6950\n",
      "1.0\n",
      "6960\n",
      "1.0\n",
      "6970\n",
      "1.0\n",
      "6980\n",
      "1.0\n",
      "6990\n",
      "1.0\n",
      "7000\n",
      "1.0\n",
      "7010\n",
      "1.0\n",
      "7020\n",
      "1.0\n",
      "7030\n",
      "1.0\n",
      "7040\n",
      "1.0\n",
      "7050\n",
      "1.0\n",
      "7060\n",
      "1.0\n",
      "7070\n",
      "1.0\n",
      "7080\n",
      "1.0\n",
      "7090\n",
      "1.0\n",
      "7100\n",
      "1.0\n",
      "7110\n",
      "1.0\n",
      "7120\n",
      "1.0\n",
      "7130\n",
      "1.0\n",
      "7140\n",
      "1.0\n",
      "7150\n",
      "1.0\n",
      "7160\n",
      "1.0\n",
      "7170\n",
      "1.0\n",
      "7180\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.78525730180806663, 0.36904516761898992)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_min_day = []\n",
    "latest_min = 60 * 30\n",
    "pred_sec = 10\n",
    "for i in range(0,9000-latest_min-pred_sec,pred_sec):\n",
    "    print i\n",
    "    data_train = data_2014_up[0][i:i+latest_min]\n",
    "    X_train = data_train.drop([\"0\"],axis=1)\n",
    "    y_train = data_train['0']\n",
    "    data_test = data_2014_up[0][i+latest_min:i+latest_min+pred_sec]\n",
    "    X_test = data_test.drop([\"0\"],axis=1)\n",
    "    y_test = data_test['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    acc = latest_day_loop_acc(X_train,y_train,X_test,y_test,model)\n",
    "    print acc\n",
    "    mean_min_day.append(acc)\n",
    "np.mean(mean_min_day),np.std(mean_min_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.6\n",
      "20\n",
      "0.45\n",
      "40\n",
      "1.0\n",
      "60\n",
      "1.0\n",
      "80\n",
      "1.0\n",
      "100\n",
      "1.0\n",
      "120\n",
      "1.0\n",
      "140\n",
      "1.0\n",
      "160\n",
      "1.0\n",
      "180\n",
      "1.0\n",
      "200\n",
      "1.0\n",
      "220\n",
      "1.0\n",
      "240\n",
      "1.0\n",
      "260\n",
      "1.0\n",
      "280\n",
      "1.0\n",
      "300\n",
      "1.0\n",
      "320\n",
      "1.0\n",
      "340\n",
      "0.85\n",
      "360\n",
      "0.3\n",
      "380\n",
      "0.05\n",
      "400\n",
      "0.0\n",
      "420\n",
      "0.0\n",
      "440\n",
      "0.05\n",
      "460\n",
      "0.0\n",
      "480\n",
      "0.0\n",
      "500\n",
      "0.4\n",
      "520\n",
      "0.0\n",
      "540\n",
      "0.0\n",
      "560\n",
      "0.0\n",
      "580\n",
      "0.0\n",
      "600\n",
      "0.0\n",
      "620\n",
      "0.0\n",
      "640\n",
      "0.25\n",
      "660\n",
      "1.0\n",
      "680\n",
      "0.75\n",
      "700\n",
      "0.7\n",
      "720\n",
      "0.5\n",
      "740\n",
      "1.0\n",
      "760\n",
      "1.0\n",
      "780\n",
      "1.0\n",
      "800\n",
      "1.0\n",
      "820\n",
      "1.0\n",
      "840\n",
      "0.55\n",
      "860\n",
      "0.3\n",
      "880\n",
      "0.45\n",
      "900\n",
      "0.0\n",
      "920\n",
      "0.25\n",
      "940\n",
      "1.0\n",
      "960\n",
      "0.3\n",
      "980\n",
      "0.0\n",
      "1000\n",
      "0.85\n",
      "1020\n",
      "0.85\n",
      "1040\n",
      "0.8\n",
      "1060\n",
      "1.0\n",
      "1080\n",
      "0.4\n",
      "1100\n",
      "0.8\n",
      "1120\n",
      "1.0\n",
      "1140\n",
      "1.0\n",
      "1160\n",
      "1.0\n",
      "1180\n",
      "1.0\n",
      "1200\n",
      "1.0\n",
      "1220\n",
      "1.0\n",
      "1240\n",
      "1.0\n",
      "1260\n",
      "1.0\n",
      "1280\n",
      "1.0\n",
      "1300\n",
      "1.0\n",
      "1320\n",
      "1.0\n",
      "1340\n",
      "1.0\n",
      "1360\n",
      "1.0\n",
      "1380\n",
      "1.0\n",
      "1400\n",
      "1.0\n",
      "1420\n",
      "1.0\n",
      "1440\n",
      "1.0\n",
      "1460\n",
      "1.0\n",
      "1480\n",
      "1.0\n",
      "1500\n",
      "1.0\n",
      "1520\n",
      "1.0\n",
      "1540\n",
      "1.0\n",
      "1560\n",
      "1.0\n",
      "1580\n",
      "1.0\n",
      "1600\n",
      "1.0\n",
      "1620\n",
      "1.0\n",
      "1640\n",
      "1.0\n",
      "1660\n",
      "1.0\n",
      "1680\n",
      "1.0\n",
      "1700\n",
      "1.0\n",
      "1720\n",
      "1.0\n",
      "1740\n",
      "1.0\n",
      "1760\n",
      "1.0\n",
      "1780\n",
      "1.0\n",
      "1800\n",
      "0.9\n",
      "1820\n",
      "1.0\n",
      "1840\n",
      "0.3\n",
      "1860\n",
      "1.0\n",
      "1880\n",
      "1.0\n",
      "1900\n",
      "0.9\n",
      "1920\n",
      "0.95\n",
      "1940\n",
      "1.0\n",
      "1960\n",
      "0.8\n",
      "1980\n",
      "0.9\n",
      "2000\n",
      "0.5\n",
      "2020\n",
      "0.85\n",
      "2040\n",
      "0.5\n",
      "2060\n",
      "0.75\n",
      "2080\n",
      "1.0\n",
      "2100\n",
      "1.0\n",
      "2120\n",
      "0.8\n",
      "2140\n",
      "1.0\n",
      "2160\n",
      "1.0\n",
      "2180\n",
      "1.0\n",
      "2200\n",
      "0.95\n",
      "2220\n",
      "1.0\n",
      "2240\n",
      "1.0\n",
      "2260\n",
      "0.3\n",
      "2280\n",
      "0.6\n",
      "2300\n",
      "0.05\n",
      "2320\n",
      "0.0\n",
      "2340\n",
      "0.0\n",
      "2360\n",
      "0.15\n",
      "2380\n",
      "0.0\n",
      "2400\n",
      "0.0\n",
      "2420\n",
      "0.4\n",
      "2440\n",
      "1.0\n",
      "2460\n",
      "0.1\n",
      "2480\n",
      "0.0\n",
      "2500\n",
      "0.0\n",
      "2520\n",
      "0.0\n",
      "2540\n",
      "0.0\n",
      "2560\n",
      "0.0\n",
      "2580\n",
      "0.0\n",
      "2600\n",
      "0.15\n",
      "2620\n",
      "0.1\n",
      "2640\n",
      "0.0\n",
      "2660\n",
      "0.0\n",
      "2680\n",
      "0.0\n",
      "2700\n",
      "0.0\n",
      "2720\n",
      "0.0\n",
      "2740\n",
      "0.0\n",
      "2760\n",
      "0.0\n",
      "2780\n",
      "0.0\n",
      "2800\n",
      "0.0\n",
      "2820\n",
      "0.05\n",
      "2840\n",
      "0.0\n",
      "2860\n",
      "0.0\n",
      "2880\n",
      "0.6\n",
      "2900\n",
      "0.1\n",
      "2920\n",
      "0.65\n",
      "2940\n",
      "0.6\n",
      "2960\n",
      "1.0\n",
      "2980\n",
      "1.0\n",
      "3000\n",
      "1.0\n",
      "3020\n",
      "1.0\n",
      "3040\n",
      "1.0\n",
      "3060\n",
      "1.0\n",
      "3080\n",
      "1.0\n",
      "3100\n",
      "1.0\n",
      "3120\n",
      "1.0\n",
      "3140\n",
      "1.0\n",
      "3160\n",
      "1.0\n",
      "3180\n",
      "1.0\n",
      "3200\n",
      "1.0\n",
      "3220\n",
      "1.0\n",
      "3240\n",
      "1.0\n",
      "3260\n",
      "1.0\n",
      "3280\n",
      "1.0\n",
      "3300\n",
      "1.0\n",
      "3320\n",
      "1.0\n",
      "3340\n",
      "1.0\n",
      "3360\n",
      "1.0\n",
      "3380\n",
      "1.0\n",
      "3400\n",
      "1.0\n",
      "3420\n",
      "1.0\n",
      "3440\n",
      "1.0\n",
      "3460\n",
      "1.0\n",
      "3480\n",
      "1.0\n",
      "3500\n",
      "1.0\n",
      "3520\n",
      "1.0\n",
      "3540\n",
      "1.0\n",
      "3560\n",
      "1.0\n",
      "3580\n",
      "1.0\n",
      "3600\n",
      "1.0\n",
      "3620\n",
      "1.0\n",
      "3640\n",
      "1.0\n",
      "3660\n",
      "1.0\n",
      "3680\n",
      "1.0\n",
      "3700\n",
      "1.0\n",
      "3720\n",
      "1.0\n",
      "3740\n",
      "1.0\n",
      "3760\n",
      "1.0\n",
      "3780\n",
      "1.0\n",
      "3800\n",
      "1.0\n",
      "3820\n",
      "1.0\n",
      "3840\n",
      "1.0\n",
      "3860\n",
      "1.0\n",
      "3880\n",
      "1.0\n",
      "3900\n",
      "1.0\n",
      "3920\n",
      "1.0\n",
      "3940\n",
      "1.0\n",
      "3960\n",
      "1.0\n",
      "3980\n",
      "0.05\n",
      "4000\n",
      "0.3\n",
      "4020\n",
      "0.15\n",
      "4040\n",
      "0.1\n",
      "4060\n",
      "0.9\n",
      "4080\n",
      "0.3\n",
      "4100\n",
      "0.0\n",
      "4120\n",
      "0.15\n",
      "4140\n",
      "1.0\n",
      "4160\n",
      "0.6\n",
      "4180\n",
      "1.0\n",
      "4200\n",
      "1.0\n",
      "4220\n",
      "0.7\n",
      "4240\n",
      "0.75\n",
      "4260\n",
      "0.0\n",
      "4280\n",
      "0.4\n",
      "4300\n",
      "0.95\n",
      "4320\n",
      "0.3\n",
      "4340\n",
      "0.0\n",
      "4360\n",
      "0.0\n",
      "4380\n",
      "0.05\n",
      "4400\n",
      "0.3\n",
      "4420\n",
      "0.5\n",
      "4440\n",
      "0.35\n",
      "4460\n",
      "0.0\n",
      "4480\n",
      "0.05\n",
      "4500\n",
      "0.5\n",
      "4520\n",
      "1.0\n",
      "4540\n",
      "1.0\n",
      "4560\n",
      "1.0\n",
      "4580\n",
      "1.0\n",
      "4600\n",
      "1.0\n",
      "4620\n",
      "1.0\n",
      "4640\n",
      "1.0\n",
      "4660\n",
      "1.0\n",
      "4680\n",
      "1.0\n",
      "4700\n",
      "1.0\n",
      "4720\n",
      "1.0\n",
      "4740\n",
      "1.0\n",
      "4760\n",
      "1.0\n",
      "4780\n",
      "1.0\n",
      "4800\n",
      "1.0\n",
      "4820\n",
      "1.0\n",
      "4840\n",
      "1.0\n",
      "4860\n",
      "1.0\n",
      "4880\n",
      "1.0\n",
      "4900\n",
      "1.0\n",
      "4920\n",
      "1.0\n",
      "4940\n",
      "1.0\n",
      "4960\n",
      "1.0\n",
      "4980\n",
      "1.0\n",
      "5000\n",
      "1.0\n",
      "5020\n",
      "1.0\n",
      "5040\n",
      "1.0\n",
      "5060\n",
      "1.0\n",
      "5080\n",
      "1.0\n",
      "5100\n",
      "0.5\n",
      "5120\n",
      "0.0\n",
      "5140\n",
      "0.0\n",
      "5160\n",
      "0.0\n",
      "5180\n",
      "0.0\n",
      "5200\n",
      "0.0\n",
      "5220\n",
      "0.0\n",
      "5240\n",
      "0.0\n",
      "5260\n",
      "0.25\n",
      "5280\n",
      "1.0\n",
      "5300\n",
      "1.0\n",
      "5320\n",
      "1.0\n",
      "5340\n",
      "0.9\n",
      "5360\n",
      "1.0\n",
      "5380\n",
      "1.0\n",
      "5400\n",
      "1.0\n",
      "5420\n",
      "1.0\n",
      "5440\n",
      "1.0\n",
      "5460\n",
      "1.0\n",
      "5480\n",
      "1.0\n",
      "5500\n",
      "0.7\n",
      "5520\n",
      "1.0\n",
      "5540\n",
      "0.05\n",
      "5560\n",
      "0.0\n",
      "5580\n",
      "0.4\n",
      "5600\n",
      "0.4\n",
      "5620\n",
      "0.1\n",
      "5640\n",
      "0.0\n",
      "5660\n",
      "0.5\n",
      "5680\n",
      "1.0\n",
      "5700\n",
      "1.0\n",
      "5720\n",
      "0.95\n",
      "5740\n",
      "0.85\n",
      "5760\n",
      "1.0\n",
      "5780\n",
      "1.0\n",
      "5800\n",
      "0.9\n",
      "5820\n",
      "0.95\n",
      "5840\n",
      "0.95\n",
      "5860\n",
      "0.15\n",
      "5880\n",
      "0.4\n",
      "5900\n",
      "1.0\n",
      "5920\n",
      "0.95\n",
      "5940\n",
      "1.0\n",
      "5960\n",
      "1.0\n",
      "5980\n",
      "1.0\n",
      "6000\n",
      "1.0\n",
      "6020\n",
      "1.0\n",
      "6040\n",
      "0.9\n",
      "6060\n",
      "0.4\n",
      "6080\n",
      "0.5\n",
      "6100\n",
      "0.9\n",
      "6120\n",
      "0.85\n",
      "6140\n",
      "0.85\n",
      "6160\n",
      "0.75\n",
      "6180\n",
      "0.7\n",
      "6200\n",
      "0.55\n",
      "6220\n",
      "0.0\n",
      "6240\n",
      "0.0\n",
      "6260\n",
      "0.55\n",
      "6280\n",
      "0.5\n",
      "6300\n",
      "0.75\n",
      "6320\n",
      "1.0\n",
      "6340\n",
      "1.0\n",
      "6360\n",
      "1.0\n",
      "6380\n",
      "1.0\n",
      "6400\n",
      "1.0\n",
      "6420\n",
      "1.0\n",
      "6440\n",
      "1.0\n",
      "6460\n",
      "1.0\n",
      "6480\n",
      "1.0\n",
      "6500\n",
      "1.0\n",
      "6520\n",
      "1.0\n",
      "6540\n",
      "1.0\n",
      "6560\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.71398176291793314, 0.39695146775457985)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_min_day = []\n",
    "latest_min = 60 * 40\n",
    "pred_sec = 20\n",
    "for i in range(0,9000-latest_min-pred_sec,pred_sec):\n",
    "    print i\n",
    "    data_train = data_2014_up[0][i:i+latest_min]\n",
    "    X_train = data_train.drop([\"0\"],axis=1)\n",
    "    y_train = data_train['0']\n",
    "    data_test = data_2014_up[0][i+latest_min:i+latest_min+pred_sec]\n",
    "    X_test = data_test.drop([\"0\"],axis=1)\n",
    "    y_test = data_test['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    acc = latest_day_loop_acc(X_train,y_train,X_test,y_test,model)\n",
    "    print acc\n",
    "    mean_min_day.append(acc)\n",
    "np.mean(mean_min_day),np.std(mean_min_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1.0\n",
      "5\n",
      "1.0\n",
      "10\n",
      "0.4\n",
      "15\n",
      "0.0\n",
      "20\n",
      "0.4\n",
      "25\n",
      "0.0\n",
      "30\n",
      "0.8\n",
      "35\n",
      "1.0\n",
      "40\n",
      "1.0\n",
      "45\n",
      "1.0\n",
      "50\n",
      "1.0\n",
      "55\n",
      "1.0\n",
      "60\n",
      "1.0\n",
      "65\n",
      "1.0\n",
      "70\n",
      "1.0\n",
      "75\n",
      "1.0\n",
      "80\n",
      "1.0\n",
      "85\n",
      "1.0\n",
      "90\n",
      "1.0\n",
      "95\n",
      "1.0\n",
      "100\n",
      "1.0\n",
      "105\n",
      "1.0\n",
      "110\n",
      "1.0\n",
      "115\n",
      "1.0\n",
      "120\n",
      "1.0\n",
      "125\n",
      "1.0\n",
      "130\n",
      "1.0\n",
      "135\n",
      "1.0\n",
      "140\n",
      "1.0\n",
      "145\n",
      "1.0\n",
      "150\n",
      "1.0\n",
      "155\n",
      "1.0\n",
      "160\n",
      "1.0\n",
      "165\n",
      "1.0\n",
      "170\n",
      "1.0\n",
      "175\n",
      "1.0\n",
      "180\n",
      "1.0\n",
      "185\n",
      "1.0\n",
      "190\n",
      "1.0\n",
      "195\n",
      "1.0\n",
      "200\n",
      "1.0\n",
      "205\n",
      "1.0\n",
      "210\n",
      "1.0\n",
      "215\n",
      "1.0\n",
      "220\n",
      "1.0\n",
      "225\n",
      "1.0\n",
      "230\n",
      "1.0\n",
      "235\n",
      "1.0\n",
      "240\n",
      "1.0\n",
      "245\n",
      "1.0\n",
      "250\n",
      "1.0\n",
      "255\n",
      "1.0\n",
      "260\n",
      "1.0\n",
      "265\n",
      "1.0\n",
      "270\n",
      "1.0\n",
      "275\n",
      "1.0\n",
      "280\n",
      "1.0\n",
      "285\n",
      "1.0\n",
      "290\n",
      "1.0\n",
      "295\n",
      "1.0\n",
      "300\n",
      "1.0\n",
      "305\n",
      "1.0\n",
      "310\n",
      "1.0\n",
      "315\n",
      "1.0\n",
      "320\n",
      "1.0\n",
      "325\n",
      "1.0\n",
      "330\n",
      "1.0\n",
      "335\n",
      "1.0\n",
      "340\n",
      "1.0\n",
      "345\n",
      "1.0\n",
      "350\n",
      "1.0\n",
      "355\n",
      "0.6\n",
      "360\n",
      "0.2\n",
      "365\n",
      "0.4\n",
      "370\n",
      "0.0\n",
      "375\n",
      "1.0\n",
      "380\n",
      "0.2\n",
      "385\n",
      "0.0\n",
      "390\n",
      "0.0\n",
      "395\n",
      "0.4\n",
      "400\n",
      "0.0\n",
      "405\n",
      "0.0\n",
      "410\n",
      "0.0\n",
      "415\n",
      "0.0\n",
      "420\n",
      "0.0\n",
      "425\n",
      "0.0\n",
      "430\n",
      "0.0\n",
      "435\n",
      "0.0\n",
      "440\n",
      "0.0\n",
      "445\n",
      "0.2\n",
      "450\n",
      "0.4\n",
      "455\n",
      "0.0\n",
      "460\n",
      "0.0\n",
      "465\n",
      "0.0\n",
      "470\n",
      "0.0\n",
      "475\n",
      "0.0\n",
      "480\n",
      "0.0\n",
      "485\n",
      "0.0\n",
      "490\n",
      "0.0\n",
      "495\n",
      "0.8\n",
      "500\n",
      "1.0\n",
      "505\n",
      "0.6\n",
      "510\n",
      "0.0\n",
      "515\n",
      "0.0\n",
      "520\n",
      "0.0\n",
      "525\n",
      "0.0\n",
      "530\n",
      "0.0\n",
      "535\n",
      "0.0\n",
      "540\n",
      "0.0\n",
      "545\n",
      "0.0\n",
      "550\n",
      "0.0\n",
      "555\n",
      "0.0\n",
      "560\n",
      "0.0\n",
      "565\n",
      "0.0\n",
      "570\n",
      "0.0\n",
      "575\n",
      "0.0\n",
      "580\n",
      "0.0\n",
      "585\n",
      "0.0\n",
      "590\n",
      "0.0\n",
      "595\n",
      "0.0\n",
      "600\n",
      "0.0\n",
      "605\n",
      "0.0\n",
      "610\n",
      "0.0\n",
      "615\n",
      "0.0\n",
      "620\n",
      "0.0\n",
      "625\n",
      "0.0\n",
      "630\n",
      "0.2\n",
      "635\n",
      "1.0\n",
      "640\n",
      "0.8\n",
      "645\n",
      "0.0\n",
      "650\n",
      "0.4\n",
      "655\n",
      "0.4\n",
      "660\n",
      "1.0\n",
      "665\n",
      "1.0\n",
      "670\n",
      "1.0\n",
      "675\n",
      "1.0\n",
      "680\n",
      "0.4\n",
      "685\n",
      "1.0\n",
      "690\n",
      "1.0\n",
      "695\n",
      "1.0\n",
      "700\n",
      "1.0\n",
      "705\n",
      "1.0\n",
      "710\n",
      "0.6\n",
      "715\n",
      "1.0\n",
      "720\n",
      "0.8\n",
      "725\n",
      "1.0\n",
      "730\n",
      "1.0\n",
      "735\n",
      "0.0\n",
      "740\n",
      "1.0\n",
      "745\n",
      "1.0\n",
      "750\n",
      "1.0\n",
      "755\n",
      "1.0\n",
      "760\n",
      "1.0\n",
      "765\n",
      "1.0\n",
      "770\n",
      "1.0\n",
      "775\n",
      "1.0\n",
      "780\n",
      "1.0\n",
      "785\n",
      "1.0\n",
      "790\n",
      "1.0\n",
      "795\n",
      "1.0\n",
      "800\n",
      "1.0\n",
      "805\n",
      "1.0\n",
      "810\n",
      "1.0\n",
      "815\n",
      "1.0\n",
      "820\n",
      "1.0\n",
      "825\n",
      "1.0\n",
      "830\n",
      "1.0\n",
      "835\n",
      "1.0\n",
      "840\n",
      "0.6\n",
      "845\n",
      "0.4\n",
      "850\n",
      "1.0\n",
      "855\n",
      "0.2\n",
      "860\n",
      "0.6\n",
      "865\n",
      "0.4\n",
      "870\n",
      "0.0\n",
      "875\n",
      "0.4\n",
      "880\n",
      "1.0\n",
      "885\n",
      "0.8\n",
      "890\n",
      "0.0\n",
      "895\n",
      "0.0\n",
      "900\n",
      "0.0\n",
      "905\n",
      "0.0\n",
      "910\n",
      "0.0\n",
      "915\n",
      "0.0\n",
      "920\n",
      "0.0\n",
      "925\n",
      "0.6\n",
      "930\n",
      "1.0\n",
      "935\n",
      "1.0\n",
      "940\n",
      "1.0\n",
      "945\n",
      "1.0\n",
      "950\n",
      "1.0\n",
      "955\n",
      "1.0\n",
      "960\n",
      "0.8\n",
      "965\n",
      "0.4\n",
      "970\n",
      "0.0\n",
      "975\n",
      "0.4\n",
      "980\n",
      "0.0\n",
      "985\n",
      "0.0\n",
      "990\n",
      "0.0\n",
      "995\n",
      "0.0\n",
      "1000\n",
      "1.0\n",
      "1005\n",
      "0.6\n",
      "1010\n",
      "0.8\n",
      "1015\n",
      "1.0\n",
      "1020\n",
      "1.0\n",
      "1025\n",
      "1.0\n",
      "1030\n",
      "1.0\n",
      "1035\n",
      "0.4\n",
      "1040\n",
      "0.2\n",
      "1045\n",
      "1.0\n",
      "1050\n",
      "1.0\n",
      "1055\n",
      "1.0\n",
      "1060\n",
      "1.0\n",
      "1065\n",
      "1.0\n",
      "1070\n",
      "1.0\n",
      "1075\n",
      "1.0\n",
      "1080\n",
      "1.0\n",
      "1085\n",
      "0.4\n",
      "1090\n",
      "0.0\n",
      "1095\n",
      "0.2\n",
      "1100\n",
      "0.2\n",
      "1105\n",
      "1.0\n",
      "1110\n",
      "1.0\n",
      "1115\n",
      "1.0\n",
      "1120\n",
      "1.0\n",
      "1125\n",
      "1.0\n",
      "1130\n",
      "1.0\n",
      "1135\n",
      "1.0\n",
      "1140\n",
      "1.0\n",
      "1145\n",
      "1.0\n",
      "1150\n",
      "1.0\n",
      "1155\n",
      "1.0\n",
      "1160\n",
      "1.0\n",
      "1165\n",
      "1.0\n",
      "1170\n",
      "1.0\n",
      "1175\n",
      "1.0\n",
      "1180\n",
      "1.0\n",
      "1185\n",
      "1.0\n",
      "1190\n",
      "1.0\n",
      "1195\n",
      "1.0\n",
      "1200\n",
      "1.0\n",
      "1205\n",
      "1.0\n",
      "1210\n",
      "1.0\n",
      "1215\n",
      "1.0\n",
      "1220\n",
      "1.0\n",
      "1225\n",
      "1.0\n",
      "1230\n",
      "1.0\n",
      "1235\n",
      "1.0\n",
      "1240\n",
      "1.0\n",
      "1245\n",
      "1.0\n",
      "1250\n",
      "1.0\n",
      "1255\n",
      "1.0\n",
      "1260\n",
      "1.0\n",
      "1265\n",
      "1.0\n",
      "1270\n",
      "1.0\n",
      "1275\n",
      "1.0\n",
      "1280\n",
      "1.0\n",
      "1285\n",
      "1.0\n",
      "1290\n",
      "1.0\n",
      "1295\n",
      "1.0\n",
      "1300\n",
      "1.0\n",
      "1305\n",
      "1.0\n",
      "1310\n",
      "1.0\n",
      "1315\n",
      "1.0\n",
      "1320\n",
      "1.0\n",
      "1325\n",
      "1.0\n",
      "1330\n",
      "1.0\n",
      "1335\n",
      "1.0\n",
      "1340\n",
      "1.0\n",
      "1345\n",
      "1.0\n",
      "1350\n",
      "1.0\n",
      "1355\n",
      "1.0\n",
      "1360\n",
      "1.0\n",
      "1365\n",
      "1.0\n",
      "1370\n",
      "1.0\n",
      "1375\n",
      "1.0\n",
      "1380\n",
      "1.0\n",
      "1385\n",
      "1.0\n",
      "1390\n",
      "1.0\n",
      "1395\n",
      "1.0\n",
      "1400\n",
      "1.0\n",
      "1405\n",
      "1.0\n",
      "1410\n",
      "1.0\n",
      "1415\n",
      "1.0\n",
      "1420\n",
      "1.0\n",
      "1425\n",
      "1.0\n",
      "1430\n",
      "1.0\n",
      "1435\n",
      "1.0\n",
      "1440\n",
      "1.0\n",
      "1445\n",
      "1.0\n",
      "1450\n",
      "1.0\n",
      "1455\n",
      "1.0\n",
      "1460\n",
      "1.0\n",
      "1465\n",
      "1.0\n",
      "1470\n",
      "1.0\n",
      "1475\n",
      "1.0\n",
      "1480\n",
      "1.0\n",
      "1485\n",
      "1.0\n",
      "1490\n",
      "1.0\n",
      "1495\n",
      "1.0\n",
      "1500\n",
      "1.0\n",
      "1505\n",
      "1.0\n",
      "1510\n",
      "1.0\n",
      "1515\n",
      "1.0\n",
      "1520\n",
      "1.0\n",
      "1525\n",
      "1.0\n",
      "1530\n",
      "1.0\n",
      "1535\n",
      "1.0\n",
      "1540\n",
      "1.0\n",
      "1545\n",
      "1.0\n",
      "1550\n",
      "1.0\n",
      "1555\n",
      "1.0\n",
      "1560\n",
      "1.0\n",
      "1565\n",
      "1.0\n",
      "1570\n",
      "1.0\n",
      "1575\n",
      "1.0\n",
      "1580\n",
      "1.0\n",
      "1585\n",
      "1.0\n",
      "1590\n",
      "1.0\n",
      "1595\n",
      "1.0\n",
      "1600\n",
      "1.0\n",
      "1605\n",
      "1.0\n",
      "1610\n",
      "1.0\n",
      "1615\n",
      "1.0\n",
      "1620\n",
      "1.0\n",
      "1625\n",
      "1.0\n",
      "1630\n",
      "1.0\n",
      "1635\n",
      "1.0\n",
      "1640\n",
      "1.0\n",
      "1645\n",
      "1.0\n",
      "1650\n",
      "1.0\n",
      "1655\n",
      "1.0\n",
      "1660\n",
      "1.0\n",
      "1665\n",
      "1.0\n",
      "1670\n",
      "1.0\n",
      "1675\n",
      "1.0\n",
      "1680\n",
      "1.0\n",
      "1685\n",
      "1.0\n",
      "1690\n",
      "1.0\n",
      "1695\n",
      "1.0\n",
      "1700\n",
      "1.0\n",
      "1705\n",
      "1.0\n",
      "1710\n",
      "1.0\n",
      "1715\n",
      "1.0\n",
      "1720\n",
      "1.0\n",
      "1725\n",
      "1.0\n",
      "1730\n",
      "1.0\n",
      "1735\n",
      "1.0\n",
      "1740\n",
      "1.0\n",
      "1745\n",
      "1.0\n",
      "1750\n",
      "1.0\n",
      "1755\n",
      "1.0\n",
      "1760\n",
      "1.0\n",
      "1765\n",
      "1.0\n",
      "1770\n",
      "1.0\n",
      "1775\n",
      "1.0\n",
      "1780\n",
      "1.0\n",
      "1785\n",
      "1.0\n",
      "1790\n",
      "1.0\n",
      "1795\n",
      "1.0\n",
      "1800\n",
      "1.0\n",
      "1805\n",
      "0.8\n",
      "1810\n",
      "1.0\n",
      "1815\n",
      "0.8\n",
      "1820\n",
      "1.0\n",
      "1825\n",
      "1.0\n",
      "1830\n",
      "1.0\n",
      "1835\n",
      "1.0\n",
      "1840\n",
      "0.2\n",
      "1845\n",
      "0.0\n",
      "1850\n",
      "0.8\n",
      "1855\n",
      "0.8\n",
      "1860\n",
      "1.0\n",
      "1865\n",
      "1.0\n",
      "1870\n",
      "1.0\n",
      "1875\n",
      "1.0\n",
      "1880\n",
      "1.0\n",
      "1885\n",
      "1.0\n",
      "1890\n",
      "1.0\n",
      "1895\n",
      "1.0\n",
      "1900\n",
      "1.0\n",
      "1905\n",
      "1.0\n",
      "1910\n",
      "1.0\n",
      "1915\n",
      "0.6\n",
      "1920\n",
      "0.8\n",
      "1925\n",
      "1.0\n",
      "1930\n",
      "1.0\n",
      "1935\n",
      "1.0\n",
      "1940\n",
      "1.0\n",
      "1945\n",
      "1.0\n",
      "1950\n",
      "1.0\n",
      "1955\n",
      "1.0\n",
      "1960\n",
      "0.4\n",
      "1965\n",
      "1.0\n",
      "1970\n",
      "1.0\n",
      "1975\n",
      "0.8\n",
      "1980\n",
      "1.0\n",
      "1985\n",
      "1.0\n",
      "1990\n",
      "1.0\n",
      "1995\n",
      "1.0\n",
      "2000\n",
      "1.0\n",
      "2005\n",
      "0.6\n",
      "2010\n",
      "0.6\n",
      "2015\n",
      "0.4\n",
      "2020\n",
      "0.6\n",
      "2025\n",
      "0.8\n",
      "2030\n",
      "1.0\n",
      "2035\n",
      "1.0\n",
      "2040\n",
      "1.0\n",
      "2045\n",
      "0.6\n",
      "2050\n",
      "0.8\n",
      "2055\n",
      "1.0\n",
      "2060\n",
      "0.8\n",
      "2065\n",
      "0.8\n",
      "2070\n",
      "1.0\n",
      "2075\n",
      "1.0\n",
      "2080\n",
      "1.0\n",
      "2085\n",
      "1.0\n",
      "2090\n",
      "1.0\n",
      "2095\n",
      "1.0\n",
      "2100\n",
      "1.0\n",
      "2105\n",
      "1.0\n",
      "2110\n",
      "1.0\n",
      "2115\n",
      "1.0\n",
      "2120\n",
      "1.0\n",
      "2125\n",
      "1.0\n",
      "2130\n",
      "0.6\n",
      "2135\n",
      "0.6\n",
      "2140\n",
      "1.0\n",
      "2145\n",
      "1.0\n",
      "2150\n",
      "1.0\n",
      "2155\n",
      "1.0\n",
      "2160\n",
      "1.0\n",
      "2165\n",
      "1.0\n",
      "2170\n",
      "1.0\n",
      "2175\n",
      "1.0\n",
      "2180\n",
      "1.0\n",
      "2185\n",
      "1.0\n",
      "2190\n",
      "1.0\n",
      "2195\n",
      "1.0\n",
      "2200\n",
      "1.0\n",
      "2205\n",
      "0.8\n",
      "2210\n",
      "1.0\n",
      "2215\n",
      "1.0\n",
      "2220\n",
      "1.0\n",
      "2225\n",
      "1.0\n",
      "2230\n",
      "1.0\n",
      "2235\n",
      "1.0\n",
      "2240\n",
      "1.0\n",
      "2245\n",
      "1.0\n",
      "2250\n",
      "1.0\n",
      "2255\n",
      "1.0\n",
      "2260\n",
      "1.0\n",
      "2265\n",
      "0.2\n",
      "2270\n",
      "0.0\n",
      "2275\n",
      "0.0\n",
      "2280\n",
      "0.0\n",
      "2285\n",
      "0.4\n",
      "2290\n",
      "1.0\n",
      "2295\n",
      "1.0\n",
      "2300\n",
      "0.2\n",
      "2305\n",
      "0.0\n",
      "2310\n",
      "0.0\n",
      "2315\n",
      "0.0\n",
      "2320\n",
      "0.0\n",
      "2325\n",
      "0.0\n",
      "2330\n",
      "0.0\n",
      "2335\n",
      "0.0\n",
      "2340\n",
      "0.0\n",
      "2345\n",
      "0.0\n",
      "2350\n",
      "0.0\n",
      "2355\n",
      "0.0\n",
      "2360\n",
      "0.0\n",
      "2365\n",
      "0.0\n",
      "2370\n",
      "0.6\n",
      "2375\n",
      "0.0\n",
      "2380\n",
      "0.0\n",
      "2385\n",
      "0.0\n",
      "2390\n",
      "0.0\n",
      "2395\n",
      "0.0\n",
      "2400\n",
      "0.0\n",
      "2405\n",
      "0.0\n",
      "2410\n",
      "0.0\n",
      "2415\n",
      "0.0\n",
      "2420\n",
      "0.0\n",
      "2425\n",
      "0.0\n",
      "2430\n",
      "0.6\n",
      "2435\n",
      "1.0\n",
      "2440\n",
      "1.0\n",
      "2445\n",
      "1.0\n",
      "2450\n",
      "1.0\n",
      "2455\n",
      "1.0\n",
      "2460\n",
      "0.4\n",
      "2465\n",
      "0.0\n",
      "2470\n",
      "0.0\n",
      "2475\n",
      "0.0\n",
      "2480\n",
      "0.0\n",
      "2485\n",
      "0.0\n",
      "2490\n",
      "0.0\n",
      "2495\n",
      "0.0\n",
      "2500\n",
      "0.0\n",
      "2505\n",
      "0.0\n",
      "2510\n",
      "0.0\n",
      "2515\n",
      "0.0\n",
      "2520\n",
      "0.0\n",
      "2525\n",
      "0.0\n",
      "2530\n",
      "0.0\n",
      "2535\n",
      "0.0\n",
      "2540\n",
      "0.0\n",
      "2545\n",
      "0.0\n",
      "2550\n",
      "0.0\n",
      "2555\n",
      "0.0\n",
      "2560\n",
      "0.0\n",
      "2565\n",
      "0.0\n",
      "2570\n",
      "0.0\n",
      "2575\n",
      "0.0\n",
      "2580\n",
      "0.0\n",
      "2585\n",
      "0.0\n",
      "2590\n",
      "0.0\n",
      "2595\n",
      "0.0\n",
      "2600\n",
      "0.0\n",
      "2605\n",
      "0.0\n",
      "2610\n",
      "0.0\n",
      "2615\n",
      "0.6\n",
      "2620\n",
      "0.4\n",
      "2625\n",
      "0.0\n",
      "2630\n",
      "0.0\n",
      "2635\n",
      "0.0\n",
      "2640\n",
      "0.0\n",
      "2645\n",
      "0.0\n",
      "2650\n",
      "0.0\n",
      "2655\n",
      "0.0\n",
      "2660\n",
      "0.0\n",
      "2665\n",
      "0.0\n",
      "2670\n",
      "0.0\n",
      "2675\n",
      "0.0\n",
      "2680\n",
      "0.0\n",
      "2685\n",
      "0.0\n",
      "2690\n",
      "0.0\n",
      "2695\n",
      "0.0\n",
      "2700\n",
      "0.0\n",
      "2705\n",
      "0.0\n",
      "2710\n",
      "0.0\n",
      "2715\n",
      "0.0\n",
      "2720\n",
      "0.0\n",
      "2725\n",
      "0.0\n",
      "2730\n",
      "0.0\n",
      "2735\n",
      "0.0\n",
      "2740\n",
      "0.0\n",
      "2745\n",
      "0.0\n",
      "2750\n",
      "0.0\n",
      "2755\n",
      "0.0\n",
      "2760\n",
      "0.0\n",
      "2765\n",
      "0.0\n",
      "2770\n",
      "0.0\n",
      "2775\n",
      "0.0\n",
      "2780\n",
      "0.0\n",
      "2785\n",
      "0.0\n",
      "2790\n",
      "0.0\n",
      "2795\n",
      "0.0\n",
      "2800\n",
      "0.0\n",
      "2805\n",
      "0.0\n",
      "2810\n",
      "0.0\n",
      "2815\n",
      "0.0\n",
      "2820\n",
      "0.0\n",
      "2825\n",
      "0.0\n",
      "2830\n",
      "0.0\n",
      "2835\n",
      "0.2\n",
      "2840\n",
      "0.0\n",
      "2845\n",
      "0.0\n",
      "2850\n",
      "0.0\n",
      "2855\n",
      "0.0\n",
      "2860\n",
      "0.0\n",
      "2865\n",
      "0.0\n",
      "2870\n",
      "0.0\n",
      "2875\n",
      "0.0\n",
      "2880\n",
      "0.0\n",
      "2885\n",
      "0.4\n",
      "2890\n",
      "1.0\n",
      "2895\n",
      "1.0\n",
      "2900\n",
      "0.4\n",
      "2905\n",
      "0.0\n",
      "2910\n",
      "0.0\n",
      "2915\n",
      "0.0\n",
      "2920\n",
      "0.8\n",
      "2925\n",
      "1.0\n",
      "2930\n",
      "1.0\n",
      "2935\n",
      "1.0\n",
      "2940\n",
      "1.0\n",
      "2945\n",
      "1.0\n",
      "2950\n",
      "0.2\n",
      "2955\n",
      "0.4\n",
      "2960\n",
      "1.0\n",
      "2965\n",
      "1.0\n",
      "2970\n",
      "1.0\n",
      "2975\n",
      "1.0\n",
      "2980\n",
      "1.0\n",
      "2985\n",
      "1.0\n",
      "2990\n",
      "1.0\n",
      "2995\n",
      "1.0\n",
      "3000\n",
      "1.0\n",
      "3005\n",
      "1.0\n",
      "3010\n",
      "1.0\n",
      "3015\n",
      "1.0\n",
      "3020\n",
      "1.0\n",
      "3025\n",
      "1.0\n",
      "3030\n",
      "1.0\n",
      "3035\n",
      "1.0\n",
      "3040\n",
      "1.0\n",
      "3045\n",
      "1.0\n",
      "3050\n",
      "1.0\n",
      "3055\n",
      "1.0\n",
      "3060\n",
      "1.0\n",
      "3065\n",
      "1.0\n",
      "3070\n",
      "1.0\n",
      "3075\n",
      "1.0\n",
      "3080\n",
      "1.0\n",
      "3085\n",
      "1.0\n",
      "3090\n",
      "1.0\n",
      "3095\n",
      "1.0\n",
      "3100\n",
      "1.0\n",
      "3105\n",
      "1.0\n",
      "3110\n",
      "1.0\n",
      "3115\n",
      "1.0\n",
      "3120\n",
      "1.0\n",
      "3125\n",
      "1.0\n",
      "3130\n",
      "1.0\n",
      "3135\n",
      "1.0\n",
      "3140\n",
      "1.0\n",
      "3145\n",
      "1.0\n",
      "3150\n",
      "1.0\n",
      "3155\n",
      "1.0\n",
      "3160\n",
      "1.0\n",
      "3165\n",
      "1.0\n",
      "3170\n",
      "1.0\n",
      "3175\n",
      "1.0\n",
      "3180\n",
      "1.0\n",
      "3185\n",
      "1.0\n",
      "3190\n",
      "1.0\n",
      "3195\n",
      "1.0\n",
      "3200\n",
      "1.0\n",
      "3205\n",
      "1.0\n",
      "3210\n",
      "1.0\n",
      "3215\n",
      "1.0\n",
      "3220\n",
      "1.0\n",
      "3225\n",
      "1.0\n",
      "3230\n",
      "1.0\n",
      "3235\n",
      "1.0\n",
      "3240\n",
      "1.0\n",
      "3245\n",
      "1.0\n",
      "3250\n",
      "1.0\n",
      "3255\n",
      "1.0\n",
      "3260\n",
      "1.0\n",
      "3265\n",
      "1.0\n",
      "3270\n",
      "1.0\n",
      "3275\n",
      "1.0\n",
      "3280\n",
      "1.0\n",
      "3285\n",
      "1.0\n",
      "3290\n",
      "1.0\n",
      "3295\n",
      "1.0\n",
      "3300\n",
      "1.0\n",
      "3305\n",
      "1.0\n",
      "3310\n",
      "1.0\n",
      "3315\n",
      "1.0\n",
      "3320\n",
      "1.0\n",
      "3325\n",
      "1.0\n",
      "3330\n",
      "1.0\n",
      "3335\n",
      "1.0\n",
      "3340\n",
      "1.0\n",
      "3345\n",
      "1.0\n",
      "3350\n",
      "1.0\n",
      "3355\n",
      "1.0\n",
      "3360\n",
      "1.0\n",
      "3365\n",
      "1.0\n",
      "3370\n",
      "1.0\n",
      "3375\n",
      "1.0\n",
      "3380\n",
      "1.0\n",
      "3385\n",
      "1.0\n",
      "3390\n",
      "1.0\n",
      "3395\n",
      "1.0\n",
      "3400\n",
      "1.0\n",
      "3405\n",
      "1.0\n",
      "3410\n",
      "1.0\n",
      "3415\n",
      "1.0\n",
      "3420\n",
      "1.0\n",
      "3425\n",
      "1.0\n",
      "3430\n",
      "1.0\n",
      "3435\n",
      "1.0\n",
      "3440\n",
      "1.0\n",
      "3445\n",
      "1.0\n",
      "3450\n",
      "1.0\n",
      "3455\n",
      "1.0\n",
      "3460\n",
      "1.0\n",
      "3465\n",
      "1.0\n",
      "3470\n",
      "1.0\n",
      "3475\n",
      "1.0\n",
      "3480\n",
      "1.0\n",
      "3485\n",
      "1.0\n",
      "3490\n",
      "1.0\n",
      "3495\n",
      "1.0\n",
      "3500\n",
      "1.0\n",
      "3505\n",
      "1.0\n",
      "3510\n",
      "1.0\n",
      "3515\n",
      "1.0\n",
      "3520\n",
      "1.0\n",
      "3525\n",
      "1.0\n",
      "3530\n",
      "1.0\n",
      "3535\n",
      "1.0\n",
      "3540\n",
      "1.0\n",
      "3545\n",
      "1.0\n",
      "3550\n",
      "1.0\n",
      "3555\n",
      "1.0\n",
      "3560\n",
      "1.0\n",
      "3565\n",
      "1.0\n",
      "3570\n",
      "1.0\n",
      "3575\n",
      "1.0\n",
      "3580\n",
      "1.0\n",
      "3585\n",
      "1.0\n",
      "3590\n",
      "1.0\n",
      "3595\n",
      "1.0\n",
      "3600\n",
      "1.0\n",
      "3605\n",
      "1.0\n",
      "3610\n",
      "1.0\n",
      "3615\n",
      "1.0\n",
      "3620\n",
      "1.0\n",
      "3625\n",
      "1.0\n",
      "3630\n",
      "1.0\n",
      "3635\n",
      "1.0\n",
      "3640\n",
      "1.0\n",
      "3645\n",
      "1.0\n",
      "3650\n",
      "1.0\n",
      "3655\n",
      "1.0\n",
      "3660\n",
      "1.0\n",
      "3665\n",
      "1.0\n",
      "3670\n",
      "1.0\n",
      "3675\n",
      "1.0\n",
      "3680\n",
      "1.0\n",
      "3685\n",
      "1.0\n",
      "3690\n",
      "1.0\n",
      "3695\n",
      "1.0\n",
      "3700\n",
      "1.0\n",
      "3705\n",
      "1.0\n",
      "3710\n",
      "1.0\n",
      "3715\n",
      "1.0\n",
      "3720\n",
      "1.0\n",
      "3725\n",
      "1.0\n",
      "3730\n",
      "1.0\n",
      "3735\n",
      "1.0\n",
      "3740\n",
      "1.0\n",
      "3745\n",
      "1.0\n",
      "3750\n",
      "1.0\n",
      "3755\n",
      "1.0\n",
      "3760\n",
      "1.0\n",
      "3765\n",
      "1.0\n",
      "3770\n",
      "1.0\n",
      "3775\n",
      "1.0\n",
      "3780\n",
      "1.0\n",
      "3785\n",
      "1.0\n",
      "3790\n",
      "1.0\n",
      "3795\n",
      "1.0\n",
      "3800\n",
      "1.0\n",
      "3805\n",
      "1.0\n",
      "3810\n",
      "1.0\n",
      "3815\n",
      "1.0\n",
      "3820\n",
      "1.0\n",
      "3825\n",
      "1.0\n",
      "3830\n",
      "1.0\n",
      "3835\n",
      "1.0\n",
      "3840\n",
      "1.0\n",
      "3845\n",
      "1.0\n",
      "3850\n",
      "1.0\n",
      "3855\n",
      "1.0\n",
      "3860\n",
      "1.0\n",
      "3865\n",
      "1.0\n",
      "3870\n",
      "1.0\n",
      "3875\n",
      "1.0\n",
      "3880\n",
      "1.0\n",
      "3885\n",
      "1.0\n",
      "3890\n",
      "1.0\n",
      "3895\n",
      "1.0\n",
      "3900\n",
      "1.0\n",
      "3905\n",
      "1.0\n",
      "3910\n",
      "1.0\n",
      "3915\n",
      "1.0\n",
      "3920\n",
      "1.0\n",
      "3925\n",
      "1.0\n",
      "3930\n",
      "1.0\n",
      "3935\n",
      "1.0\n",
      "3940\n",
      "1.0\n",
      "3945\n",
      "1.0\n",
      "3950\n",
      "1.0\n",
      "3955\n",
      "1.0\n",
      "3960\n",
      "1.0\n",
      "3965\n",
      "1.0\n",
      "3970\n",
      "1.0\n",
      "3975\n",
      "1.0\n",
      "3980\n",
      "0.2\n",
      "3985\n",
      "0.0\n",
      "3990\n",
      "0.0\n",
      "3995\n",
      "0.2\n",
      "4000\n",
      "1.0\n",
      "4005\n",
      "0.2\n",
      "4010\n",
      "0.0\n",
      "4015\n",
      "0.0\n",
      "4020\n",
      "0.6\n",
      "4025\n",
      "0.0\n",
      "4030\n",
      "0.6\n",
      "4035\n",
      "0.0\n",
      "4040\n",
      "0.0\n",
      "4045\n",
      "0.0\n",
      "4050\n",
      "0.0\n",
      "4055\n",
      "0.4\n",
      "4060\n",
      "1.0\n",
      "4065\n",
      "0.8\n",
      "4070\n",
      "1.0\n",
      "4075\n",
      "1.0\n",
      "4080\n",
      "0.2\n",
      "4085\n",
      "1.0\n",
      "4090\n",
      "0.2\n",
      "4095\n",
      "0.0\n",
      "4100\n",
      "0.0\n",
      "4105\n",
      "0.0\n",
      "4110\n",
      "0.0\n",
      "4115\n",
      "0.0\n",
      "4120\n",
      "0.4\n",
      "4125\n",
      "0.0\n",
      "4130\n",
      "0.2\n",
      "4135\n",
      "1.0\n",
      "4140\n",
      "1.0\n",
      "4145\n",
      "1.0\n",
      "4150\n",
      "1.0\n",
      "4155\n",
      "1.0\n",
      "4160\n",
      "1.0\n",
      "4165\n",
      "0.0\n",
      "4170\n",
      "0.4\n",
      "4175\n",
      "1.0\n",
      "4180\n",
      "1.0\n",
      "4185\n",
      "1.0\n",
      "4190\n",
      "1.0\n",
      "4195\n",
      "1.0\n",
      "4200\n",
      "1.0\n",
      "4205\n",
      "1.0\n",
      "4210\n",
      "1.0\n",
      "4215\n",
      "1.0\n",
      "4220\n",
      "0.6\n",
      "4225\n",
      "0.2\n",
      "4230\n",
      "1.0\n",
      "4235\n",
      "1.0\n",
      "4240\n",
      "1.0\n",
      "4245\n",
      "1.0\n",
      "4250\n",
      "1.0\n",
      "4255\n",
      "0.0\n",
      "4260\n",
      "0.0\n",
      "4265\n",
      "0.0\n",
      "4270\n",
      "0.0\n",
      "4275\n",
      "0.6\n",
      "4280\n",
      "1.0\n",
      "4285\n",
      "1.0\n",
      "4290\n",
      "0.6\n",
      "4295\n",
      "1.0\n",
      "4300\n",
      "1.0\n",
      "4305\n",
      "1.0\n",
      "4310\n",
      "1.0\n",
      "4315\n",
      "1.0\n",
      "4320\n",
      "1.0\n",
      "4325\n",
      "0.2\n",
      "4330\n",
      "0.0\n",
      "4335\n",
      "0.0\n",
      "4340\n",
      "0.0\n",
      "4345\n",
      "0.0\n",
      "4350\n",
      "0.0\n",
      "4355\n",
      "0.0\n",
      "4360\n",
      "0.0\n",
      "4365\n",
      "0.0\n",
      "4370\n",
      "0.0\n",
      "4375\n",
      "0.0\n",
      "4380\n",
      "0.2\n",
      "4385\n",
      "0.0\n",
      "4390\n",
      "0.0\n",
      "4395\n",
      "1.0\n",
      "4400\n",
      "0.2\n",
      "4405\n",
      "0.2\n",
      "4410\n",
      "0.6\n",
      "4415\n",
      "1.0\n",
      "4420\n",
      "0.6\n",
      "4425\n",
      "1.0\n",
      "4430\n",
      "0.6\n",
      "4435\n",
      "0.0\n",
      "4440\n",
      "1.0\n",
      "4445\n",
      "0.4\n",
      "4450\n",
      "0.0\n",
      "4455\n",
      "0.0\n",
      "4460\n",
      "0.0\n",
      "4465\n",
      "0.0\n",
      "4470\n",
      "0.0\n",
      "4475\n",
      "0.0\n",
      "4480\n",
      "0.0\n",
      "4485\n",
      "0.0\n",
      "4490\n",
      "0.2\n",
      "4495\n",
      "0.0\n",
      "4500\n",
      "0.0\n",
      "4505\n",
      "0.0\n",
      "4510\n",
      "0.6\n",
      "4515\n",
      "1.0\n",
      "4520\n",
      "1.0\n",
      "4525\n",
      "1.0\n",
      "4530\n",
      "1.0\n",
      "4535\n",
      "1.0\n",
      "4540\n",
      "1.0\n",
      "4545\n",
      "1.0\n",
      "4550\n",
      "1.0\n",
      "4555\n",
      "1.0\n",
      "4560\n",
      "1.0\n",
      "4565\n",
      "1.0\n",
      "4570\n",
      "1.0\n",
      "4575\n",
      "1.0\n",
      "4580\n",
      "1.0\n",
      "4585\n",
      "1.0\n",
      "4590\n",
      "1.0\n",
      "4595\n",
      "1.0\n",
      "4600\n",
      "1.0\n",
      "4605\n",
      "1.0\n",
      "4610\n",
      "1.0\n",
      "4615\n",
      "1.0\n",
      "4620\n",
      "1.0\n",
      "4625\n",
      "1.0\n",
      "4630\n",
      "1.0\n",
      "4635\n",
      "1.0\n",
      "4640\n",
      "1.0\n",
      "4645\n",
      "1.0\n",
      "4650\n",
      "1.0\n",
      "4655\n",
      "1.0\n",
      "4660\n",
      "1.0\n",
      "4665\n",
      "1.0\n",
      "4670\n",
      "1.0\n",
      "4675\n",
      "1.0\n",
      "4680\n",
      "1.0\n",
      "4685\n",
      "1.0\n",
      "4690\n",
      "1.0\n",
      "4695\n",
      "1.0\n",
      "4700\n",
      "1.0\n",
      "4705\n",
      "1.0\n",
      "4710\n",
      "1.0\n",
      "4715\n",
      "1.0\n",
      "4720\n",
      "1.0\n",
      "4725\n",
      "1.0\n",
      "4730\n",
      "1.0\n",
      "4735\n",
      "1.0\n",
      "4740\n",
      "1.0\n",
      "4745\n",
      "1.0\n",
      "4750\n",
      "1.0\n",
      "4755\n",
      "1.0\n",
      "4760\n",
      "1.0\n",
      "4765\n",
      "1.0\n",
      "4770\n",
      "1.0\n",
      "4775\n",
      "1.0\n",
      "4780\n",
      "1.0\n",
      "4785\n",
      "1.0\n",
      "4790\n",
      "1.0\n",
      "4795\n",
      "1.0\n",
      "4800\n",
      "1.0\n",
      "4805\n",
      "1.0\n",
      "4810\n",
      "1.0\n",
      "4815\n",
      "1.0\n",
      "4820\n",
      "1.0\n",
      "4825\n",
      "1.0\n",
      "4830\n",
      "1.0\n",
      "4835\n",
      "1.0\n",
      "4840\n",
      "1.0\n",
      "4845\n",
      "1.0\n",
      "4850\n",
      "1.0\n",
      "4855\n",
      "1.0\n",
      "4860\n",
      "1.0\n",
      "4865\n",
      "1.0\n",
      "4870\n",
      "1.0\n",
      "4875\n",
      "1.0\n",
      "4880\n",
      "1.0\n",
      "4885\n",
      "1.0\n",
      "4890\n",
      "1.0\n",
      "4895\n",
      "1.0\n",
      "4900\n",
      "1.0\n",
      "4905\n",
      "1.0\n",
      "4910\n",
      "1.0\n",
      "4915\n",
      "1.0\n",
      "4920\n",
      "1.0\n",
      "4925\n",
      "1.0\n",
      "4930\n",
      "1.0\n",
      "4935\n",
      "1.0\n",
      "4940\n",
      "1.0\n",
      "4945\n",
      "1.0\n",
      "4950\n",
      "1.0\n",
      "4955\n",
      "1.0\n",
      "4960\n",
      "1.0\n",
      "4965\n",
      "1.0\n",
      "4970\n",
      "1.0\n",
      "4975\n",
      "1.0\n",
      "4980\n",
      "1.0\n",
      "4985\n",
      "1.0\n",
      "4990\n",
      "1.0\n",
      "4995\n",
      "1.0\n",
      "5000\n",
      "1.0\n",
      "5005\n",
      "1.0\n",
      "5010\n",
      "1.0\n",
      "5015\n",
      "1.0\n",
      "5020\n",
      "1.0\n",
      "5025\n",
      "1.0\n",
      "5030\n",
      "1.0\n",
      "5035\n",
      "1.0\n",
      "5040\n",
      "1.0\n",
      "5045\n",
      "1.0\n",
      "5050\n",
      "1.0\n",
      "5055\n",
      "1.0\n",
      "5060\n",
      "1.0\n",
      "5065\n",
      "1.0\n",
      "5070\n",
      "1.0\n",
      "5075\n",
      "1.0\n",
      "5080\n",
      "1.0\n",
      "5085\n",
      "1.0\n",
      "5090\n",
      "1.0\n",
      "5095\n",
      "1.0\n",
      "5100\n",
      "1.0\n",
      "5105\n",
      "1.0\n",
      "5110\n",
      "0.0\n",
      "5115\n",
      "0.0\n",
      "5120\n",
      "0.0\n",
      "5125\n",
      "0.0\n",
      "5130\n",
      "0.0\n",
      "5135\n",
      "0.0\n",
      "5140\n",
      "0.0\n",
      "5145\n",
      "0.0\n",
      "5150\n",
      "0.0\n",
      "5155\n",
      "0.0\n",
      "5160\n",
      "0.0\n",
      "5165\n",
      "0.0\n",
      "5170\n",
      "0.0\n",
      "5175\n",
      "0.0\n",
      "5180\n",
      "0.0\n",
      "5185\n",
      "0.0\n",
      "5190\n",
      "0.0\n",
      "5195\n",
      "0.0\n",
      "5200\n",
      "0.0\n",
      "5205\n",
      "0.0\n",
      "5210\n",
      "0.0\n",
      "5215\n",
      "0.0\n",
      "5220\n",
      "0.0\n",
      "5225\n",
      "0.0\n",
      "5230\n",
      "0.0\n",
      "5235\n",
      "0.0\n",
      "5240\n",
      "0.0\n",
      "5245\n",
      "0.0\n",
      "5250\n",
      "0.0\n",
      "5255\n",
      "0.0\n",
      "5260\n",
      "0.0\n",
      "5265\n",
      "0.0\n",
      "5270\n",
      "0.0\n",
      "5275\n",
      "1.0\n",
      "5280\n",
      "1.0\n",
      "5285\n",
      "1.0\n",
      "5290\n",
      "1.0\n",
      "5295\n",
      "1.0\n",
      "5300\n",
      "1.0\n",
      "5305\n",
      "1.0\n",
      "5310\n",
      "1.0\n",
      "5315\n",
      "1.0\n",
      "5320\n",
      "1.0\n",
      "5325\n",
      "1.0\n",
      "5330\n",
      "1.0\n",
      "5335\n",
      "1.0\n",
      "5340\n",
      "1.0\n",
      "5345\n",
      "0.8\n",
      "5350\n",
      "0.8\n",
      "5355\n",
      "1.0\n",
      "5360\n",
      "1.0\n",
      "5365\n",
      "1.0\n",
      "5370\n",
      "1.0\n",
      "5375\n",
      "1.0\n",
      "5380\n",
      "1.0\n",
      "5385\n",
      "1.0\n",
      "5390\n",
      "1.0\n",
      "5395\n",
      "1.0\n",
      "5400\n",
      "1.0\n",
      "5405\n",
      "1.0\n",
      "5410\n",
      "1.0\n",
      "5415\n",
      "1.0\n",
      "5420\n",
      "1.0\n",
      "5425\n",
      "1.0\n",
      "5430\n",
      "1.0\n",
      "5435\n",
      "1.0\n",
      "5440\n",
      "1.0\n",
      "5445\n",
      "1.0\n",
      "5450\n",
      "1.0\n",
      "5455\n",
      "1.0\n",
      "5460\n",
      "1.0\n",
      "5465\n",
      "1.0\n",
      "5470\n",
      "1.0\n",
      "5475\n",
      "1.0\n",
      "5480\n",
      "1.0\n",
      "5485\n",
      "1.0\n",
      "5490\n",
      "1.0\n",
      "5495\n",
      "1.0\n",
      "5500\n",
      "0.6\n",
      "5505\n",
      "0.4\n",
      "5510\n",
      "1.0\n",
      "5515\n",
      "1.0\n",
      "5520\n",
      "1.0\n",
      "5525\n",
      "1.0\n",
      "5530\n",
      "1.0\n",
      "5535\n",
      "1.0\n",
      "5540\n",
      "0.2\n",
      "5545\n",
      "0.0\n",
      "5550\n",
      "0.0\n",
      "5555\n",
      "0.0\n",
      "5560\n",
      "0.0\n",
      "5565\n",
      "0.0\n",
      "5570\n",
      "0.0\n",
      "5575\n",
      "0.0\n",
      "5580\n",
      "0.0\n",
      "5585\n",
      "0.4\n",
      "5590\n",
      "1.0\n",
      "5595\n",
      "0.2\n",
      "5600\n",
      "0.2\n",
      "5605\n",
      "1.0\n",
      "5610\n",
      "0.8\n",
      "5615\n",
      "1.0\n",
      "5620\n",
      "0.4\n",
      "5625\n",
      "0.0\n",
      "5630\n",
      "0.0\n",
      "5635\n",
      "0.0\n",
      "5640\n",
      "0.0\n",
      "5645\n",
      "0.0\n",
      "5650\n",
      "0.0\n",
      "5655\n",
      "0.0\n",
      "5660\n",
      "0.8\n",
      "5665\n",
      "0.8\n",
      "5670\n",
      "1.0\n",
      "5675\n",
      "1.0\n",
      "5680\n",
      "1.0\n",
      "5685\n",
      "1.0\n",
      "5690\n",
      "1.0\n",
      "5695\n",
      "1.0\n",
      "5700\n",
      "1.0\n",
      "5705\n",
      "1.0\n",
      "5710\n",
      "1.0\n",
      "5715\n",
      "1.0\n",
      "5720\n",
      "1.0\n",
      "5725\n",
      "1.0\n",
      "5730\n",
      "1.0\n",
      "5735\n",
      "0.8\n",
      "5740\n",
      "0.4\n",
      "5745\n",
      "1.0\n",
      "5750\n",
      "1.0\n",
      "5755\n",
      "1.0\n",
      "5760\n",
      "1.0\n",
      "5765\n",
      "1.0\n",
      "5770\n",
      "1.0\n",
      "5775\n",
      "1.0\n",
      "5780\n",
      "1.0\n",
      "5785\n",
      "1.0\n",
      "5790\n",
      "1.0\n",
      "5795\n",
      "1.0\n",
      "5800\n",
      "0.8\n",
      "5805\n",
      "0.8\n",
      "5810\n",
      "1.0\n",
      "5815\n",
      "1.0\n",
      "5820\n",
      "1.0\n",
      "5825\n",
      "1.0\n",
      "5830\n",
      "0.8\n",
      "5835\n",
      "1.0\n",
      "5840\n",
      "1.0\n",
      "5845\n",
      "1.0\n",
      "5850\n",
      "0.8\n",
      "5855\n",
      "1.0\n",
      "5860\n",
      "0.6\n",
      "5865\n",
      "0.0\n",
      "5870\n",
      "0.6\n",
      "5875\n",
      "0.0\n",
      "5880\n",
      "0.0\n",
      "5885\n",
      "0.0\n",
      "5890\n",
      "0.8\n",
      "5895\n",
      "1.0\n",
      "5900\n",
      "1.0\n",
      "5905\n",
      "1.0\n",
      "5910\n",
      "1.0\n",
      "5915\n",
      "1.0\n",
      "5920\n",
      "0.8\n",
      "5925\n",
      "1.0\n",
      "5930\n",
      "1.0\n",
      "5935\n",
      "1.0\n",
      "5940\n",
      "1.0\n",
      "5945\n",
      "1.0\n",
      "5950\n",
      "1.0\n",
      "5955\n",
      "1.0\n",
      "5960\n",
      "1.0\n",
      "5965\n",
      "1.0\n",
      "5970\n",
      "1.0\n",
      "5975\n",
      "1.0\n",
      "5980\n",
      "1.0\n",
      "5985\n",
      "1.0\n",
      "5990\n",
      "1.0\n",
      "5995\n",
      "1.0\n",
      "6000\n",
      "1.0\n",
      "6005\n",
      "1.0\n",
      "6010\n",
      "1.0\n",
      "6015\n",
      "1.0\n",
      "6020\n",
      "1.0\n",
      "6025\n",
      "1.0\n",
      "6030\n",
      "1.0\n",
      "6035\n",
      "1.0\n",
      "6040\n",
      "1.0\n",
      "6045\n",
      "1.0\n",
      "6050\n",
      "1.0\n",
      "6055\n",
      "1.0\n",
      "6060\n",
      "0.4\n",
      "6065\n",
      "0.4\n",
      "6070\n",
      "0.8\n",
      "6075\n",
      "0.8\n",
      "6080\n",
      "0.0\n",
      "6085\n",
      "0.2\n",
      "6090\n",
      "1.0\n",
      "6095\n",
      "1.0\n",
      "6100\n",
      "1.0\n",
      "6105\n",
      "0.6\n",
      "6110\n",
      "1.0\n",
      "6115\n",
      "1.0\n",
      "6120\n",
      "1.0\n",
      "6125\n",
      "1.0\n",
      "6130\n",
      "0.8\n",
      "6135\n",
      "1.0\n",
      "6140\n",
      "0.4\n",
      "6145\n",
      "1.0\n",
      "6150\n",
      "1.0\n",
      "6155\n",
      "1.0\n",
      "6160\n",
      "1.0\n",
      "6165\n",
      "1.0\n",
      "6170\n",
      "0.2\n",
      "6175\n",
      "1.0\n",
      "6180\n",
      "1.0\n",
      "6185\n",
      "1.0\n",
      "6190\n",
      "1.0\n",
      "6195\n",
      "1.0\n",
      "6200\n",
      "0.8\n",
      "6205\n",
      "1.0\n",
      "6210\n",
      "0.8\n",
      "6215\n",
      "1.0\n",
      "6220\n",
      "0.0\n",
      "6225\n",
      "0.0\n",
      "6230\n",
      "0.0\n",
      "6235\n",
      "0.0\n",
      "6240\n",
      "0.0\n",
      "6245\n",
      "0.0\n",
      "6250\n",
      "0.0\n",
      "6255\n",
      "0.0\n",
      "6260\n",
      "0.2\n",
      "6265\n",
      "1.0\n",
      "6270\n",
      "1.0\n",
      "6275\n",
      "0.0\n",
      "6280\n",
      "0.0\n",
      "6285\n",
      "0.2\n",
      "6290\n",
      "1.0\n",
      "6295\n",
      "0.0\n",
      "6300\n",
      "1.0\n",
      "6305\n",
      "0.6\n",
      "6310\n",
      "1.0\n",
      "6315\n",
      "1.0\n",
      "6320\n",
      "1.0\n",
      "6325\n",
      "1.0\n",
      "6330\n",
      "1.0\n",
      "6335\n",
      "1.0\n",
      "6340\n",
      "1.0\n",
      "6345\n",
      "1.0\n",
      "6350\n",
      "1.0\n",
      "6355\n",
      "1.0\n",
      "6360\n",
      "1.0\n",
      "6365\n",
      "1.0\n",
      "6370\n",
      "1.0\n",
      "6375\n",
      "1.0\n",
      "6380\n",
      "1.0\n",
      "6385\n",
      "1.0\n",
      "6390\n",
      "1.0\n",
      "6395\n",
      "1.0\n",
      "6400\n",
      "1.0\n",
      "6405\n",
      "1.0\n",
      "6410\n",
      "1.0\n",
      "6415\n",
      "1.0\n",
      "6420\n",
      "1.0\n",
      "6425\n",
      "1.0\n",
      "6430\n",
      "1.0\n",
      "6435\n",
      "1.0\n",
      "6440\n",
      "1.0\n",
      "6445\n",
      "1.0\n",
      "6450\n",
      "1.0\n",
      "6455\n",
      "1.0\n",
      "6460\n",
      "1.0\n",
      "6465\n",
      "1.0\n",
      "6470\n",
      "1.0\n",
      "6475\n",
      "1.0\n",
      "6480\n",
      "1.0\n",
      "6485\n",
      "1.0\n",
      "6490\n",
      "1.0\n",
      "6495\n",
      "1.0\n",
      "6500\n",
      "1.0\n",
      "6505\n",
      "1.0\n",
      "6510\n",
      "1.0\n",
      "6515\n",
      "1.0\n",
      "6520\n",
      "1.0\n",
      "6525\n",
      "1.0\n",
      "6530\n",
      "1.0\n",
      "6535\n",
      "1.0\n",
      "6540\n",
      "1.0\n",
      "6545\n",
      "1.0\n",
      "6550\n",
      "1.0\n",
      "6555\n",
      "1.0\n",
      "6560\n",
      "1.0\n",
      "6565\n",
      "1.0\n",
      "6570\n",
      "1.0\n",
      "6575\n",
      "1.0\n",
      "6580\n",
      "1.0\n",
      "6585\n",
      "1.0\n",
      "6590\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.73631539044730865, 0.41708262635617926)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_min_day = []\n",
    "latest_min = 60 * 40\n",
    "pred_sec = 5\n",
    "for i in range(0,9000-latest_min-pred_sec,pred_sec):\n",
    "    print i\n",
    "    data_train = data_2014_up[0][i:i+latest_min]\n",
    "    X_train = data_train.drop([\"0\"],axis=1)\n",
    "    y_train = data_train['0']\n",
    "    data_test = data_2014_up[0][i+latest_min:i+latest_min+pred_sec]\n",
    "    X_test = data_test.drop([\"0\"],axis=1)\n",
    "    y_test = data_test['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    acc = latest_day_loop_acc(X_train,y_train,X_test,y_test,model)\n",
    "    print acc\n",
    "    mean_min_day.append(acc)\n",
    "np.mean(mean_min_day),np.std(mean_min_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1.0\n",
      "5\n",
      "1.0\n",
      "10\n",
      "1.0\n",
      "15\n",
      "1.0\n",
      "20\n",
      "1.0\n",
      "25\n",
      "1.0\n",
      "30\n",
      "1.0\n",
      "35\n",
      "1.0\n",
      "40\n",
      "1.0\n",
      "45\n",
      "1.0\n",
      "50\n",
      "1.0\n",
      "55\n",
      "1.0\n",
      "60\n",
      "1.0\n",
      "65\n",
      "1.0\n",
      "70\n",
      "1.0\n",
      "75\n",
      "1.0\n",
      "80\n",
      "1.0\n",
      "85\n",
      "1.0\n",
      "90\n",
      "1.0\n",
      "95\n",
      "1.0\n",
      "100\n",
      "1.0\n",
      "105\n",
      "1.0\n",
      "110\n",
      "1.0\n",
      "115\n",
      "1.0\n",
      "120\n",
      "1.0\n",
      "125\n",
      "1.0\n",
      "130\n",
      "1.0\n",
      "135\n",
      "1.0\n",
      "140\n",
      "1.0\n",
      "145\n",
      "1.0\n",
      "150\n",
      "1.0\n",
      "155\n",
      "1.0\n",
      "160\n",
      "1.0\n",
      "165\n",
      "1.0\n",
      "170\n",
      "1.0\n",
      "175\n",
      "1.0\n",
      "180\n",
      "1.0\n",
      "185\n",
      "1.0\n",
      "190\n",
      "1.0\n",
      "195\n",
      "1.0\n",
      "200\n",
      "1.0\n",
      "205\n",
      "1.0\n",
      "210\n",
      "1.0\n",
      "215\n",
      "1.0\n",
      "220\n",
      "1.0\n",
      "225\n",
      "1.0\n",
      "230\n",
      "1.0\n",
      "235\n",
      "1.0\n",
      "240\n",
      "1.0\n",
      "245\n",
      "1.0\n",
      "250\n",
      "1.0\n",
      "255\n",
      "1.0\n",
      "260\n",
      "1.0\n",
      "265\n",
      "1.0\n",
      "270\n",
      "1.0\n",
      "275\n",
      "1.0\n",
      "280\n",
      "1.0\n",
      "285\n",
      "1.0\n",
      "290\n",
      "1.0\n",
      "295\n",
      "1.0\n",
      "300\n",
      "1.0\n",
      "305\n",
      "1.0\n",
      "310\n",
      "1.0\n",
      "315\n",
      "1.0\n",
      "320\n",
      "1.0\n",
      "325\n",
      "1.0\n",
      "330\n",
      "1.0\n",
      "335\n",
      "1.0\n",
      "340\n",
      "1.0\n",
      "345\n",
      "1.0\n",
      "350\n",
      "1.0\n",
      "355\n",
      "1.0\n",
      "360\n",
      "1.0\n",
      "365\n",
      "1.0\n",
      "370\n",
      "1.0\n",
      "375\n",
      "1.0\n",
      "380\n",
      "1.0\n",
      "385\n",
      "1.0\n",
      "390\n",
      "1.0\n",
      "395\n",
      "1.0\n",
      "400\n",
      "1.0\n",
      "405\n",
      "1.0\n",
      "410\n",
      "0.6\n",
      "415\n",
      "1.0\n",
      "420\n",
      "1.0\n",
      "425\n",
      "1.0\n",
      "430\n",
      "1.0\n",
      "435\n",
      "1.0\n",
      "440\n",
      "1.0\n",
      "445\n",
      "1.0\n",
      "450\n",
      "1.0\n",
      "455\n",
      "0.6\n",
      "460\n",
      "0.0\n",
      "465\n",
      "0.0\n",
      "470\n",
      "0.0\n",
      "475\n",
      "0.0\n",
      "480\n",
      "0.4\n",
      "485\n",
      "1.0\n",
      "490\n",
      "1.0\n",
      "495\n",
      "1.0\n",
      "500\n",
      "1.0\n",
      "505\n",
      "1.0\n",
      "510\n",
      "1.0\n",
      "515\n",
      "1.0\n",
      "520\n",
      "1.0\n",
      "525\n",
      "1.0\n",
      "530\n",
      "1.0\n",
      "535\n",
      "0.2\n",
      "540\n",
      "0.0\n",
      "545\n",
      "0.0\n",
      "550\n",
      "0.2\n",
      "555\n",
      "1.0\n",
      "560\n",
      "0.8\n",
      "565\n",
      "0.2\n",
      "570\n",
      "0.0\n",
      "575\n",
      "0.4\n",
      "580\n",
      "1.0\n",
      "585\n",
      "1.0\n",
      "590\n",
      "1.0\n",
      "595\n",
      "0.2\n",
      "600\n",
      "1.0\n",
      "605\n",
      "1.0\n",
      "610\n",
      "0.4\n",
      "615\n",
      "0.6\n",
      "620\n",
      "0.4\n",
      "625\n",
      "0.4\n",
      "630\n",
      "1.0\n",
      "635\n",
      "1.0\n",
      "640\n",
      "1.0\n",
      "645\n",
      "1.0\n",
      "650\n",
      "1.0\n",
      "655\n",
      "1.0\n",
      "660\n",
      "1.0\n",
      "665\n",
      "1.0\n",
      "670\n",
      "1.0\n",
      "675\n",
      "1.0\n",
      "680\n",
      "1.0\n",
      "685\n",
      "1.0\n",
      "690\n",
      "1.0\n",
      "695\n",
      "1.0\n",
      "700\n",
      "1.0\n",
      "705\n",
      "1.0\n",
      "710\n",
      "1.0\n",
      "715\n",
      "1.0\n",
      "720\n",
      "1.0\n",
      "725\n",
      "1.0\n",
      "730\n",
      "1.0\n",
      "735\n",
      "1.0\n",
      "740\n",
      "1.0\n",
      "745\n",
      "1.0\n",
      "750\n",
      "1.0\n",
      "755\n",
      "1.0\n",
      "760\n",
      "1.0\n",
      "765\n",
      "1.0\n",
      "770\n",
      "1.0\n",
      "775\n",
      "1.0\n",
      "780\n",
      "1.0\n",
      "785\n",
      "1.0\n",
      "790\n",
      "1.0\n",
      "795\n",
      "1.0\n",
      "800\n",
      "1.0\n",
      "805\n",
      "1.0\n",
      "810\n",
      "1.0\n",
      "815\n",
      "1.0\n",
      "820\n",
      "1.0\n",
      "825\n",
      "1.0\n",
      "830\n",
      "1.0\n",
      "835\n",
      "1.0\n",
      "840\n",
      "1.0\n",
      "845\n",
      "1.0\n",
      "850\n",
      "1.0\n",
      "855\n",
      "1.0\n",
      "860\n",
      "1.0\n",
      "865\n",
      "1.0\n",
      "870\n",
      "1.0\n",
      "875\n",
      "1.0\n",
      "880\n",
      "1.0\n",
      "885\n",
      "1.0\n",
      "890\n",
      "1.0\n",
      "895\n",
      "1.0\n",
      "900\n",
      "1.0\n",
      "905\n",
      "1.0\n",
      "910\n",
      "1.0\n",
      "915\n",
      "1.0\n",
      "920\n",
      "1.0\n",
      "925\n",
      "1.0\n",
      "930\n",
      "1.0\n",
      "935\n",
      "1.0\n",
      "940\n",
      "1.0\n",
      "945\n",
      "1.0\n",
      "950\n",
      "1.0\n",
      "955\n",
      "1.0\n",
      "960\n",
      "0.2\n",
      "965\n",
      "0.2\n",
      "970\n",
      "0.0\n",
      "975\n",
      "1.0\n",
      "980\n",
      "0.2\n",
      "985\n",
      "0.0\n",
      "990\n",
      "0.0\n",
      "995\n",
      "0.4\n",
      "1000\n",
      "0.0\n",
      "1005\n",
      "0.0\n",
      "1010\n",
      "0.0\n",
      "1015\n",
      "0.0\n",
      "1020\n",
      "0.0\n",
      "1025\n",
      "0.0\n",
      "1030\n",
      "0.0\n",
      "1035\n",
      "0.0\n",
      "1040\n",
      "0.0\n",
      "1045\n",
      "0.2\n",
      "1050\n",
      "0.4\n",
      "1055\n",
      "0.0\n",
      "1060\n",
      "0.0\n",
      "1065\n",
      "0.0\n",
      "1070\n",
      "0.0\n",
      "1075\n",
      "0.0\n",
      "1080\n",
      "0.0\n",
      "1085\n",
      "0.0\n",
      "1090\n",
      "0.4\n",
      "1095\n",
      "1.0\n",
      "1100\n",
      "1.0\n",
      "1105\n",
      "1.0\n",
      "1110\n",
      "1.0\n",
      "1115\n",
      "0.2\n",
      "1120\n",
      "0.0\n",
      "1125\n",
      "0.0\n",
      "1130\n",
      "0.0\n",
      "1135\n",
      "0.0\n",
      "1140\n",
      "0.0\n",
      "1145\n",
      "0.0\n",
      "1150\n",
      "0.0\n",
      "1155\n",
      "0.0\n",
      "1160\n",
      "0.0\n",
      "1165\n",
      "0.0\n",
      "1170\n",
      "0.0\n",
      "1175\n",
      "0.0\n",
      "1180\n",
      "0.0\n",
      "1185\n",
      "0.0\n",
      "1190\n",
      "0.0\n",
      "1195\n",
      "0.0\n",
      "1200\n",
      "0.0\n",
      "1205\n",
      "0.0\n",
      "1210\n",
      "0.0\n",
      "1215\n",
      "0.0\n",
      "1220\n",
      "0.0\n",
      "1225\n",
      "0.0\n",
      "1230\n",
      "0.2\n",
      "1235\n",
      "1.0\n",
      "1240\n",
      "0.8\n",
      "1245\n",
      "0.0\n",
      "1250\n",
      "0.0\n",
      "1255\n",
      "0.0\n",
      "1260\n",
      "0.8\n",
      "1265\n",
      "1.0\n",
      "1270\n",
      "1.0\n",
      "1275\n",
      "1.0\n",
      "1280\n",
      "1.0\n",
      "1285\n",
      "1.0\n",
      "1290\n",
      "1.0\n",
      "1295\n",
      "0.6\n",
      "1300\n",
      "1.0\n",
      "1305\n",
      "1.0\n",
      "1310\n",
      "1.0\n",
      "1315\n",
      "1.0\n",
      "1320\n",
      "1.0\n",
      "1325\n",
      "1.0\n",
      "1330\n",
      "1.0\n",
      "1335\n",
      "1.0\n",
      "1340\n",
      "1.0\n",
      "1345\n",
      "1.0\n",
      "1350\n",
      "1.0\n",
      "1355\n",
      "1.0\n",
      "1360\n",
      "1.0\n",
      "1365\n",
      "1.0\n",
      "1370\n",
      "1.0\n",
      "1375\n",
      "1.0\n",
      "1380\n",
      "0.2\n",
      "1385\n",
      "1.0\n",
      "1390\n",
      "1.0\n",
      "1395\n",
      "1.0\n",
      "1400\n",
      "0.0\n",
      "1405\n",
      "0.4\n",
      "1410\n",
      "1.0\n",
      "1415\n",
      "1.0\n",
      "1420\n",
      "1.0\n",
      "1425\n",
      "1.0\n",
      "1430\n",
      "1.0\n",
      "1435\n",
      "1.0\n",
      "1440\n",
      "0.6\n",
      "1445\n",
      "0.8\n",
      "1450\n",
      "1.0\n",
      "1455\n",
      "0.8\n",
      "1460\n",
      "0.4\n",
      "1465\n",
      "1.0\n",
      "1470\n",
      "1.0\n",
      "1475\n",
      "1.0\n",
      "1480\n",
      "0.8\n",
      "1485\n",
      "0.4\n",
      "1490\n",
      "0.4\n",
      "1495\n",
      "0.0\n",
      "1500\n",
      "0.6\n",
      "1505\n",
      "0.6\n",
      "1510\n",
      "1.0\n",
      "1515\n",
      "1.0\n",
      "1520\n",
      "0.6\n",
      "1525\n",
      "0.8\n",
      "1530\n",
      "1.0\n",
      "1535\n",
      "1.0\n",
      "1540\n",
      "1.0\n",
      "1545\n",
      "0.8\n",
      "1550\n",
      "1.0\n",
      "1555\n",
      "1.0\n",
      "1560\n",
      "1.0\n",
      "1565\n",
      "0.4\n",
      "1570\n",
      "1.0\n",
      "1575\n",
      "1.0\n",
      "1580\n",
      "1.0\n",
      "1585\n",
      "1.0\n",
      "1590\n",
      "1.0\n",
      "1595\n",
      "1.0\n",
      "1600\n",
      "1.0\n",
      "1605\n",
      "1.0\n",
      "1610\n",
      "1.0\n",
      "1615\n",
      "1.0\n",
      "1620\n",
      "1.0\n",
      "1625\n",
      "1.0\n",
      "1630\n",
      "0.8\n",
      "1635\n",
      "0.8\n",
      "1640\n",
      "0.0\n",
      "1645\n",
      "1.0\n",
      "1650\n",
      "1.0\n",
      "1655\n",
      "1.0\n",
      "1660\n",
      "1.0\n",
      "1665\n",
      "1.0\n",
      "1670\n",
      "1.0\n",
      "1675\n",
      "1.0\n",
      "1680\n",
      "1.0\n",
      "1685\n",
      "0.4\n",
      "1690\n",
      "0.0\n",
      "1695\n",
      "0.0\n",
      "1700\n",
      "0.6\n",
      "1705\n",
      "1.0\n",
      "1710\n",
      "1.0\n",
      "1715\n",
      "1.0\n",
      "1720\n",
      "1.0\n",
      "1725\n",
      "1.0\n",
      "1730\n",
      "1.0\n",
      "1735\n",
      "1.0\n",
      "1740\n",
      "1.0\n",
      "1745\n",
      "1.0\n",
      "1750\n",
      "1.0\n",
      "1755\n",
      "1.0\n",
      "1760\n",
      "1.0\n",
      "1765\n",
      "1.0\n",
      "1770\n",
      "1.0\n",
      "1775\n",
      "1.0\n",
      "1780\n",
      "1.0\n",
      "1785\n",
      "1.0\n",
      "1790\n",
      "1.0\n",
      "1795\n",
      "1.0\n",
      "1800\n",
      "1.0\n",
      "1805\n",
      "1.0\n",
      "1810\n",
      "1.0\n",
      "1815\n",
      "1.0\n",
      "1820\n",
      "1.0\n",
      "1825\n",
      "1.0\n",
      "1830\n",
      "1.0\n",
      "1835\n",
      "1.0\n",
      "1840\n",
      "1.0\n",
      "1845\n",
      "1.0\n",
      "1850\n",
      "1.0\n",
      "1855\n",
      "1.0\n",
      "1860\n",
      "1.0\n",
      "1865\n",
      "1.0\n",
      "1870\n",
      "1.0\n",
      "1875\n",
      "1.0\n",
      "1880\n",
      "1.0\n",
      "1885\n",
      "1.0\n",
      "1890\n",
      "1.0\n",
      "1895\n",
      "1.0\n",
      "1900\n",
      "1.0\n",
      "1905\n",
      "1.0\n",
      "1910\n",
      "1.0\n",
      "1915\n",
      "1.0\n",
      "1920\n",
      "1.0\n",
      "1925\n",
      "1.0\n",
      "1930\n",
      "1.0\n",
      "1935\n",
      "1.0\n",
      "1940\n",
      "1.0\n",
      "1945\n",
      "1.0\n",
      "1950\n",
      "1.0\n",
      "1955\n",
      "1.0\n",
      "1960\n",
      "1.0\n",
      "1965\n",
      "1.0\n",
      "1970\n",
      "1.0\n",
      "1975\n",
      "1.0\n",
      "1980\n",
      "1.0\n",
      "1985\n",
      "1.0\n",
      "1990\n",
      "1.0\n",
      "1995\n",
      "1.0\n",
      "2000\n",
      "1.0\n",
      "2005\n",
      "1.0\n",
      "2010\n",
      "1.0\n",
      "2015\n",
      "1.0\n",
      "2020\n",
      "1.0\n",
      "2025\n",
      "1.0\n",
      "2030\n",
      "1.0\n",
      "2035\n",
      "1.0\n",
      "2040\n",
      "1.0\n",
      "2045\n",
      "1.0\n",
      "2050\n",
      "1.0\n",
      "2055\n",
      "1.0\n",
      "2060\n",
      "1.0\n",
      "2065\n",
      "1.0\n",
      "2070\n",
      "1.0\n",
      "2075\n",
      "1.0\n",
      "2080\n",
      "1.0\n",
      "2085\n",
      "1.0\n",
      "2090\n",
      "1.0\n",
      "2095\n",
      "1.0\n",
      "2100\n",
      "1.0\n",
      "2105\n",
      "1.0\n",
      "2110\n",
      "1.0\n",
      "2115\n",
      "1.0\n",
      "2120\n",
      "1.0\n",
      "2125\n",
      "1.0\n",
      "2130\n",
      "1.0\n",
      "2135\n",
      "1.0\n",
      "2140\n",
      "1.0\n",
      "2145\n",
      "1.0\n",
      "2150\n",
      "1.0\n",
      "2155\n",
      "1.0\n",
      "2160\n",
      "1.0\n",
      "2165\n",
      "1.0\n",
      "2170\n",
      "1.0\n",
      "2175\n",
      "1.0\n",
      "2180\n",
      "1.0\n",
      "2185\n",
      "1.0\n",
      "2190\n",
      "1.0\n",
      "2195\n",
      "1.0\n",
      "2200\n",
      "1.0\n",
      "2205\n",
      "1.0\n",
      "2210\n",
      "1.0\n",
      "2215\n",
      "1.0\n",
      "2220\n",
      "1.0\n",
      "2225\n",
      "1.0\n",
      "2230\n",
      "1.0\n",
      "2235\n",
      "1.0\n",
      "2240\n",
      "1.0\n",
      "2245\n",
      "1.0\n",
      "2250\n",
      "1.0\n",
      "2255\n",
      "1.0\n",
      "2260\n",
      "1.0\n",
      "2265\n",
      "1.0\n",
      "2270\n",
      "1.0\n",
      "2275\n",
      "1.0\n",
      "2280\n",
      "1.0\n",
      "2285\n",
      "1.0\n",
      "2290\n",
      "1.0\n",
      "2295\n",
      "1.0\n",
      "2300\n",
      "1.0\n",
      "2305\n",
      "1.0\n",
      "2310\n",
      "1.0\n",
      "2315\n",
      "1.0\n",
      "2320\n",
      "1.0\n",
      "2325\n",
      "1.0\n",
      "2330\n",
      "1.0\n",
      "2335\n",
      "1.0\n",
      "2340\n",
      "1.0\n",
      "2345\n",
      "1.0\n",
      "2350\n",
      "1.0\n",
      "2355\n",
      "1.0\n",
      "2360\n",
      "1.0\n",
      "2365\n",
      "1.0\n",
      "2370\n",
      "1.0\n",
      "2375\n",
      "1.0\n",
      "2380\n",
      "1.0\n",
      "2385\n",
      "1.0\n",
      "2390\n",
      "1.0\n",
      "2395\n",
      "1.0\n",
      "2400\n",
      "1.0\n",
      "2405\n",
      "1.0\n",
      "2410\n",
      "0.6\n",
      "2415\n",
      "0.4\n",
      "2420\n",
      "1.0\n",
      "2425\n",
      "1.0\n",
      "2430\n",
      "0.8\n",
      "2435\n",
      "1.0\n",
      "2440\n",
      "0.8\n",
      "2445\n",
      "1.0\n",
      "2450\n",
      "1.0\n",
      "2455\n",
      "0.6\n",
      "2460\n",
      "1.0\n",
      "2465\n",
      "1.0\n",
      "2470\n",
      "0.6\n",
      "2475\n",
      "1.0\n",
      "2480\n",
      "1.0\n",
      "2485\n",
      "1.0\n",
      "2490\n",
      "1.0\n",
      "2495\n",
      "1.0\n",
      "2500\n",
      "1.0\n",
      "2505\n",
      "0.6\n",
      "2510\n",
      "1.0\n",
      "2515\n",
      "0.6\n",
      "2520\n",
      "1.0\n",
      "2525\n",
      "1.0\n",
      "2530\n",
      "0.4\n",
      "2535\n",
      "1.0\n",
      "2540\n",
      "1.0\n",
      "2545\n",
      "1.0\n",
      "2550\n",
      "1.0\n",
      "2555\n",
      "1.0\n",
      "2560\n",
      "0.2\n",
      "2565\n",
      "1.0\n",
      "2570\n",
      "1.0\n",
      "2575\n",
      "0.8\n",
      "2580\n",
      "1.0\n",
      "2585\n",
      "1.0\n",
      "2590\n",
      "1.0\n",
      "2595\n",
      "1.0\n",
      "2600\n",
      "1.0\n",
      "2605\n",
      "0.6\n",
      "2610\n",
      "0.0\n",
      "2615\n",
      "0.4\n",
      "2620\n",
      "0.4\n",
      "2625\n",
      "0.8\n",
      "2630\n",
      "1.0\n",
      "2635\n",
      "1.0\n",
      "2640\n",
      "0.8\n",
      "2645\n",
      "0.2\n",
      "2650\n",
      "0.0\n",
      "2655\n",
      "0.0\n",
      "2660\n",
      "0.0\n",
      "2665\n",
      "0.0\n",
      "2670\n",
      "1.0\n",
      "2675\n",
      "1.0\n",
      "2680\n",
      "0.6\n",
      "2685\n",
      "1.0\n",
      "2690\n",
      "1.0\n",
      "2695\n",
      "1.0\n",
      "2700\n",
      "1.0\n",
      "2705\n",
      "1.0\n",
      "2710\n",
      "1.0\n",
      "2715\n",
      "1.0\n",
      "2720\n",
      "1.0\n",
      "2725\n",
      "1.0\n",
      "2730\n",
      "0.6\n",
      "2735\n",
      "0.6\n",
      "2740\n",
      "1.0\n",
      "2745\n",
      "1.0\n",
      "2750\n",
      "1.0\n",
      "2755\n",
      "1.0\n",
      "2760\n",
      "1.0\n",
      "2765\n",
      "1.0\n",
      "2770\n",
      "1.0\n",
      "2775\n",
      "1.0\n",
      "2780\n",
      "1.0\n",
      "2785\n",
      "1.0\n",
      "2790\n",
      "1.0\n",
      "2795\n",
      "1.0\n",
      "2800\n",
      "1.0\n",
      "2805\n",
      "0.8\n",
      "2810\n",
      "1.0\n",
      "2815\n",
      "1.0\n",
      "2820\n",
      "1.0\n",
      "2825\n",
      "1.0\n",
      "2830\n",
      "1.0\n",
      "2835\n",
      "1.0\n",
      "2840\n",
      "1.0\n",
      "2845\n",
      "1.0\n",
      "2850\n",
      "1.0\n",
      "2855\n",
      "1.0\n",
      "2860\n",
      "1.0\n",
      "2865\n",
      "1.0\n",
      "2870\n",
      "0.0\n",
      "2875\n",
      "0.0\n",
      "2880\n",
      "0.4\n",
      "2885\n",
      "0.4\n",
      "2890\n",
      "1.0\n",
      "2895\n",
      "1.0\n",
      "2900\n",
      "0.2\n",
      "2905\n",
      "0.0\n",
      "2910\n",
      "0.2\n",
      "2915\n",
      "0.2\n",
      "2920\n",
      "0.0\n",
      "2925\n",
      "0.0\n",
      "2930\n",
      "0.0\n",
      "2935\n",
      "0.6\n",
      "2940\n",
      "0.0\n",
      "2945\n",
      "0.0\n",
      "2950\n",
      "0.0\n",
      "2955\n",
      "0.0\n",
      "2960\n",
      "0.0\n",
      "2965\n",
      "0.0\n",
      "2970\n",
      "0.6\n",
      "2975\n",
      "0.0\n",
      "2980\n",
      "0.0\n",
      "2985\n",
      "0.0\n",
      "2990\n",
      "0.0\n",
      "2995\n",
      "0.0\n",
      "3000\n",
      "0.0\n",
      "3005\n",
      "0.0\n",
      "3010\n",
      "0.2\n",
      "3015\n",
      "0.0\n",
      "3020\n",
      "0.0\n",
      "3025\n",
      "0.0\n",
      "3030\n",
      "0.6\n",
      "3035\n",
      "1.0\n",
      "3040\n",
      "1.0\n",
      "3045\n",
      "1.0\n",
      "3050\n",
      "1.0\n",
      "3055\n",
      "1.0\n",
      "3060\n",
      "0.4\n",
      "3065\n",
      "0.0\n",
      "3070\n",
      "0.0\n",
      "3075\n",
      "0.0\n",
      "3080\n",
      "0.0\n",
      "3085\n",
      "0.0\n",
      "3090\n",
      "0.0\n",
      "3095\n",
      "0.0\n",
      "3100\n",
      "0.0\n",
      "3105\n",
      "0.0\n",
      "3110\n",
      "0.0\n",
      "3115\n",
      "0.0\n",
      "3120\n",
      "0.4\n",
      "3125\n",
      "0.0\n",
      "3130\n",
      "0.0\n",
      "3135\n",
      "0.4\n",
      "3140\n",
      "0.0\n",
      "3145\n",
      "0.0\n",
      "3150\n",
      "0.0\n",
      "3155\n",
      "0.0\n",
      "3160\n",
      "0.0\n",
      "3165\n",
      "0.0\n",
      "3170\n",
      "0.0\n",
      "3175\n",
      "0.0\n",
      "3180\n",
      "0.0\n",
      "3185\n",
      "0.0\n",
      "3190\n",
      "0.8\n",
      "3195\n",
      "0.2\n",
      "3200\n",
      "0.2\n",
      "3205\n",
      "0.6\n",
      "3210\n",
      "1.0\n",
      "3215\n",
      "1.0\n",
      "3220\n",
      "1.0\n",
      "3225\n",
      "1.0\n",
      "3230\n",
      "1.0\n",
      "3235\n",
      "1.0\n",
      "3240\n",
      "1.0\n",
      "3245\n",
      "0.8\n",
      "3250\n",
      "0.0\n",
      "3255\n",
      "0.8\n",
      "3260\n",
      "0.0\n",
      "3265\n",
      "0.0\n",
      "3270\n",
      "0.0\n",
      "3275\n",
      "0.0\n",
      "3280\n",
      "0.0\n",
      "3285\n",
      "0.0\n",
      "3290\n",
      "0.0\n",
      "3295\n",
      "0.0\n",
      "3300\n",
      "0.0\n",
      "3305\n",
      "0.0\n",
      "3310\n",
      "0.0\n",
      "3315\n",
      "0.0\n",
      "3320\n",
      "0.0\n",
      "3325\n",
      "0.0\n",
      "3330\n",
      "0.0\n",
      "3335\n",
      "0.0\n",
      "3340\n",
      "0.0\n",
      "3345\n",
      "0.0\n",
      "3350\n",
      "0.0\n",
      "3355\n",
      "0.0\n",
      "3360\n",
      "0.0\n",
      "3365\n",
      "0.0\n",
      "3370\n",
      "0.0\n",
      "3375\n",
      "0.0\n",
      "3380\n",
      "0.0\n",
      "3385\n",
      "0.0\n",
      "3390\n",
      "0.0\n",
      "3395\n",
      "0.0\n",
      "3400\n",
      "0.0\n",
      "3405\n",
      "0.0\n",
      "3410\n",
      "0.0\n",
      "3415\n",
      "0.0\n",
      "3420\n",
      "0.0\n",
      "3425\n",
      "0.0\n",
      "3430\n",
      "0.0\n",
      "3435\n",
      "0.2\n",
      "3440\n",
      "0.0\n",
      "3445\n",
      "0.0\n",
      "3450\n",
      "0.0\n",
      "3455\n",
      "0.0\n",
      "3460\n",
      "0.0\n",
      "3465\n",
      "0.0\n",
      "3470\n",
      "0.0\n",
      "3475\n",
      "0.0\n",
      "3480\n",
      "0.0\n",
      "3485\n",
      "0.0\n",
      "3490\n",
      "0.0\n",
      "3495\n",
      "0.2\n",
      "3500\n",
      "0.2\n",
      "3505\n",
      "0.0\n",
      "3510\n",
      "0.0\n",
      "3515\n",
      "0.0\n",
      "3520\n",
      "1.0\n",
      "3525\n",
      "1.0\n",
      "3530\n",
      "1.0\n",
      "3535\n",
      "1.0\n",
      "3540\n",
      "1.0\n",
      "3545\n",
      "1.0\n",
      "3550\n",
      "0.2\n",
      "3555\n",
      "0.2\n",
      "3560\n",
      "1.0\n",
      "3565\n",
      "1.0\n",
      "3570\n",
      "1.0\n",
      "3575\n",
      "1.0\n",
      "3580\n",
      "1.0\n",
      "3585\n",
      "1.0\n",
      "3590\n",
      "1.0\n",
      "3595\n",
      "1.0\n",
      "3600\n",
      "1.0\n",
      "3605\n",
      "1.0\n",
      "3610\n",
      "1.0\n",
      "3615\n",
      "1.0\n",
      "3620\n",
      "1.0\n",
      "3625\n",
      "1.0\n",
      "3630\n",
      "1.0\n",
      "3635\n",
      "1.0\n",
      "3640\n",
      "1.0\n",
      "3645\n",
      "1.0\n",
      "3650\n",
      "1.0\n",
      "3655\n",
      "1.0\n",
      "3660\n",
      "1.0\n",
      "3665\n",
      "1.0\n",
      "3670\n",
      "1.0\n",
      "3675\n",
      "1.0\n",
      "3680\n",
      "1.0\n",
      "3685\n",
      "1.0\n",
      "3690\n",
      "1.0\n",
      "3695\n",
      "1.0\n",
      "3700\n",
      "1.0\n",
      "3705\n",
      "1.0\n",
      "3710\n",
      "1.0\n",
      "3715\n",
      "1.0\n",
      "3720\n",
      "1.0\n",
      "3725\n",
      "1.0\n",
      "3730\n",
      "1.0\n",
      "3735\n",
      "1.0\n",
      "3740\n",
      "1.0\n",
      "3745\n",
      "1.0\n",
      "3750\n",
      "1.0\n",
      "3755\n",
      "1.0\n",
      "3760\n",
      "1.0\n",
      "3765\n",
      "1.0\n",
      "3770\n",
      "1.0\n",
      "3775\n",
      "1.0\n",
      "3780\n",
      "1.0\n",
      "3785\n",
      "1.0\n",
      "3790\n",
      "1.0\n",
      "3795\n",
      "1.0\n",
      "3800\n",
      "1.0\n",
      "3805\n",
      "1.0\n",
      "3810\n",
      "1.0\n",
      "3815\n",
      "1.0\n",
      "3820\n",
      "1.0\n",
      "3825\n",
      "1.0\n",
      "3830\n",
      "1.0\n",
      "3835\n",
      "1.0\n",
      "3840\n",
      "1.0\n",
      "3845\n",
      "1.0\n",
      "3850\n",
      "1.0\n",
      "3855\n",
      "1.0\n",
      "3860\n",
      "1.0\n",
      "3865\n",
      "1.0\n",
      "3870\n",
      "1.0\n",
      "3875\n",
      "1.0\n",
      "3880\n",
      "1.0\n",
      "3885\n",
      "1.0\n",
      "3890\n",
      "1.0\n",
      "3895\n",
      "1.0\n",
      "3900\n",
      "1.0\n",
      "3905\n",
      "1.0\n",
      "3910\n",
      "1.0\n",
      "3915\n",
      "1.0\n",
      "3920\n",
      "1.0\n",
      "3925\n",
      "1.0\n",
      "3930\n",
      "1.0\n",
      "3935\n",
      "1.0\n",
      "3940\n",
      "1.0\n",
      "3945\n",
      "1.0\n",
      "3950\n",
      "1.0\n",
      "3955\n",
      "1.0\n",
      "3960\n",
      "1.0\n",
      "3965\n",
      "1.0\n",
      "3970\n",
      "1.0\n",
      "3975\n",
      "1.0\n",
      "3980\n",
      "1.0\n",
      "3985\n",
      "1.0\n",
      "3990\n",
      "1.0\n",
      "3995\n",
      "1.0\n",
      "4000\n",
      "1.0\n",
      "4005\n",
      "1.0\n",
      "4010\n",
      "1.0\n",
      "4015\n",
      "1.0\n",
      "4020\n",
      "1.0\n",
      "4025\n",
      "1.0\n",
      "4030\n",
      "1.0\n",
      "4035\n",
      "1.0\n",
      "4040\n",
      "1.0\n",
      "4045\n",
      "1.0\n",
      "4050\n",
      "1.0\n",
      "4055\n",
      "1.0\n",
      "4060\n",
      "1.0\n",
      "4065\n",
      "1.0\n",
      "4070\n",
      "1.0\n",
      "4075\n",
      "1.0\n",
      "4080\n",
      "1.0\n",
      "4085\n",
      "1.0\n",
      "4090\n",
      "1.0\n",
      "4095\n",
      "1.0\n",
      "4100\n",
      "1.0\n",
      "4105\n",
      "1.0\n",
      "4110\n",
      "1.0\n",
      "4115\n",
      "1.0\n",
      "4120\n",
      "1.0\n",
      "4125\n",
      "1.0\n",
      "4130\n",
      "1.0\n",
      "4135\n",
      "1.0\n",
      "4140\n",
      "1.0\n",
      "4145\n",
      "1.0\n",
      "4150\n",
      "1.0\n",
      "4155\n",
      "1.0\n",
      "4160\n",
      "1.0\n",
      "4165\n",
      "1.0\n",
      "4170\n",
      "1.0\n",
      "4175\n",
      "1.0\n",
      "4180\n",
      "1.0\n",
      "4185\n",
      "1.0\n",
      "4190\n",
      "1.0\n",
      "4195\n",
      "1.0\n",
      "4200\n",
      "1.0\n",
      "4205\n",
      "1.0\n",
      "4210\n",
      "1.0\n",
      "4215\n",
      "1.0\n",
      "4220\n",
      "1.0\n",
      "4225\n",
      "1.0\n",
      "4230\n",
      "1.0\n",
      "4235\n",
      "1.0\n",
      "4240\n",
      "1.0\n",
      "4245\n",
      "1.0\n",
      "4250\n",
      "1.0\n",
      "4255\n",
      "1.0\n",
      "4260\n",
      "1.0\n",
      "4265\n",
      "1.0\n",
      "4270\n",
      "1.0\n",
      "4275\n",
      "1.0\n",
      "4280\n",
      "1.0\n",
      "4285\n",
      "1.0\n",
      "4290\n",
      "1.0\n",
      "4295\n",
      "1.0\n",
      "4300\n",
      "1.0\n",
      "4305\n",
      "1.0\n",
      "4310\n",
      "1.0\n",
      "4315\n",
      "1.0\n",
      "4320\n",
      "1.0\n",
      "4325\n",
      "1.0\n",
      "4330\n",
      "1.0\n",
      "4335\n",
      "1.0\n",
      "4340\n",
      "1.0\n",
      "4345\n",
      "1.0\n",
      "4350\n",
      "1.0\n",
      "4355\n",
      "1.0\n",
      "4360\n",
      "1.0\n",
      "4365\n",
      "1.0\n",
      "4370\n",
      "1.0\n",
      "4375\n",
      "1.0\n",
      "4380\n",
      "1.0\n",
      "4385\n",
      "1.0\n",
      "4390\n",
      "1.0\n",
      "4395\n",
      "1.0\n",
      "4400\n",
      "1.0\n",
      "4405\n",
      "1.0\n",
      "4410\n",
      "1.0\n",
      "4415\n",
      "1.0\n",
      "4420\n",
      "1.0\n",
      "4425\n",
      "1.0\n",
      "4430\n",
      "1.0\n",
      "4435\n",
      "1.0\n",
      "4440\n",
      "1.0\n",
      "4445\n",
      "1.0\n",
      "4450\n",
      "1.0\n",
      "4455\n",
      "1.0\n",
      "4460\n",
      "1.0\n",
      "4465\n",
      "1.0\n",
      "4470\n",
      "1.0\n",
      "4475\n",
      "1.0\n",
      "4480\n",
      "1.0\n",
      "4485\n",
      "1.0\n",
      "4490\n",
      "1.0\n",
      "4495\n",
      "1.0\n",
      "4500\n",
      "1.0\n",
      "4505\n",
      "1.0\n",
      "4510\n",
      "1.0\n",
      "4515\n",
      "1.0\n",
      "4520\n",
      "1.0\n",
      "4525\n",
      "1.0\n",
      "4530\n",
      "1.0\n",
      "4535\n",
      "1.0\n",
      "4540\n",
      "1.0\n",
      "4545\n",
      "1.0\n",
      "4550\n",
      "1.0\n",
      "4555\n",
      "1.0\n",
      "4560\n",
      "1.0\n",
      "4565\n",
      "1.0\n",
      "4570\n",
      "1.0\n",
      "4575\n",
      "1.0\n",
      "4580\n",
      "0.2\n",
      "4585\n",
      "0.4\n",
      "4590\n",
      "0.6\n",
      "4595\n",
      "0.6\n",
      "4600\n",
      "1.0\n",
      "4605\n",
      "1.0\n",
      "4610\n",
      "1.0\n",
      "4615\n",
      "0.8\n",
      "4620\n",
      "0.6\n",
      "4625\n",
      "0.2\n",
      "4630\n",
      "0.4\n",
      "4635\n",
      "1.0\n",
      "4640\n",
      "0.8\n",
      "4645\n",
      "1.0\n",
      "4650\n",
      "0.8\n",
      "4655\n",
      "0.8\n",
      "4660\n",
      "1.0\n",
      "4665\n",
      "0.8\n",
      "4670\n",
      "1.0\n",
      "4675\n",
      "1.0\n",
      "4680\n",
      "0.4\n",
      "4685\n",
      "1.0\n",
      "4690\n",
      "0.6\n",
      "4695\n",
      "0.4\n",
      "4700\n",
      "0.0\n",
      "4705\n",
      "0.8\n",
      "4710\n",
      "1.0\n",
      "4715\n",
      "1.0\n",
      "4720\n",
      "1.0\n",
      "4725\n",
      "1.0\n",
      "4730\n",
      "1.0\n",
      "4735\n",
      "1.0\n",
      "4740\n",
      "1.0\n",
      "4745\n",
      "1.0\n",
      "4750\n",
      "1.0\n",
      "4755\n",
      "1.0\n",
      "4760\n",
      "1.0\n",
      "4765\n",
      "0.2\n",
      "4770\n",
      "1.0\n",
      "4775\n",
      "1.0\n",
      "4780\n",
      "1.0\n",
      "4785\n",
      "1.0\n",
      "4790\n",
      "1.0\n",
      "4795\n",
      "1.0\n",
      "4800\n",
      "1.0\n",
      "4805\n",
      "1.0\n",
      "4810\n",
      "1.0\n",
      "4815\n",
      "1.0\n",
      "4820\n",
      "1.0\n",
      "4825\n",
      "1.0\n",
      "4830\n",
      "1.0\n",
      "4835\n",
      "1.0\n",
      "4840\n",
      "1.0\n",
      "4845\n",
      "1.0\n",
      "4850\n",
      "1.0\n",
      "4855\n",
      "0.0\n",
      "4860\n",
      "0.2\n",
      "4865\n",
      "0.2\n",
      "4870\n",
      "0.0\n",
      "4875\n",
      "1.0\n",
      "4880\n",
      "1.0\n",
      "4885\n",
      "1.0\n",
      "4890\n",
      "0.8\n",
      "4895\n",
      "1.0\n",
      "4900\n",
      "1.0\n",
      "4905\n",
      "0.4\n",
      "4910\n",
      "1.0\n",
      "4915\n",
      "1.0\n",
      "4920\n",
      "1.0\n",
      "4925\n",
      "1.0\n",
      "4930\n",
      "0.8\n",
      "4935\n",
      "1.0\n",
      "4940\n",
      "1.0\n",
      "4945\n",
      "1.0\n",
      "4950\n",
      "1.0\n",
      "4955\n",
      "0.0\n",
      "4960\n",
      "0.2\n",
      "4965\n",
      "0.0\n",
      "4970\n",
      "0.2\n",
      "4975\n",
      "1.0\n",
      "4980\n",
      "0.6\n",
      "4985\n",
      "0.2\n",
      "4990\n",
      "1.0\n",
      "4995\n",
      "1.0\n",
      "5000\n",
      "1.0\n",
      "5005\n",
      "1.0\n",
      "5010\n",
      "1.0\n",
      "5015\n",
      "1.0\n",
      "5020\n",
      "1.0\n",
      "5025\n",
      "1.0\n",
      "5030\n",
      "1.0\n",
      "5035\n",
      "1.0\n",
      "5040\n",
      "1.0\n",
      "5045\n",
      "1.0\n",
      "5050\n",
      "0.6\n",
      "5055\n",
      "1.0\n",
      "5060\n",
      "1.0\n",
      "5065\n",
      "0.8\n",
      "5070\n",
      "1.0\n",
      "5075\n",
      "0.6\n",
      "5080\n",
      "0.0\n",
      "5085\n",
      "0.6\n",
      "5090\n",
      "1.0\n",
      "5095\n",
      "1.0\n",
      "5100\n",
      "1.0\n",
      "5105\n",
      "1.0\n",
      "5110\n",
      "0.0\n",
      "5115\n",
      "0.0\n",
      "5120\n",
      "0.0\n",
      "5125\n",
      "0.8\n",
      "5130\n",
      "0.6\n",
      "5135\n",
      "1.0\n",
      "5140\n",
      "1.0\n",
      "5145\n",
      "1.0\n",
      "5150\n",
      "1.0\n",
      "5155\n",
      "1.0\n",
      "5160\n",
      "1.0\n",
      "5165\n",
      "1.0\n",
      "5170\n",
      "1.0\n",
      "5175\n",
      "1.0\n",
      "5180\n",
      "1.0\n",
      "5185\n",
      "1.0\n",
      "5190\n",
      "1.0\n",
      "5195\n",
      "1.0\n",
      "5200\n",
      "1.0\n",
      "5205\n",
      "1.0\n",
      "5210\n",
      "1.0\n",
      "5215\n",
      "1.0\n",
      "5220\n",
      "1.0\n",
      "5225\n",
      "1.0\n",
      "5230\n",
      "1.0\n",
      "5235\n",
      "1.0\n",
      "5240\n",
      "1.0\n",
      "5245\n",
      "1.0\n",
      "5250\n",
      "1.0\n",
      "5255\n",
      "1.0\n",
      "5260\n",
      "1.0\n",
      "5265\n",
      "1.0\n",
      "5270\n",
      "1.0\n",
      "5275\n",
      "1.0\n",
      "5280\n",
      "1.0\n",
      "5285\n",
      "1.0\n",
      "5290\n",
      "1.0\n",
      "5295\n",
      "1.0\n",
      "5300\n",
      "1.0\n",
      "5305\n",
      "1.0\n",
      "5310\n",
      "1.0\n",
      "5315\n",
      "1.0\n",
      "5320\n",
      "1.0\n",
      "5325\n",
      "1.0\n",
      "5330\n",
      "1.0\n",
      "5335\n",
      "1.0\n",
      "5340\n",
      "1.0\n",
      "5345\n",
      "1.0\n",
      "5350\n",
      "1.0\n",
      "5355\n",
      "1.0\n",
      "5360\n",
      "1.0\n",
      "5365\n",
      "1.0\n",
      "5370\n",
      "1.0\n",
      "5375\n",
      "1.0\n",
      "5380\n",
      "1.0\n",
      "5385\n",
      "1.0\n",
      "5390\n",
      "1.0\n",
      "5395\n",
      "1.0\n",
      "5400\n",
      "1.0\n",
      "5405\n",
      "1.0\n",
      "5410\n",
      "1.0\n",
      "5415\n",
      "1.0\n",
      "5420\n",
      "1.0\n",
      "5425\n",
      "1.0\n",
      "5430\n",
      "1.0\n",
      "5435\n",
      "1.0\n",
      "5440\n",
      "1.0\n",
      "5445\n",
      "1.0\n",
      "5450\n",
      "1.0\n",
      "5455\n",
      "1.0\n",
      "5460\n",
      "1.0\n",
      "5465\n",
      "1.0\n",
      "5470\n",
      "1.0\n",
      "5475\n",
      "1.0\n",
      "5480\n",
      "1.0\n",
      "5485\n",
      "1.0\n",
      "5490\n",
      "1.0\n",
      "5495\n",
      "1.0\n",
      "5500\n",
      "1.0\n",
      "5505\n",
      "1.0\n",
      "5510\n",
      "1.0\n",
      "5515\n",
      "1.0\n",
      "5520\n",
      "1.0\n",
      "5525\n",
      "1.0\n",
      "5530\n",
      "1.0\n",
      "5535\n",
      "1.0\n",
      "5540\n",
      "1.0\n",
      "5545\n",
      "1.0\n",
      "5550\n",
      "1.0\n",
      "5555\n",
      "1.0\n",
      "5560\n",
      "1.0\n",
      "5565\n",
      "1.0\n",
      "5570\n",
      "1.0\n",
      "5575\n",
      "1.0\n",
      "5580\n",
      "1.0\n",
      "5585\n",
      "1.0\n",
      "5590\n",
      "1.0\n",
      "5595\n",
      "1.0\n",
      "5600\n",
      "1.0\n",
      "5605\n",
      "1.0\n",
      "5610\n",
      "1.0\n",
      "5615\n",
      "1.0\n",
      "5620\n",
      "1.0\n",
      "5625\n",
      "1.0\n",
      "5630\n",
      "1.0\n",
      "5635\n",
      "1.0\n",
      "5640\n",
      "1.0\n",
      "5645\n",
      "1.0\n",
      "5650\n",
      "1.0\n",
      "5655\n",
      "1.0\n",
      "5660\n",
      "1.0\n",
      "5665\n",
      "1.0\n",
      "5670\n",
      "1.0\n",
      "5675\n",
      "1.0\n",
      "5680\n",
      "1.0\n",
      "5685\n",
      "1.0\n",
      "5690\n",
      "1.0\n",
      "5695\n",
      "1.0\n",
      "5700\n",
      "1.0\n",
      "5705\n",
      "1.0\n",
      "5710\n",
      "0.0\n",
      "5715\n",
      "0.0\n",
      "5720\n",
      "0.0\n",
      "5725\n",
      "0.0\n",
      "5730\n",
      "0.0\n",
      "5735\n",
      "0.0\n",
      "5740\n",
      "0.0\n",
      "5745\n",
      "0.0\n",
      "5750\n",
      "0.0\n",
      "5755\n",
      "0.0\n",
      "5760\n",
      "0.0\n",
      "5765\n",
      "0.0\n",
      "5770\n",
      "0.0\n",
      "5775\n",
      "0.0\n",
      "5780\n",
      "0.0\n",
      "5785\n",
      "0.0\n",
      "5790\n",
      "0.0\n",
      "5795\n",
      "0.0\n",
      "5800\n",
      "0.4\n",
      "5805\n",
      "0.0\n",
      "5810\n",
      "0.0\n",
      "5815\n",
      "0.0\n",
      "5820\n",
      "0.0\n",
      "5825\n",
      "0.0\n",
      "5830\n",
      "0.0\n",
      "5835\n",
      "0.0\n",
      "5840\n",
      "0.0\n",
      "5845\n",
      "0.0\n",
      "5850\n",
      "0.0\n",
      "5855\n",
      "0.0\n",
      "5860\n",
      "0.0\n",
      "5865\n",
      "0.0\n",
      "5870\n",
      "0.0\n",
      "5875\n",
      "1.0\n",
      "5880\n",
      "1.0\n",
      "5885\n",
      "1.0\n",
      "5890\n",
      "1.0\n",
      "5895\n",
      "1.0\n",
      "5900\n",
      "1.0\n",
      "5905\n",
      "1.0\n",
      "5910\n",
      "1.0\n",
      "5915\n",
      "1.0\n",
      "5920\n",
      "1.0\n",
      "5925\n",
      "1.0\n",
      "5930\n",
      "1.0\n",
      "5935\n",
      "1.0\n",
      "5940\n",
      "1.0\n",
      "5945\n",
      "0.8\n",
      "5950\n",
      "0.8\n",
      "5955\n",
      "1.0\n",
      "5960\n",
      "1.0\n",
      "5965\n",
      "1.0\n",
      "5970\n",
      "1.0\n",
      "5975\n",
      "1.0\n",
      "5980\n",
      "1.0\n",
      "5985\n",
      "1.0\n",
      "5990\n",
      "1.0\n",
      "5995\n",
      "1.0\n",
      "6000\n",
      "1.0\n",
      "6005\n",
      "1.0\n",
      "6010\n",
      "1.0\n",
      "6015\n",
      "1.0\n",
      "6020\n",
      "1.0\n",
      "6025\n",
      "1.0\n",
      "6030\n",
      "1.0\n",
      "6035\n",
      "1.0\n",
      "6040\n",
      "1.0\n",
      "6045\n",
      "1.0\n",
      "6050\n",
      "1.0\n",
      "6055\n",
      "1.0\n",
      "6060\n",
      "1.0\n",
      "6065\n",
      "1.0\n",
      "6070\n",
      "1.0\n",
      "6075\n",
      "1.0\n",
      "6080\n",
      "1.0\n",
      "6085\n",
      "1.0\n",
      "6090\n",
      "1.0\n",
      "6095\n",
      "1.0\n",
      "6100\n",
      "1.0\n",
      "6105\n",
      "0.2\n",
      "6110\n",
      "1.0\n",
      "6115\n",
      "0.8\n",
      "6120\n",
      "1.0\n",
      "6125\n",
      "1.0\n",
      "6130\n",
      "1.0\n",
      "6135\n",
      "1.0\n",
      "6140\n",
      "0.2\n",
      "6145\n",
      "0.0\n",
      "6150\n",
      "0.0\n",
      "6155\n",
      "0.0\n",
      "6160\n",
      "0.0\n",
      "6165\n",
      "0.0\n",
      "6170\n",
      "0.0\n",
      "6175\n",
      "0.4\n",
      "6180\n",
      "0.4\n",
      "6185\n",
      "0.6\n",
      "6190\n",
      "1.0\n",
      "6195\n",
      "0.2\n",
      "6200\n",
      "0.0\n",
      "6205\n",
      "0.0\n",
      "6210\n",
      "0.2\n",
      "6215\n",
      "0.2\n",
      "6220\n",
      "0.6\n",
      "6225\n",
      "0.0\n",
      "6230\n",
      "0.0\n",
      "6235\n",
      "0.0\n",
      "6240\n",
      "0.0\n",
      "6245\n",
      "0.0\n",
      "6250\n",
      "0.6\n",
      "6255\n",
      "1.0\n",
      "6260\n",
      "1.0\n",
      "6265\n",
      "1.0\n",
      "6270\n",
      "1.0\n",
      "6275\n",
      "1.0\n",
      "6280\n",
      "1.0\n",
      "6285\n",
      "1.0\n",
      "6290\n",
      "1.0\n",
      "6295\n",
      "1.0\n",
      "6300\n",
      "1.0\n",
      "6305\n",
      "1.0\n",
      "6310\n",
      "1.0\n",
      "6315\n",
      "1.0\n",
      "6320\n",
      "1.0\n",
      "6325\n",
      "1.0\n",
      "6330\n",
      "1.0\n",
      "6335\n",
      "1.0\n",
      "6340\n",
      "1.0\n",
      "6345\n",
      "1.0\n",
      "6350\n",
      "1.0\n",
      "6355\n",
      "1.0\n",
      "6360\n",
      "1.0\n",
      "6365\n",
      "1.0\n",
      "6370\n",
      "1.0\n",
      "6375\n",
      "1.0\n",
      "6380\n",
      "1.0\n",
      "6385\n",
      "1.0\n",
      "6390\n",
      "1.0\n",
      "6395\n",
      "1.0\n",
      "6400\n",
      "0.8\n",
      "6405\n",
      "1.0\n",
      "6410\n",
      "1.0\n",
      "6415\n",
      "1.0\n",
      "6420\n",
      "1.0\n",
      "6425\n",
      "1.0\n",
      "6430\n",
      "1.0\n",
      "6435\n",
      "1.0\n",
      "6440\n",
      "1.0\n",
      "6445\n",
      "1.0\n",
      "6450\n",
      "1.0\n",
      "6455\n",
      "1.0\n",
      "6460\n",
      "1.0\n",
      "6465\n",
      "1.0\n",
      "6470\n",
      "1.0\n",
      "6475\n",
      "1.0\n",
      "6480\n",
      "1.0\n",
      "6485\n",
      "0.6\n",
      "6490\n",
      "0.6\n",
      "6495\n",
      "1.0\n",
      "6500\n",
      "1.0\n",
      "6505\n",
      "0.8\n",
      "6510\n",
      "1.0\n",
      "6515\n",
      "1.0\n",
      "6520\n",
      "1.0\n",
      "6525\n",
      "1.0\n",
      "6530\n",
      "0.8\n",
      "6535\n",
      "0.6\n",
      "6540\n",
      "1.0\n",
      "6545\n",
      "1.0\n",
      "6550\n",
      "1.0\n",
      "6555\n",
      "1.0\n",
      "6560\n",
      "0.4\n",
      "6565\n",
      "1.0\n",
      "6570\n",
      "1.0\n",
      "6575\n",
      "1.0\n",
      "6580\n",
      "1.0\n",
      "6585\n",
      "1.0\n",
      "6590\n",
      "1.0\n",
      "6595\n",
      "1.0\n",
      "6600\n",
      "1.0\n",
      "6605\n",
      "0.8\n",
      "6610\n",
      "1.0\n",
      "6615\n",
      "1.0\n",
      "6620\n",
      "1.0\n",
      "6625\n",
      "1.0\n",
      "6630\n",
      "1.0\n",
      "6635\n",
      "1.0\n",
      "6640\n",
      "1.0\n",
      "6645\n",
      "0.8\n",
      "6650\n",
      "1.0\n",
      "6655\n",
      "1.0\n",
      "6660\n",
      "0.2\n",
      "6665\n",
      "0.4\n",
      "6670\n",
      "1.0\n",
      "6675\n",
      "0.2\n",
      "6680\n",
      "0.0\n",
      "6685\n",
      "0.8\n",
      "6690\n",
      "1.0\n",
      "6695\n",
      "0.2\n",
      "6700\n",
      "0.0\n",
      "6705\n",
      "0.2\n",
      "6710\n",
      "1.0\n",
      "6715\n",
      "1.0\n",
      "6720\n",
      "1.0\n",
      "6725\n",
      "1.0\n",
      "6730\n",
      "1.0\n",
      "6735\n",
      "1.0\n",
      "6740\n",
      "1.0\n",
      "6745\n",
      "1.0\n",
      "6750\n",
      "1.0\n",
      "6755\n",
      "1.0\n",
      "6760\n",
      "1.0\n",
      "6765\n",
      "1.0\n",
      "6770\n",
      "1.0\n",
      "6775\n",
      "1.0\n",
      "6780\n",
      "1.0\n",
      "6785\n",
      "1.0\n",
      "6790\n",
      "1.0\n",
      "6795\n",
      "1.0\n",
      "6800\n",
      "1.0\n",
      "6805\n",
      "1.0\n",
      "6810\n",
      "1.0\n",
      "6815\n",
      "1.0\n",
      "6820\n",
      "1.0\n",
      "6825\n",
      "0.0\n",
      "6830\n",
      "1.0\n",
      "6835\n",
      "1.0\n",
      "6840\n",
      "1.0\n",
      "6845\n",
      "1.0\n",
      "6850\n",
      "1.0\n",
      "6855\n",
      "1.0\n",
      "6860\n",
      "0.8\n",
      "6865\n",
      "0.0\n",
      "6870\n",
      "0.0\n",
      "6875\n",
      "1.0\n",
      "6880\n",
      "1.0\n",
      "6885\n",
      "0.8\n",
      "6890\n",
      "0.0\n",
      "6895\n",
      "0.0\n",
      "6900\n",
      "0.6\n",
      "6905\n",
      "0.6\n",
      "6910\n",
      "1.0\n",
      "6915\n",
      "1.0\n",
      "6920\n",
      "1.0\n",
      "6925\n",
      "1.0\n",
      "6930\n",
      "1.0\n",
      "6935\n",
      "1.0\n",
      "6940\n",
      "1.0\n",
      "6945\n",
      "1.0\n",
      "6950\n",
      "1.0\n",
      "6955\n",
      "1.0\n",
      "6960\n",
      "1.0\n",
      "6965\n",
      "1.0\n",
      "6970\n",
      "1.0\n",
      "6975\n",
      "1.0\n",
      "6980\n",
      "1.0\n",
      "6985\n",
      "1.0\n",
      "6990\n",
      "1.0\n",
      "6995\n",
      "1.0\n",
      "7000\n",
      "1.0\n",
      "7005\n",
      "1.0\n",
      "7010\n",
      "1.0\n",
      "7015\n",
      "1.0\n",
      "7020\n",
      "1.0\n",
      "7025\n",
      "1.0\n",
      "7030\n",
      "1.0\n",
      "7035\n",
      "1.0\n",
      "7040\n",
      "1.0\n",
      "7045\n",
      "1.0\n",
      "7050\n",
      "1.0\n",
      "7055\n",
      "1.0\n",
      "7060\n",
      "1.0\n",
      "7065\n",
      "1.0\n",
      "7070\n",
      "1.0\n",
      "7075\n",
      "1.0\n",
      "7080\n",
      "1.0\n",
      "7085\n",
      "1.0\n",
      "7090\n",
      "1.0\n",
      "7095\n",
      "1.0\n",
      "7100\n",
      "1.0\n",
      "7105\n",
      "1.0\n",
      "7110\n",
      "1.0\n",
      "7115\n",
      "1.0\n",
      "7120\n",
      "1.0\n",
      "7125\n",
      "1.0\n",
      "7130\n",
      "1.0\n",
      "7135\n",
      "1.0\n",
      "7140\n",
      "1.0\n",
      "7145\n",
      "1.0\n",
      "7150\n",
      "1.0\n",
      "7155\n",
      "1.0\n",
      "7160\n",
      "1.0\n",
      "7165\n",
      "1.0\n",
      "7170\n",
      "1.0\n",
      "7175\n",
      "1.0\n",
      "7180\n",
      "1.0\n",
      "7185\n",
      "1.0\n",
      "7190\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.79624739402362754, 0.3757963385485863)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_min_day = []\n",
    "latest_min = 60 * 30\n",
    "pred_sec = 5\n",
    "for i in range(0,9000-latest_min-pred_sec,pred_sec):\n",
    "    print i\n",
    "    data_train = data_2014_up[0][i:i+latest_min]\n",
    "    X_train = data_train.drop([\"0\"],axis=1)\n",
    "    y_train = data_train['0']\n",
    "    data_test = data_2014_up[0][i+latest_min:i+latest_min+pred_sec]\n",
    "    X_test = data_test.drop([\"0\"],axis=1)\n",
    "    y_test = data_test['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    acc = latest_day_loop_acc(X_train,y_train,X_test,y_test,model)\n",
    "    print acc\n",
    "    mean_min_day.append(acc)\n",
    "np.mean(mean_min_day),np.std(mean_min_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = 0.785257301808, std = 0.369045167619\n",
      "mean = 0.797913769124, std = 0.355538085912\n",
      "mean = 0.884005563282, std = 0.269719021309\n",
      "mean = 0.753407510431, std = 0.381799326153\n",
      "mean = 0.824339360223, std = 0.325768720054\n",
      "mean = 0.774269819193, std = 0.371093920339\n",
      "mean = 0.814325452017, std = 0.336600132645\n",
      "mean = 0.7719054242, std = 0.362253205172\n",
      "mean = 0.819610570236, std = 0.339994050017\n",
      "mean = 0.715020862309, std = 0.406583127565\n",
      "mean = 0.841307371349, std = 0.320038182421\n",
      "mean = 0.782336578581, std = 0.36319872863\n",
      "mean = 0.749374130737, std = 0.37645816336\n",
      "mean = 0.658970792768, std = 0.412933530579\n",
      "mean = 0.762169680111, std = 0.379757288895\n",
      "mean = 0.724895688456, std = 0.40052880063\n",
      "mean = 0.802086230876, std = 0.349023813809\n",
      "mean = 0.762447844228, std = 0.388939737394\n",
      "mean = 0.715159944367, std = 0.408880483516\n",
      "mean = 0.747287899861, std = 0.399325250441\n",
      "mean = 0.807232267038, std = 0.346173986121\n",
      "mean = 0.694158553547, std = 0.39152259028\n",
      "mean = 0.796662030598, std = 0.341460474631\n",
      "mean = 0.831293463143, std = 0.331653876162\n",
      "mean = 0.771210013908, std = 0.381881090523\n",
      "mean = 0.834492350487, std = 0.326324389806\n",
      "mean = 0.783171070932, std = 0.386212970928\n",
      "mean = 0.872044506259, std = 0.294048747001\n",
      "mean = 0.712656467316, std = 0.401587299397\n",
      "mean = 0.86314325452, std = 0.297119448978\n",
      "mean = 0.76495132128, std = 0.374902441267\n",
      "mean = 0.784005563282, std = 0.367084638874\n",
      "mean = 0.764673157163, std = 0.379851918647\n",
      "mean = 0.725173852573, std = 0.383444518236\n",
      "mean = 0.713212795549, std = 0.398335337309\n",
      "mean = 0.756745479833, std = 0.39567337442\n",
      "mean = 0.701808066759, std = 0.416435048395\n",
      "mean = 0.728789986092, std = 0.381261443501\n",
      "mean = 0.821418636996, std = 0.332249368014\n",
      "mean = 0.77454798331, std = 0.357522435945\n",
      "mean = 0.671627260083, std = 0.41538760843\n",
      "mean = 0.763699582754, std = 0.389040685828\n",
      "mean = 0.681502086231, std = 0.408777099788\n",
      "mean = 0.765229485396, std = 0.367850458548\n",
      "mean = 0.803059805285, std = 0.338394451033\n",
      "mean = 0.783449235049, std = 0.363748537407\n",
      "mean = 0.816411682893, std = 0.335034389163\n",
      "mean = 0.740055632823, std = 0.372575888182\n",
      "mean = 0.733240611961, std = 0.412724502694\n",
      "mean = 0.751460361613, std = 0.385545450402\n"
     ]
    }
   ],
   "source": [
    "latest_min = 60 * 30\n",
    "pred_sec = 10\n",
    "#traded_day = \n",
    "for day in range(0,50,1):\n",
    "    mean_min_day = []\n",
    "    for i in range(0,9000-latest_min-pred_sec,pred_sec):\n",
    "        #print i\n",
    "        data_train = data_2014_up[day][i:i+latest_min]\n",
    "        X_train = data_train.drop([\"0\"],axis=1)\n",
    "        y_train = data_train['0']\n",
    "        data_test = data_2014_up[day][i+latest_min:i+latest_min+pred_sec]\n",
    "        X_test = data_test.drop([\"0\"],axis=1)\n",
    "        y_test = data_test['0']\n",
    "        model = linear_model.LogisticRegression()\n",
    "        acc = latest_day_loop_acc(X_train,y_train,X_test,y_test,model)\n",
    "        #print acc\n",
    "        mean_min_day.append(acc)\n",
    "    print 'mean = %s, std = %s'%(np.mean(mean_min_day),np.std(mean_min_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.692666666667\n",
      "0.692027777778\n",
      "0.691\n",
      "0.676166666667\n",
      "0.672138888889\n",
      "0.663861111111\n",
      "0.653527777778\n",
      "0.635055555556\n",
      "0.649611111111\n",
      "0.634\n",
      "0.60675\n",
      "0.615222222222\n",
      "0.617611111111\n",
      "0.612861111111\n",
      "0.592166666667\n",
      "0.601361111111\n",
      "0.594555555556\n",
      "0.600444444444\n",
      "0.590194444444\n",
      "0.628555555556\n",
      "0.654916666667\n",
      "0.675194444444\n",
      "0.641166666667\n",
      "0.581194444444\n",
      "0.6245\n",
      "0.619083333333\n",
      "0.644361111111\n",
      "0.627222222222\n",
      "0.637333333333\n",
      "0.604111111111\n",
      "0.628472222222\n",
      "0.619472222222\n",
      "0.598333333333\n",
      "0.595694444444\n",
      "0.632055555556\n",
      "0.637416666667\n",
      "0.60325\n",
      "0.635083333333\n",
      "0.675611111111\n",
      "0.620861111111\n",
      "0.623805555556\n",
      "0.633916666667\n",
      "0.659472222222\n",
      "0.62925\n",
      "0.625\n",
      "0.609277777778\n",
      "0.612388888889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6312387706855791, 0.028371821573114121)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only rise ratio\n",
    "mean_four_day = []\n",
    "for i in range(1,len(day_trade[0])+len(day_trade[1])+len(day_trade[2])-3,1):\n",
    "    #print i\n",
    "    data_train = pd.concat([data_2014_up[i-1],data_2014_up[i],data_2014_up[i+1]\\\n",
    "                           ,data_2014_up[i+2]],axis = 0).reset_index(drop=True)\n",
    "    data_train = data_train#[['0','63']]\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_acc(Feature_data, Label, model, 4)\n",
    "    print CV_AUC\n",
    "    mean_four_day.append(CV_AUC)\n",
    "np.mean(mean_four_day),np.std(mean_four_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728148148148\n",
      "0.690740740741\n",
      "0.703740740741\n",
      "0.677185185185\n",
      "0.700037037037\n",
      "0.66137037037\n",
      "0.672\n",
      "0.661740740741\n",
      "0.665814814815\n",
      "0.646296296296\n",
      "0.628888888889\n",
      "0.633777777778\n",
      "0.608814814815\n",
      "0.631444444444\n",
      "0.632777777778\n",
      "0.612185185185\n",
      "0.618925925926\n",
      "0.582037037037\n",
      "0.605814814815\n",
      "0.586962962963\n",
      "0.668185185185\n",
      "0.697111111111\n",
      "0.672814814815\n",
      "0.615296296296\n",
      "0.589148148148\n",
      "0.626037037037\n",
      "0.663518518519\n",
      "0.657888888889\n",
      "0.648518518519\n",
      "0.631740740741\n",
      "0.636777777778\n",
      "0.660185185185\n",
      "0.630259259259\n",
      "0.617962962963\n",
      "0.615814814815\n",
      "0.632592592593\n",
      "0.659592592593\n",
      "0.621962962963\n",
      "0.656481481481\n",
      "0.668703703704\n",
      "0.639185185185\n",
      "0.643740740741\n",
      "0.65137037037\n",
      "0.663518518519\n",
      "0.655\n",
      "0.620777777778\n",
      "0.617148148148\n",
      "0.624962962963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.64447916666666671, 0.030933719915200403)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only rise ratio\n",
    "mean_three_day = []\n",
    "for i in range(1,len(day_trade[0])+len(day_trade[1])+len(day_trade[2])-2,1):\n",
    "    #print i\n",
    "    data_train = pd.concat([data_2014_up[i-1],data_2014_up[i],data_2014_up[i+1]],\\\n",
    "                           axis = 0).reset_index(drop=True)\n",
    "    data_train = data_train#[['0','6']]\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_acc(Feature_data, Label, model, 3)\n",
    "    print CV_AUC\n",
    "    mean_three_day.append(CV_AUC)\n",
    "np.mean(mean_three_day),np.std(mean_three_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.708777777778\n",
      "2\n",
      "0.747388888889\n",
      "3\n",
      "0.685944444444\n",
      "4\n",
      "0.691444444444\n",
      "5\n",
      "0.718722222222\n",
      "6\n",
      "0.693611111111\n",
      "7\n",
      "0.676944444444\n",
      "8\n",
      "0.700388888889\n",
      "9\n",
      "0.704277777778\n",
      "10\n",
      "0.675722222222\n",
      "11\n",
      "0.661722222222\n",
      "12\n",
      "0.666277777778\n",
      "13\n",
      "0.640555555556\n",
      "14\n",
      "0.647777777778\n",
      "15\n",
      "0.641222222222\n",
      "16\n",
      "0.675166666667\n",
      "17\n",
      "0.645277777778\n",
      "18\n",
      "0.622555555556\n",
      "19\n",
      "0.603333333333\n",
      "20\n",
      "0.626333333333\n",
      "21\n",
      "0.613166666667\n",
      "22\n",
      "0.747777777778\n",
      "23\n",
      "0.716055555556\n",
      "24\n",
      "0.672\n",
      "25\n",
      "0.5895\n",
      "26\n",
      "0.669888888889\n",
      "27\n",
      "0.695388888889\n",
      "28\n",
      "0.701722222222\n",
      "29\n",
      "0.703055555556\n",
      "30\n",
      "0.657388888889\n",
      "31\n",
      "0.628388888889\n",
      "32\n",
      "0.673888888889\n",
      "33\n",
      "0.670944444444\n",
      "34\n",
      "0.638722222222\n",
      "35\n",
      "0.631722222222\n",
      "36\n",
      "0.623666666667\n",
      "37\n",
      "0.648444444444\n",
      "38\n",
      "0.703333333333\n",
      "39\n",
      "0.642\n",
      "40\n",
      "0.663166666667\n",
      "41\n",
      "0.698111111111\n",
      "42\n",
      "0.670166666667\n",
      "43\n",
      "0.652222222222\n",
      "44\n",
      "0.699555555556\n",
      "45\n",
      "0.719666666667\n",
      "46\n",
      "0.651166666667\n",
      "47\n",
      "0.635944444444\n",
      "48\n",
      "0.635666666667\n",
      "49\n",
      "0.638944444444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.66785941043083918, 0.035640044708911564)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only rise ratio\n",
    "mean_two_day = []\n",
    "for i in range(1,len(day_trade[0])+len(day_trade[1])+len(day_trade[2])-1,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_up[i-1],data_2014_up[i]],axis = 0).reset_index(drop=True)\n",
    "    data_train = data_train\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_ACC = cv_loop_acc(Feature_data, Label, model, 2)\n",
    "    print CV_ACC\n",
    "    mean_two_day.append(CV_ACC)\n",
    "np.mean(mean_two_day),np.std(mean_two_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.571798974985\n",
      "0.577569020798\n",
      "0.586861808077\n",
      "0.611873297912\n",
      "0.614223545204\n",
      "0.637848497821\n",
      "0.631198134123\n",
      "0.587979301303\n",
      "0.592348415243\n",
      "0.577784307917\n",
      "0.578350013682\n",
      "0.595428310112\n",
      "0.577624147813\n",
      "0.572466343699\n",
      "0.571395033304\n",
      "0.555353892839\n",
      "0.555709243857\n",
      "0.536442703454\n",
      "0.536958107611\n",
      "0.543134282918\n",
      "0.55132561886\n",
      "0.573089862141\n",
      "0.556103822793\n",
      "0.468330162162\n",
      "0.453868584308\n",
      "0.465242814923\n",
      "0.543699114503\n",
      "0.580054912546\n",
      "0.594955187296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.56548336076564998, 0.042991347954555344)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only Depth\n",
    "mean_four_day = []\n",
    "for i in range(1,len(day_trade[0])+len(day_trade[1]+len(day_trade[2])-3,1):\n",
    "    #print i\n",
    "    data_train = pd.concat([data_2014_up[i-1],data_2014_up[i],data_2014_up[i+1]\\\n",
    "                           ,data_2014_up[i+2]],axis = 0).reset_index(drop=True)\n",
    "    data_train = data_train[['0','19']]\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_auc(Feature_data, Label, model, 4)\n",
    "    print CV_AUC\n",
    "    mean_four_day.append(CV_AUC)\n",
    "np.mean(mean_four_day),np.std(mean_four_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67662676151\n",
      "0.672647796252\n",
      "0.697763880051\n",
      "0.697559737842\n",
      "0.696048168644\n",
      "0.714019384577\n",
      "0.728658961634\n",
      "0.702018221394\n",
      "0.682405480155\n",
      "0.681555212583\n",
      "0.692427603355\n",
      "0.69545172047\n",
      "0.666122747076\n",
      "0.682898730968\n",
      "0.685810653739\n",
      "0.635254602904\n",
      "0.654802484114\n",
      "0.634649360295\n",
      "0.656290588752\n",
      "0.635270481531\n",
      "0.679934747308\n",
      "0.762707329983\n",
      "0.710908885134\n",
      "0.672490906954\n",
      "0.621213581257\n",
      "0.658965600004\n",
      "0.73187592294\n",
      "0.712129833339\n",
      "0.664679269567\n",
      "0.671117253994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.68247686361088311, 0.030796653504250093)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only rise ratio\n",
    "mean_four_day = []\n",
    "for i in range(1,len(day_trade[0])+len(day_trade[1])-2,1):\n",
    "    #print i\n",
    "    data_train = pd.concat([data_2014_up[i-1],data_2014_up[i],data_2014_up[i+1]],axis = 0).reset_index(drop=True)\n",
    "    data_train = data_train#[['0','6']]\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_auc(Feature_data, Label, model, 3)\n",
    "    print CV_AUC\n",
    "    mean_four_day.append(CV_AUC)\n",
    "np.mean(mean_four_day),np.std(mean_four_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.662241639321\n",
      "0.604646278684\n",
      "0.612827493544\n",
      "0.63447771378\n",
      "0.614835591354\n",
      "0.677633135501\n",
      "0.675110734785\n",
      "0.665476021923\n",
      "0.625026540492\n",
      "0.553040840709\n",
      "0.604451971765\n",
      "0.636904323475\n",
      "0.597316907373\n",
      "0.590800332427\n",
      "0.608003674167\n",
      "0.666288240067\n",
      "0.595670152966\n",
      "0.535329117793\n",
      "0.585819272293\n",
      "0.531895576848\n",
      "0.58655373117\n",
      "0.667107274371\n",
      "0.626122500556\n",
      "0.630126455909\n",
      "0.513283952804\n",
      "0.54708770225\n",
      "0.576118437028\n",
      "0.609411987712\n",
      "0.592878496683\n",
      "0.612564297232\n",
      "0.645273950544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.60917175308151872, 0.042552156269468171)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only Depth\n",
    "mean_four_day = []\n",
    "for i in range(1,len(day_trade[0])+len(day_trade[1])-1,1):\n",
    "    #print i\n",
    "    data_train = pd.concat([data_2014_up[i-1],data_2014_up[i]],axis = 0).reset_index(drop=True)\n",
    "    data_train = data_train[['0','6','21']]\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_auc(Feature_data, Label, model, 4)\n",
    "    print CV_AUC\n",
    "    mean_four_day.append(CV_AUC)\n",
    "np.mean(mean_four_day),np.std(mean_four_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.661796149777\n",
      "2\n",
      "0.65896797605\n",
      "3\n",
      "0.656377116655\n",
      "4\n",
      "0.692617632446\n",
      "5\n",
      "0.669556707984\n",
      "6\n",
      "0.703189906266\n",
      "7\n",
      "0.680416709499\n",
      "8\n",
      "0.632925147269\n",
      "9\n",
      "0.644213735963\n",
      "10\n",
      "0.652471352805\n",
      "11\n",
      "0.649696372383\n",
      "12\n",
      "0.66341352857\n",
      "13\n",
      "0.656645992271\n",
      "14\n",
      "0.647053826417\n",
      "15\n",
      "0.609376970299\n",
      "16\n",
      "0.626373859826\n",
      "17\n",
      "0.609911759189\n",
      "18\n",
      "0.618564605115\n",
      "19\n",
      "0.617470814273\n",
      "20\n",
      "0.623081198204\n",
      "21\n",
      "0.645653434321\n",
      "22\n",
      "0.671750526776\n",
      "23\n",
      "0.638686942935\n",
      "24\n",
      "0.609493293946\n",
      "25\n",
      "0.620455762396\n",
      "26\n",
      "0.626034650069\n",
      "27\n",
      "0.66431447279\n",
      "28\n",
      "0.660343642736\n",
      "29\n",
      "0.659576439964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.64725622507565184, 0.024394034941287109)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_four_day = []\n",
    "for i in range(1,len(day_trade[0])+len(day_trade[1])-3,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_up[i-1],data_2014_up[i],data_2014_up[i+1]\\\n",
    "                           ,data_2014_up[i+2]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_auc(Feature_data, Label, model, 4)\n",
    "    print CV_AUC\n",
    "    mean_four_day.append(CV_AUC)\n",
    "np.mean(mean_four_day),np.std(mean_four_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.659518466662\n",
      "2\n",
      "0.661424982142\n",
      "3\n",
      "0.667215201821\n",
      "4\n",
      "0.685159318813\n",
      "5\n",
      "0.680634851736\n",
      "6\n",
      "0.704792795532\n",
      "7\n",
      "0.719143760494\n",
      "8\n",
      "0.684300850517\n",
      "9\n",
      "0.642323442403\n",
      "10\n",
      "0.651659866889\n",
      "11\n",
      "0.668353513135\n",
      "12\n",
      "0.679707509178\n",
      "13\n",
      "0.656806578231\n",
      "14\n",
      "0.667332648194\n",
      "15\n",
      "0.662285975904\n",
      "16\n",
      "0.62696746069\n",
      "17\n",
      "0.626314786522\n",
      "18\n",
      "0.61993030269\n",
      "19\n",
      "0.634715806918\n",
      "20\n",
      "0.615733583688\n",
      "21\n",
      "0.642997260475\n",
      "22\n",
      "0.71069228925\n",
      "23\n",
      "0.658887347901\n",
      "24\n",
      "0.649286809992\n",
      "25\n",
      "0.601635517738\n",
      "26\n",
      "0.627177207163\n",
      "27\n",
      "0.682167767694\n",
      "28\n",
      "0.694424394243\n",
      "29\n",
      "0.647513464161\n",
      "30\n",
      "0.664411205118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.65978383219646564, 0.02819964880437361)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_three_day = []\n",
    "for i in range(1,len(day_trade[0])+len(day_trade[1])-2,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_up[i-1],data_2014_up[i],data_2014_up[i+1]],\n",
    "                           axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_auc(Feature_data, Label, model, 3)\n",
    "    print CV_AUC\n",
    "    mean_three_day.append(CV_AUC)\n",
    "np.mean(mean_three_day),np.std(mean_three_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.683517942246\n",
      "2\n",
      "0.649930451676\n",
      "3\n",
      "0.680837249057\n",
      "4\n",
      "0.699396590947\n",
      "5\n",
      "0.666877703866\n",
      "6\n",
      "0.706550898333\n",
      "7\n",
      "0.723177674344\n",
      "8\n",
      "0.752817256103\n",
      "9\n",
      "0.708492928389\n",
      "10\n",
      "0.686999857662\n",
      "11\n",
      "0.669971584613\n",
      "12\n",
      "0.722716845074\n",
      "13\n",
      "0.677885441002\n",
      "14\n",
      "0.689941651373\n",
      "15\n",
      "0.668652996115\n",
      "16\n",
      "0.719660419611\n",
      "17\n",
      "0.664285331788\n",
      "18\n",
      "0.648851996961\n",
      "19\n",
      "0.633952792674\n",
      "20\n",
      "0.648657512141\n",
      "21\n",
      "0.641612964481\n",
      "22\n",
      "0.723105577174\n",
      "23\n",
      "0.718668400694\n",
      "24\n",
      "0.703365064608\n",
      "25\n",
      "0.638796318995\n",
      "26\n",
      "0.587225776423\n",
      "27\n",
      "0.684657668395\n",
      "28\n",
      "0.722869549913\n",
      "29\n",
      "0.700656713161\n",
      "30\n",
      "0.662512245568\n",
      "31\n",
      "0.667483943522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6823912692550661, 0.034312285040423419)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_two_day = []\n",
    "for i in range(1,len(day_trade[0])+len(day_trade[1])-1,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_up[i-1],data_2014_up[i]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_auc(Feature_data, Label, model, 3)\n",
    "    print CV_AUC\n",
    "    mean_two_day.append(CV_AUC)\n",
    "np.mean(mean_two_day),np.std(mean_two_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.642997260475\n",
      "2\n",
      "0.71069228925\n",
      "3\n",
      "0.658887347901\n",
      "4\n",
      "0.649286809992\n",
      "5\n",
      "0.601635517738\n",
      "6\n",
      "0.627177207163\n",
      "7\n",
      "0.682167767694\n",
      "8\n",
      "0.694424394243\n",
      "9\n",
      "0.647513464161\n",
      "10\n",
      "0.664411205118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.65791932637343109, 0.030441894518751095)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_three_day = []\n",
    "for i in range(1,len(day_trade_2)-2,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_2_up[i-1],data_2014_2_up[i],data_2014_2_up[i+1]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_auc(Feature_data, Label, model, 3)\n",
    "    print CV_AUC\n",
    "    mean_three_day.append(CV_AUC)\n",
    "np.mean(mean_three_day),np.std(mean_three_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.641612964481\n",
      "2\n",
      "0.723105577174\n",
      "3\n",
      "0.718668400694\n",
      "4\n",
      "0.703365064608\n",
      "5\n",
      "0.638796318995\n",
      "6\n",
      "0.587225776423\n",
      "7\n",
      "0.684657668395\n",
      "8\n",
      "0.722869549913\n",
      "9\n",
      "0.700656713161\n",
      "10\n",
      "0.662512245568\n",
      "11\n",
      "0.667483943522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.67735947481212133, 0.0408429316536204)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_two_day = []\n",
    "for i in range(1,len(day_trade_2)-1,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_2_up[i-1],data_2014_2_up[i]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_auc(Feature_data, Label, model, 3)\n",
    "    print CV_AUC\n",
    "    mean_two_day.append(CV_AUC)\n",
    "np.mean(mean_two_day),np.std(mean_two_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_two_day = []\n",
    "for i in range(1,len(day_trade_1)+len(day_trade_2)-1,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_2_up[i-1],data_2014_2_up[i]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_auc(Feature_data, Label, model, 3)\n",
    "    print CV_AUC\n",
    "    mean_two_day.append(CV_AUC)\n",
    "np.mean(mean_two_day),np.std(mean_two_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.6482665898\n",
      "2\n",
      "0.649234993474\n",
      "3\n",
      "0.657458923811\n",
      "4\n",
      "0.695657786666\n",
      "5\n",
      "0.669368564252\n",
      "6\n",
      "0.69700164744\n",
      "7\n",
      "0.683451704848\n",
      "8\n",
      "0.636732387742\n",
      "9\n",
      "0.64659538748\n",
      "10\n",
      "0.657269752604\n",
      "11\n",
      "0.651986483333\n",
      "12\n",
      "0.659152011559\n",
      "13\n",
      "0.649951651681\n",
      "14\n",
      "0.641743600791\n",
      "15\n",
      "0.609619273847\n",
      "16\n",
      "0.619050797725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.65453384731589892, 0.023053038869505375)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_four_day = []\n",
    "for i in range(1,17,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_1_up[i-1],data_2014_1_up[i],data_2014_1_up[i+1]\\\n",
    "                           ,data_2014_1_up[i+2]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_auc(Feature_data, Label, model, 4)\n",
    "    print CV_AUC\n",
    "    mean_four_day.append(CV_AUC)\n",
    "np.mean(mean_four_day),np.std(mean_four_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.638519378179\n",
      "2\n",
      "0.646801742363\n",
      "3\n",
      "0.665250707983\n",
      "4\n",
      "0.686643502692\n",
      "5\n",
      "0.6894535405\n",
      "6\n",
      "0.697338086252\n",
      "7\n",
      "0.714962276135\n",
      "8\n",
      "0.688061732046\n",
      "9\n",
      "0.649769203417\n",
      "10\n",
      "0.65872728623\n",
      "11\n",
      "0.677555790136\n",
      "12\n",
      "0.672687172158\n",
      "13\n",
      "0.656915708696\n",
      "14\n",
      "0.658690195167\n",
      "15\n",
      "0.652191110562\n",
      "16\n",
      "0.632771010633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.66789615269685354, 0.022128192068738301)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_three_day = []\n",
    "for i in range(1,17,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_1_up[i-1],data_2014_1_up[i],data_2014_1_up[i+1]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_auc(Feature_data, Label, model, 3)\n",
    "    print CV_AUC\n",
    "    mean_three_day.append(CV_AUC)\n",
    "np.mean(mean_three_day),np.std(mean_three_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.661421408394\n",
      "2\n",
      "0.627436084429\n",
      "3\n",
      "0.667353521554\n",
      "4\n",
      "0.703335048355\n",
      "5\n",
      "0.674535300572\n",
      "6\n",
      "0.705139482891\n",
      "7\n",
      "0.726671955058\n",
      "8\n",
      "0.746007567354\n",
      "9\n",
      "0.710333911879\n",
      "10\n",
      "0.68301892679\n",
      "11\n",
      "0.675363645467\n",
      "12\n",
      "0.730878772559\n",
      "13\n",
      "0.675839337698\n",
      "14\n",
      "0.688139193422\n",
      "15\n",
      "0.651607666172\n",
      "16\n",
      "0.708062203476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.68969650162931462, 0.030333494929333359)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_two_day = []\n",
    "for i in range(1,17,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_1_up[i-1],data_2014_1_up[i]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_AUC = cv_loop_auc(Feature_data, Label, model, 2)\n",
    "    print CV_AUC\n",
    "    mean_two_day.append(CV_AUC)\n",
    "np.mean(mean_two_day),np.std(mean_two_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.6995\n",
      "2\n",
      "0.754722222222\n",
      "3\n",
      "0.690722222222\n",
      "4\n",
      "0.673\n",
      "5\n",
      "0.706388888889\n",
      "6\n",
      "0.686222222222\n",
      "7\n",
      "0.663444444444\n",
      "8\n",
      "0.690111111111\n",
      "9\n",
      "0.671833333333\n",
      "10\n",
      "0.633444444444\n",
      "11\n",
      "0.629166666667\n",
      "12\n",
      "0.670388888889\n",
      "13\n",
      "0.6145\n",
      "14\n",
      "0.6435\n",
      "15\n",
      "0.610944444444\n",
      "16\n",
      "0.645222222222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.66769444444444448, 0.036509749458953508)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_two_day_acc = []\n",
    "for i in range(1,17,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_1_up[i-1],data_2014_1_up[i]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_ACC = cv_loop_acc(Feature_data, Label, model, 2)\n",
    "    print CV_ACC\n",
    "    mean_two_day_acc.append(CV_ACC)\n",
    "np.mean(mean_two_day_acc),np.std(mean_two_day_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.734185185185\n",
      "2\n",
      "0.69337037037\n",
      "3\n",
      "0.69762962963\n",
      "4\n",
      "0.668037037037\n",
      "5\n",
      "0.701814814815\n",
      "6\n",
      "0.658888888889\n",
      "7\n",
      "0.668074074074\n",
      "8\n",
      "0.644444444444\n",
      "9\n",
      "0.62637037037\n",
      "10\n",
      "0.617074074074\n",
      "11\n",
      "0.62637037037\n",
      "12\n",
      "0.623259259259\n",
      "13\n",
      "0.601185185185\n",
      "14\n",
      "0.61262962963\n",
      "15\n",
      "0.595222222222\n",
      "16\n",
      "0.599962962963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.64803240740740742, 0.040942852782018514)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_three_day_acc = []\n",
    "for i in range(1,17,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_1_up[i-1],data_2014_1_up[i],data_2014_1_up[i+1]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_ACC = cv_loop_acc(Feature_data, Label, model, 3)\n",
    "    print CV_ACC\n",
    "    mean_three_day_acc.append(CV_ACC)\n",
    "np.mean(mean_three_day_acc),np.std(mean_three_day_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.674351851852\n",
      "2\n",
      "0.655138888889\n",
      "3\n",
      "0.70875\n",
      "4\n",
      "0.726342592593\n",
      "5\n",
      "0.67837962963\n",
      "6\n",
      "0.671990740741\n",
      "7\n",
      "0.744953703704\n",
      "8\n",
      "0.75662037037\n",
      "9\n",
      "0.739074074074\n",
      "10\n",
      "0.668101851852\n",
      "11\n",
      "0.654166666667\n",
      "12\n",
      "0.722916666667\n",
      "13\n",
      "0.706759259259\n",
      "14\n",
      "0.760555555556\n",
      "15\n",
      "0.681064814815\n",
      "16\n",
      "0.755833333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.70656249999999998, 0.036733115367711823)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_two_day_acc_down = []\n",
    "for i in range(1,17,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_1_down[i-1],data_2014_1_down[i]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_ACC = cv_loop_acc(Feature_data, Label, model, 2)\n",
    "    print CV_ACC\n",
    "    mean_two_day_acc_down.append(CV_ACC)\n",
    "np.mean(mean_two_day_acc_down),np.std(mean_two_day_acc_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.691635802469\n",
      "2\n",
      "0.661913580247\n",
      "3\n",
      "0.676666666667\n",
      "4\n",
      "0.695555555556\n",
      "5\n",
      "0.660462962963\n",
      "6\n",
      "0.692561728395\n",
      "7\n",
      "0.722777777778\n",
      "8\n",
      "0.746913580247\n",
      "9\n",
      "0.67799382716\n",
      "10\n",
      "0.667901234568\n",
      "11\n",
      "0.658518518519\n",
      "12\n",
      "0.708055555556\n",
      "13\n",
      "0.73487654321\n",
      "14\n",
      "0.689660493827\n",
      "15\n",
      "0.72237654321\n",
      "16\n",
      "0.736327160494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.69651234567901232, 0.028142703237994723)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_three_day_acc_down = []\n",
    "for i in range(1,17,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_1_down[i-1],data_2014_1_down[i],\\\n",
    "                            data_2014_1_down[i+1]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_ACC = cv_loop_acc(Feature_data, Label, model, 3)\n",
    "    print CV_ACC\n",
    "    mean_three_day_acc_down.append(CV_ACC)\n",
    "np.mean(mean_three_day_acc_down),np.std(mean_three_day_acc_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.686759259259\n",
      "2\n",
      "0.656296296296\n",
      "3\n",
      "0.672384259259\n",
      "4\n",
      "0.676875\n",
      "5\n",
      "0.691018518519\n",
      "6\n",
      "0.693009259259\n",
      "7\n",
      "0.718935185185\n",
      "8\n",
      "0.686273148148\n",
      "9\n",
      "0.674884259259\n",
      "10\n",
      "0.662731481481\n",
      "11\n",
      "0.664398148148\n",
      "12\n",
      "0.723726851852\n",
      "13\n",
      "0.69099537037\n",
      "14\n",
      "0.729398148148\n",
      "15\n",
      "0.737453703704\n",
      "16\n",
      "0.766018518519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.69569733796296296, 0.029916322768312586)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_four_day_acc_down = []\n",
    "for i in range(1,17,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_1_down[i-1],data_2014_1_down[i],\\\n",
    "                            data_2014_1_down[i+1],data_2014_1_down[i+2]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_ACC = cv_loop_acc(Feature_data, Label, model, 4)\n",
    "    print CV_ACC\n",
    "    mean_four_day_acc_down.append(CV_ACC)\n",
    "np.mean(mean_four_day_acc_down),np.std(mean_four_day_acc_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.6755\n",
      "2\n",
      "0.652851851852\n",
      "3\n",
      "0.668296296296\n",
      "4\n",
      "0.693777777778\n",
      "5\n",
      "0.689111111111\n",
      "6\n",
      "0.689018518519\n",
      "7\n",
      "0.684018518519\n",
      "8\n",
      "0.686425925926\n",
      "9\n",
      "0.675055555556\n",
      "10\n",
      "0.68087037037\n",
      "11\n",
      "0.676796296296\n",
      "12\n",
      "0.694518518519\n",
      "13\n",
      "0.717611111111\n",
      "14\n",
      "0.739240740741\n",
      "15\n",
      "0.759018518519\n",
      "16\n",
      "0.729814814815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.69449537037037046, 0.027202676672366057)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_five_day_acc_down = []\n",
    "for i in range(1,17,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_1_down[i-1],data_2014_1_down[i],\\\n",
    "                            data_2014_1_down[i+1],data_2014_1_down[i+2],data_2014_1_down[i+3]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_ACC = cv_loop_acc(Feature_data, Label, model, 5)\n",
    "    print CV_ACC\n",
    "    mean_five_day_acc_down.append(CV_ACC)\n",
    "np.mean(mean_five_day_acc_down),np.std(mean_five_day_acc_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.670262345679\n",
      "2\n",
      "0.643904320988\n",
      "3\n",
      "0.687222222222\n",
      "4\n",
      "0.697546296296\n",
      "5\n",
      "0.694089506173\n",
      "6\n",
      "0.669151234568\n",
      "7\n",
      "0.681882716049\n",
      "8\n",
      "0.686111111111\n",
      "9\n",
      "0.67912037037\n",
      "10\n",
      "0.688441358025\n",
      "11\n",
      "0.665077160494\n",
      "12\n",
      "0.716311728395\n",
      "13\n",
      "0.725848765432\n",
      "14\n",
      "0.75737654321\n",
      "15\n",
      "0.728981481481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.69275514403292182, 0.028019201699110586)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_six_day_acc_down = []\n",
    "for i in range(1,16,1):\n",
    "    print i\n",
    "    data_train = pd.concat([data_2014_1_down[i-1],data_2014_1_down[i],\\\n",
    "                            data_2014_1_down[i+1],data_2014_1_down[i+2],\\\n",
    "                            data_2014_1_down[i+3],data_2014_1_down[i+4]],axis = 0).reset_index(drop=True)\n",
    "    Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "    Label = data_train['0']\n",
    "    model = linear_model.LogisticRegression()\n",
    "    CV_ACC = cv_loop_acc(Feature_data, Label, model, 6)\n",
    "    print CV_ACC\n",
    "    mean_six_day_acc_down.append(CV_ACC)\n",
    "np.mean(mean_six_day_acc_down),np.std(mean_six_day_acc_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latest_min = 60 * 30\n",
    "pred_sec = 5\n",
    "#traded_day = \n",
    "for day in range(0,50,1):\n",
    "    mean_min_day = []\n",
    "    for i in range(0,9000-latest_min-pred_sec,pred_sec):\n",
    "        #print i\n",
    "        data_train = data_2014_up[day][i:i+latest_min]\n",
    "        X_train = data_train.drop([\"0\"],axis=1)\n",
    "        y_train = data_train['0']\n",
    "        data_test = data_2014_up[day][i+latest_min:i+latest_min+pred_sec]\n",
    "        X_test = data_test.drop([\"0\"],axis=1)\n",
    "        y_test = data_test['0']\n",
    "        model = linear_model.LogisticRegression()\n",
    "        acc = latest_day_loop_acc(X_train,y_train,X_test,y_test,model)\n",
    "        #print acc\n",
    "        mean_min_day.append(acc)\n",
    "    print 'mean = %s, std = %s'%(np.mean(mean_min_day),np.std(mean_min_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import (GridSearchCV, RandomizedSearchCV)\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------Rolling Window Time = 0.0----------------------------\n",
      "CPU times: user 39.2 s, sys: 22.5 ms, total: 39.2 s\n",
      "Wall time: 47.1 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.759444444444\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.3023897803509884), (28, 0.21202505004423394), (31, 0.079383284107762603), (46, 0.048711538152274594), (25, 0.048604700120157288), (30, 0.0459666050649334), (47, 0.036493037153471873), (24, 0.025646844609163881), (35, 0.022527237189719468), (34, 0.021025040228923424)]\n",
      "----------------------------Rolling Window Time = 1.0----------------------------\n",
      "CPU times: user 37.7 s, sys: 8.26 ms, total: 37.7 s\n",
      "Wall time: 38.8 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 20, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.74\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.3460936522088181), (28, 0.1112305336290895), (25, 0.0786906924578121), (31, 0.077023784921537164), (30, 0.057952125757462891), (47, 0.054584990319792859), (29, 0.039591535064891784), (24, 0.035699025174612015), (35, 0.022436765085183365), (5, 0.018342946368099515)]\n",
      "----------------------------Rolling Window Time = 2.0----------------------------\n",
      "CPU times: user 36.7 s, sys: 8.02 ms, total: 36.8 s\n",
      "Wall time: 38 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 15, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.747222222222\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.32756745944193538), (28, 0.15023751736159216), (31, 0.066852729530953642), (30, 0.064029408146522387), (29, 0.04738474833515275), (25, 0.044508510911839841), (46, 0.038830668769699851), (48, 0.028277044109281533), (34, 0.022627217506301436), (5, 0.021247919607673707)]\n",
      "----------------------------Rolling Window Time = 3.0----------------------------\n",
      "CPU times: user 37.2 s, sys: 8.27 ms, total: 37.2 s\n",
      "Wall time: 38 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 20, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.754444444444\n",
      "Accuracy = 0.8\n",
      "Top Ten Importance Features = [(26, 0.39762031416668214), (28, 0.11034513187596687), (30, 0.071815707831835149), (25, 0.056812465386414934), (31, 0.047590955535243326), (48, 0.039308141280699885), (35, 0.031265345204558502), (47, 0.02571372626929655), (29, 0.024818996441974439), (24, 0.021135053748849698)]\n",
      "----------------------------Rolling Window Time = 4.0----------------------------\n",
      "CPU times: user 37.5 s, sys: 7.83 ms, total: 37.5 s\n",
      "Wall time: 39.9 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 20, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.756666666667\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.37095280179301132), (28, 0.15192908280512565), (48, 0.0584486001784736), (25, 0.057180014237314135), (30, 0.052160371640952462), (31, 0.046702055202486485), (49, 0.028923701037449855), (47, 0.02237765951698064), (24, 0.020967384024563038), (35, 0.019242677530615607)]\n",
      "----------------------------Rolling Window Time = 5.0----------------------------\n",
      "CPU times: user 37.2 s, sys: 228 µs, total: 37.2 s\n",
      "Wall time: 37.8 s\n",
      "{'min_samples_leaf': 10, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.753888888889\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.36914252921523472), (30, 0.099457483756063197), (25, 0.095903014328862682), (28, 0.075027705284194376), (48, 0.074514957498925186), (49, 0.031053480399618104), (24, 0.030131999561080182), (2, 0.023099403615337709), (34, 0.021856070436395415), (5, 0.020561199219457059)]\n",
      "----------------------------Rolling Window Time = 6.0----------------------------\n",
      "CPU times: user 36.8 s, sys: 4.15 ms, total: 36.8 s\n",
      "Wall time: 37.4 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.715\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.46861625828870385), (28, 0.096061991474620717), (46, 0.084543838338144175), (49, 0.08383160022992231), (48, 0.045428036334886061), (35, 0.027878951603777104), (34, 0.025954927190911536), (33, 0.020810290494276858), (24, 0.017363220495249326), (8, 0.01545861938448021)]\n",
      "----------------------------Rolling Window Time = 7.0----------------------------\n",
      "CPU times: user 37.3 s, sys: 12 ms, total: 37.3 s\n",
      "Wall time: 38 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 20, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.686666666667\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.37194484414750006), (28, 0.10256102272156128), (25, 0.075083674923530352), (49, 0.067981362721416624), (48, 0.066754265716801778), (29, 0.032665099437457212), (47, 0.025966527137413865), (24, 0.023235692847699878), (35, 0.022295089745137504), (46, 0.02126980968011152)]\n",
      "----------------------------Rolling Window Time = 8.0----------------------------\n",
      "CPU times: user 38.6 s, sys: 8.15 ms, total: 38.6 s\n",
      "Wall time: 39.2 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.704444444444\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.27109536573988224), (25, 0.21750494399272569), (48, 0.12920636307379374), (49, 0.079425515532311766), (28, 0.07597922476253019), (34, 0.044084770927349561), (35, 0.027993993836929749), (8, 0.02558133231428859), (2, 0.020923231967592489), (5, 0.018192459139508656)]\n",
      "----------------------------Rolling Window Time = 9.0----------------------------\n",
      "CPU times: user 36.6 s, sys: 8.14 ms, total: 36.6 s\n",
      "Wall time: 37.1 s\n",
      "{'min_samples_leaf': 10, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.717777777778\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.46658467899653805), (49, 0.1422240544650612), (28, 0.098915340105764304), (35, 0.036724990859715326), (62, 0.033451836910872265), (2, 0.028481230722552868), (32, 0.022046749500499167), (24, 0.018992968865279364), (8, 0.017445854644932295), (5, 0.013078468404901377)]\n",
      "----------------------------Rolling Window Time = 10.0----------------------------\n",
      "CPU times: user 36.6 s, sys: 8.12 ms, total: 36.7 s\n",
      "Wall time: 37.2 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 15, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.713333333333\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.38544714457409884), (28, 0.096990653009436559), (25, 0.072117579833438056), (48, 0.065005221517719688), (49, 0.053586967838668273), (5, 0.036103890534544734), (27, 0.032065135654480457), (51, 0.022218224531161054), (24, 0.01965241578538017), (63, 0.019203152043165021)]\n",
      "----------------------------Rolling Window Time = 11.0----------------------------\n",
      "CPU times: user 36.2 s, sys: 132 µs, total: 36.2 s\n",
      "Wall time: 36.7 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 15, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.712222222222\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.39106050536828374), (28, 0.17613268239220994), (49, 0.056469050850652604), (48, 0.043406767660178681), (25, 0.038688393342709251), (34, 0.032166835590394086), (8, 0.026355830308734191), (2, 0.024025541802784656), (35, 0.021135315073429108), (62, 0.020119298004350885)]\n",
      "----------------------------Rolling Window Time = 12.0----------------------------\n",
      "CPU times: user 35.5 s, sys: 169 µs, total: 35.5 s\n",
      "Wall time: 36.1 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.721666666667\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.41875299921762144), (25, 0.11944559924432514), (49, 0.11781797904560561), (28, 0.075135427704040053), (32, 0.032520896055267756), (5, 0.031123881282340022), (50, 0.028296752617273734), (34, 0.01907277123345005), (24, 0.018175371044475204), (4, 0.016697382894773096)]\n",
      "----------------------------Rolling Window Time = 13.0----------------------------\n",
      "CPU times: user 35.3 s, sys: 4.04 ms, total: 35.3 s\n",
      "Wall time: 36 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.772222222222\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.32050973981918984), (28, 0.17687571474814739), (25, 0.1152632180782143), (48, 0.092262159643376029), (49, 0.039447117330225517), (5, 0.031909121789190656), (50, 0.029899079368154176), (35, 0.02068876391477719), (15, 0.016132037195879308), (24, 0.016129384060489207)]\n",
      "----------------------------Rolling Window Time = 14.0----------------------------\n",
      "CPU times: user 35.1 s, sys: 16 ms, total: 35.1 s\n",
      "Wall time: 35.6 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.742222222222\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.5032626101065828), (49, 0.086846608263882463), (28, 0.083851987846575743), (5, 0.066614864423705344), (48, 0.034670536137918401), (62, 0.024624948389611798), (24, 0.021404788524765704), (33, 0.018115934003864617), (1, 0.014313459641759941), (3, 0.013533568523369727)]\n",
      "----------------------------Rolling Window Time = 15.0----------------------------\n",
      "CPU times: user 35.4 s, sys: 12.1 ms, total: 35.4 s\n",
      "Wall time: 35.9 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.739444444444\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.49520889509688359), (28, 0.13488465949934764), (49, 0.050350921981008726), (5, 0.030687059398154026), (48, 0.026268474138174236), (24, 0.016943091445121992), (34, 0.016942302742779386), (51, 0.014350673284806834), (15, 0.01412660218546088), (50, 0.0135439060132868)]\n",
      "----------------------------Rolling Window Time = 16.0----------------------------\n",
      "CPU times: user 35.6 s, sys: 4.04 ms, total: 35.6 s\n",
      "Wall time: 36.1 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.756666666667\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.55536989965701367), (28, 0.071134069821442575), (5, 0.046687406961522279), (49, 0.042684253758687576), (48, 0.031225989824843016), (1, 0.026308280723973766), (47, 0.022992005085609613), (63, 0.019441406511183139), (3, 0.01823404265283806), (24, 0.015613252877902103)]\n",
      "----------------------------Rolling Window Time = 17.0----------------------------\n",
      "CPU times: user 35.9 s, sys: 8.17 ms, total: 35.9 s\n",
      "Wall time: 36.4 s\n",
      "{'min_samples_leaf': 10, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.731666666667\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.56593132716884287), (28, 0.10115828888543288), (49, 0.043490893881324974), (48, 0.026716539333765693), (35, 0.025572435665192284), (34, 0.023953185899991507), (46, 0.023451325966246876), (24, 0.021655557952961722), (47, 0.019819922963475296), (62, 0.017742811352323227)]\n",
      "----------------------------Rolling Window Time = 18.0----------------------------\n",
      "CPU times: user 35.7 s, sys: 4.02 ms, total: 35.7 s\n",
      "Wall time: 36.2 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.767222222222\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.57001616995998128), (28, 0.087266685932110927), (5, 0.043036161107778949), (49, 0.028592223355708025), (46, 0.027130352560043945), (24, 0.021633128663158037), (51, 0.019477717009330847), (35, 0.019467802184153955), (48, 0.018688285929757183), (47, 0.01827570085947218)]\n",
      "----------------------------Rolling Window Time = 19.0----------------------------\n",
      "CPU times: user 35.7 s, sys: 4.03 ms, total: 35.7 s\n",
      "Wall time: 36.2 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.765\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.33737335508442529), (28, 0.27844379594863938), (62, 0.041443321484491404), (5, 0.023113986635064046), (63, 0.021550602538054738), (3, 0.01998712496094145), (29, 0.019595811887377187), (48, 0.018721999306273895), (49, 0.018633382284295192), (60, 0.017824606150440892)]\n",
      "----------------------------Rolling Window Time = 20.0----------------------------\n",
      "CPU times: user 36.5 s, sys: 8.05 ms, total: 36.5 s\n",
      "Wall time: 37 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.778888888889\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.42964127731209445), (28, 0.13581993758790131), (25, 0.063334300448623634), (48, 0.049949450392585339), (46, 0.039793476030815109), (5, 0.036131494145702166), (24, 0.019979682031802979), (4, 0.017997604168138621), (50, 0.014365738401406161), (34, 0.014327396481855912)]\n",
      "----------------------------Rolling Window Time = 21.0----------------------------\n",
      "CPU times: user 35.2 s, sys: 20 ms, total: 35.2 s\n",
      "Wall time: 35.7 s\n",
      "{'min_samples_leaf': 10, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.783333333333\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.35056129111865031), (28, 0.17963864348378597), (25, 0.11159132552365254), (48, 0.030142724643163338), (29, 0.026075636983157825), (33, 0.025184450922086422), (35, 0.021517525305162731), (49, 0.021188082099619567), (51, 0.019879706576809476), (34, 0.019323896841810406)]\n",
      "----------------------------Rolling Window Time = 22.0----------------------------\n",
      "CPU times: user 36.7 s, sys: 4.05 ms, total: 36.7 s\n",
      "Wall time: 37.3 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.808333333333\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.23178670064902337), (25, 0.22871228384045533), (28, 0.16249953889782892), (5, 0.04887700666205682), (48, 0.029620148037074939), (24, 0.023804190485670899), (49, 0.016382533626972253), (51, 0.016075821122184284), (46, 0.014787351165973036), (47, 0.014216322063996933)]\n",
      "----------------------------Rolling Window Time = 23.0----------------------------\n",
      "CPU times: user 37.1 s, sys: 8.05 ms, total: 37.1 s\n",
      "Wall time: 37.7 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.81\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.5780993536195318), (28, 0.077125619877672885), (49, 0.062670265283582355), (5, 0.038805690493539088), (48, 0.0319446255758895), (24, 0.029715065161373639), (6, 0.023820268442108168), (57, 0.016966196121105954), (29, 0.012450770052306151), (43, 0.011118879476755212)]\n",
      "----------------------------Rolling Window Time = 24.0----------------------------\n",
      "CPU times: user 36.6 s, sys: 4.03 ms, total: 36.6 s\n",
      "Wall time: 37.1 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.798888888889\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.35031646098921876), (28, 0.2705191133728565), (49, 0.046035732268899038), (63, 0.038262467688964373), (29, 0.024034314314808264), (5, 0.023583797340691909), (48, 0.019943830900655615), (24, 0.018351308929775868), (46, 0.015591672307370349), (8, 0.015212269933792844)]\n",
      "----------------------------Rolling Window Time = 25.0----------------------------\n",
      "CPU times: user 36.6 s, sys: 8 ms, total: 36.6 s\n",
      "Wall time: 37.1 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 15, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.790555555556\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.46364678719560609), (28, 0.14028568751960935), (5, 0.059201947025046289), (25, 0.041628560359437822), (48, 0.034948429818350876), (24, 0.021026806894746387), (47, 0.017521045610877511), (49, 0.01555906147218199), (2, 0.013853522654948408), (51, 0.013207062603426111)]\n",
      "----------------------------Rolling Window Time = 26.0----------------------------\n",
      "CPU times: user 38.1 s, sys: 12 ms, total: 38.1 s\n",
      "Wall time: 38.7 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.818888888889\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.34199777283806243), (28, 0.16377035681281216), (25, 0.13740292721913083), (50, 0.046536510841900296), (5, 0.038635162145614563), (49, 0.031630191895313804), (63, 0.024194117336491495), (62, 0.022403826230309325), (24, 0.014243594526346895), (10, 0.013000498885445442)]\n",
      "----------------------------Rolling Window Time = 27.0----------------------------\n",
      "CPU times: user 38.8 s, sys: 3.98 ms, total: 38.8 s\n",
      "Wall time: 39.3 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.785\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.48498731528031264), (28, 0.18762571472000494), (63, 0.056311853622988448), (5, 0.045324366966015626), (48, 0.039023038326248649), (24, 0.019448090871162154), (47, 0.016375301508530959), (15, 0.014736069103710028), (2, 0.014383181658940264), (34, 0.013545526738188724)]\n",
      "----------------------------Rolling Window Time = 28.0----------------------------\n",
      "CPU times: user 39.1 s, sys: 4.03 ms, total: 39.1 s\n",
      "Wall time: 39.6 s\n",
      "{'min_samples_leaf': 10, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.810555555556\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.6113873744452174), (28, 0.088820675310542788), (35, 0.030696691082558354), (47, 0.022558963512102716), (34, 0.021888808072064347), (24, 0.017280442181460111), (15, 0.016054830447191821), (49, 0.016048143565073485), (48, 0.015686134547934313), (5, 0.015631790429640878)]\n",
      "----------------------------Rolling Window Time = 29.0----------------------------\n",
      "CPU times: user 39.3 s, sys: 42 µs, total: 39.3 s\n",
      "Wall time: 39.9 s\n",
      "{'min_samples_leaf': 10, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.786111111111\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.57582424422158029), (28, 0.15747194562694494), (35, 0.032164415448963382), (46, 0.020137550997789841), (48, 0.019690386485724333), (24, 0.018435220638651002), (50, 0.018231271570491307), (6, 0.015216358146177602), (63, 0.014577283165135988), (1, 0.013003623575292297)]\n",
      "----------------------------Rolling Window Time = 30.0----------------------------\n",
      "CPU times: user 35.8 s, sys: 4.07 ms, total: 35.8 s\n",
      "Wall time: 36.3 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.818333333333\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.50128988478605785), (25, 0.12545113650298961), (28, 0.094264374686814131), (35, 0.036172101481830823), (63, 0.034297254386667468), (51, 0.02916439158096925), (5, 0.022843912814356694), (62, 0.018245448648871305), (24, 0.017573393172322031), (29, 0.017257009520730001)]\n",
      "----------------------------Rolling Window Time = 31.0----------------------------\n",
      "CPU times: user 35.9 s, sys: 50 µs, total: 35.9 s\n",
      "Wall time: 36.5 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 20, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.814444444444\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.62004043106533147), (28, 0.14402431288717304), (25, 0.037207277289656747), (24, 0.029626559928345349), (3, 0.019967276605258606), (5, 0.017689731443651047), (34, 0.016582328974216777), (50, 0.01364514582462451), (31, 0.013271173506583033), (1, 0.011382890624794294)]\n",
      "----------------------------Rolling Window Time = 32.0----------------------------\n",
      "CPU times: user 36.1 s, sys: 4.14 ms, total: 36.1 s\n",
      "Wall time: 36.6 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.808333333333\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.6967140245610991), (28, 0.12347048433668484), (50, 0.02928547848801551), (34, 0.023111022438693792), (6, 0.020677152751155378), (24, 0.019766574738854369), (3, 0.017993308696159865), (1, 0.014873043186099513), (5, 0.010737639590670353), (47, 0.0075227161601961771)]\n",
      "----------------------------Rolling Window Time = 33.0----------------------------\n",
      "CPU times: user 35.8 s, sys: 86 µs, total: 35.8 s\n",
      "Wall time: 36.4 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.826666666667\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.657006319319973), (28, 0.10544278386106636), (5, 0.025148786758594298), (6, 0.023537328237284123), (35, 0.019856647577197555), (8, 0.016110034144631809), (1, 0.013955673779321137), (24, 0.012111417446289333), (31, 0.011879197230273249), (47, 0.01003985994497017)]\n",
      "----------------------------Rolling Window Time = 34.0----------------------------\n",
      "CPU times: user 31.7 s, sys: 4.03 ms, total: 31.7 s\n",
      "Wall time: 32.2 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.831666666667\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.58397553061547725), (28, 0.25199301185631828), (24, 0.023850560779346001), (31, 0.022633671499274893), (34, 0.02114553308827782), (29, 0.017810741385826903), (6, 0.014885498767970485), (4, 0.014334749088191734), (21, 0.00640877942941105), (30, 0.0063671584956407554)]\n",
      "----------------------------Rolling Window Time = 35.0----------------------------\n",
      "CPU times: user 31.5 s, sys: 7.98 ms, total: 31.5 s\n",
      "Wall time: 31.9 s\n",
      "{'min_samples_leaf': 10, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.844444444444\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.57505255005285194), (27, 0.14006789640000925), (28, 0.09126562258817017), (34, 0.033248428910204783), (5, 0.027838219451093143), (24, 0.022387049507020258), (3, 0.019859259426876656), (1, 0.015053509202021984), (35, 0.014539214768247179), (15, 0.013735622933143368)]\n",
      "----------------------------Rolling Window Time = 36.0----------------------------\n",
      "CPU times: user 31.7 s, sys: 28 µs, total: 31.7 s\n",
      "Wall time: 32.1 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.835555555556\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.74010905985522313), (28, 0.11171328996117739), (34, 0.025065329594361814), (5, 0.019446402398389549), (24, 0.018744355876248963), (15, 0.01374466531367884), (1, 0.013607732583900323), (6, 0.010267990288979491), (3, 0.0067144118803372926), (17, 0.0061167980217171347)]\n",
      "----------------------------Rolling Window Time = 37.0----------------------------\n",
      "CPU times: user 31.7 s, sys: 47 µs, total: 31.7 s\n",
      "Wall time: 32.1 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.835\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.59785589735693223), (28, 0.17298757039742538), (27, 0.071379598710326272), (24, 0.024553789639099804), (5, 0.017726447431538554), (15, 0.015680933461249331), (35, 0.013438125668975422), (31, 0.011476067421949027), (34, 0.011024381085221266), (3, 0.01079040787315046)]\n",
      "----------------------------Rolling Window Time = 38.0----------------------------\n",
      "CPU times: user 31.6 s, sys: 7.95 ms, total: 31.6 s\n",
      "Wall time: 32.1 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 20, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.833333333333\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.51576211335247457), (28, 0.15059331410462026), (25, 0.075160854544062156), (5, 0.051377693371016032), (27, 0.036795500783399479), (24, 0.026373419442522222), (34, 0.015233084428726049), (35, 0.01337610006248916), (2, 0.011405742851616154), (3, 0.011405680695667008)]\n",
      "----------------------------Rolling Window Time = 39.0----------------------------\n",
      "CPU times: user 31.3 s, sys: 4 ms, total: 31.3 s\n",
      "Wall time: 31.7 s\n",
      "{'min_samples_leaf': 10, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.840555555556\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.69924174751543122), (28, 0.12576305964724618), (34, 0.023663653975652628), (35, 0.021508882069839703), (1, 0.019975118286469584), (5, 0.015511288962605899), (7, 0.011802644620866842), (15, 0.011303858990428142), (2, 0.010409418773308069), (4, 0.010320041766393983)]\n",
      "----------------------------Rolling Window Time = 40.0----------------------------\n",
      "CPU times: user 31.4 s, sys: 3.99 ms, total: 31.4 s\n",
      "Wall time: 31.8 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 20, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.843333333333\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.59279683067608469), (28, 0.20186843214054639), (5, 0.038408134429820412), (24, 0.022484872409462221), (35, 0.016698479615159516), (1, 0.014073686537767546), (3, 0.013387969692379964), (34, 0.012396673044268355), (2, 0.011409579640857795), (15, 0.01052852033396056)]\n",
      "----------------------------Rolling Window Time = 41.0----------------------------\n",
      "CPU times: user 30.8 s, sys: 4.02 ms, total: 30.8 s\n",
      "Wall time: 31.3 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.85\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.70023913577675112), (28, 0.098846402968494881), (5, 0.035689095912150651), (6, 0.025233521561510319), (35, 0.02205271177188722), (1, 0.014557694783955166), (24, 0.014032207579556486), (34, 0.0088556769280229153), (3, 0.0082953191751090512), (17, 0.0082857559464204669)]\n",
      "----------------------------Rolling Window Time = 42.0----------------------------\n",
      "CPU times: user 31.7 s, sys: 8.01 ms, total: 31.7 s\n",
      "Wall time: 32.1 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 20, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.837222222222\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.5586235123922545), (28, 0.16067388909888275), (25, 0.073206392434661274), (5, 0.03709366983119515), (24, 0.025642415950392412), (1, 0.016585608674167435), (35, 0.013330852794414927), (4, 0.013144074470285302), (34, 0.012263024289917223), (6, 0.0096022958352950834)]\n",
      "----------------------------Rolling Window Time = 43.0----------------------------\n",
      "CPU times: user 30.9 s, sys: 7.96 ms, total: 30.9 s\n",
      "Wall time: 31.4 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.844444444444\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.61282224731324753), (28, 0.14860341589213361), (5, 0.035093963151795478), (35, 0.029081004076071094), (24, 0.019454353163861647), (15, 0.014128411848987724), (18, 0.012298467188242387), (6, 0.011118277600741586), (34, 0.010643546161312587), (17, 0.010009190643459584)]\n",
      "----------------------------Rolling Window Time = 44.0----------------------------\n",
      "CPU times: user 31.4 s, sys: 12 ms, total: 31.4 s\n",
      "Wall time: 31.8 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.856666666667\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.61485266112239567), (28, 0.09088348935545934), (25, 0.073232337977035267), (5, 0.056368511864351066), (34, 0.023098876791741146), (24, 0.014425536549335743), (6, 0.014294872609067703), (15, 0.012279148262057473), (1, 0.0099010851040997501), (2, 0.0091675364984588181)]\n",
      "----------------------------Rolling Window Time = 45.0----------------------------\n",
      "CPU times: user 31.6 s, sys: 29 µs, total: 31.6 s\n",
      "Wall time: 32.1 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 20, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.879444444444\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.48426857297540221), (28, 0.18661250244265387), (27, 0.068640558139747038), (25, 0.042536238669146562), (5, 0.03560146954515124), (35, 0.021841247408999835), (34, 0.015322183721272401), (24, 0.015314311184769534), (6, 0.014227841826937732), (29, 0.0086255616149833146)]\n",
      "----------------------------Rolling Window Time = 46.0----------------------------\n",
      "CPU times: user 32 s, sys: 4.04 ms, total: 32 s\n",
      "Wall time: 32.5 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.881111111111\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.4950077319112679), (28, 0.21701726860780574), (25, 0.074492660830755703), (35, 0.025546777540407371), (24, 0.022362084188399006), (5, 0.016812439821060621), (29, 0.01519802572206197), (15, 0.014904772443896286), (3, 0.013732838753475579), (34, 0.011426684512870957)]\n",
      "----------------------------Rolling Window Time = 47.0----------------------------\n",
      "CPU times: user 32.4 s, sys: 3.99 ms, total: 32.4 s\n",
      "Wall time: 32.9 s\n",
      "{'min_samples_leaf': 10, 'n_estimators': 15, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.871111111111\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.61524830093261651), (28, 0.10334690614562796), (27, 0.096789274419214519), (35, 0.026918707750502056), (24, 0.025485434646496399), (34, 0.019418853406630149), (3, 0.014685198469302082), (6, 0.013219450762397939), (1, 0.013164290001457867), (5, 0.0094370231848069606)]\n",
      "----------------------------Rolling Window Time = 48.0----------------------------\n",
      "CPU times: user 32.6 s, sys: 8.02 ms, total: 32.6 s\n",
      "Wall time: 33 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.883333333333\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.56480291116605874), (25, 0.13518256236897508), (28, 0.072041726454304775), (5, 0.05970358583925113), (24, 0.032660696363648919), (15, 0.014889160954131153), (6, 0.01477202715993318), (17, 0.012836270168794345), (2, 0.012134212190087867), (18, 0.011588543269164378)]\n",
      "----------------------------Rolling Window Time = 49.0----------------------------\n",
      "CPU times: user 32 s, sys: 12 ms, total: 32 s\n",
      "Wall time: 32.4 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.867777777778\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.58965850671882059), (28, 0.17266324938779817), (25, 0.072817830248074714), (24, 0.031709092799865446), (5, 0.016718439087348002), (34, 0.015893257808490269), (29, 0.01487845402004756), (3, 0.013463603785011796), (1, 0.013012672954496613), (15, 0.011665702839232187)]\n",
      "----------------------------Rolling Window Time = 50.0----------------------------\n",
      "CPU times: user 32 s, sys: 61 µs, total: 32 s\n",
      "Wall time: 32.4 s\n",
      "{'min_samples_leaf': 10, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.867777777778\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.64587243540285111), (28, 0.17711854268308297), (34, 0.037122819687136058), (24, 0.018680084329562884), (5, 0.018037781503286814), (15, 0.017177113334707818), (1, 0.014608071811331117), (35, 0.012118399380564523), (3, 0.0097767971212744612), (29, 0.0063894933582944067)]\n",
      "----------------------------Rolling Window Time = 51.0----------------------------\n",
      "CPU times: user 32.6 s, sys: 7.97 ms, total: 32.6 s\n",
      "Wall time: 33.1 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.866111111111\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.57330734419249341), (28, 0.13768085253820958), (25, 0.070470469513253892), (5, 0.056500289473391543), (24, 0.018911944794275513), (1, 0.013044852346738623), (6, 0.010474627554794908), (2, 0.010251761896447447), (34, 0.0093777076719543211), (4, 0.0088300834190935696)]\n",
      "----------------------------Rolling Window Time = 52.0----------------------------\n",
      "CPU times: user 32.4 s, sys: 28 µs, total: 32.4 s\n",
      "Wall time: 32.9 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.871666666667\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.62008312457342307), (28, 0.090877419432130763), (25, 0.074701587554507523), (5, 0.034133707786483634), (34, 0.022516857344327221), (24, 0.018819552126324659), (6, 0.016363508764295619), (15, 0.011322354158893946), (4, 0.008758631492191379), (17, 0.0085367024970058897)]\n",
      "----------------------------Rolling Window Time = 53.0----------------------------\n",
      "CPU times: user 32.1 s, sys: 0 ns, total: 32.1 s\n",
      "Wall time: 32.6 s\n",
      "{'min_samples_leaf': 10, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.873333333333\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.71243103100186045), (28, 0.1105825981949899), (34, 0.036323433820626576), (35, 0.01921557353137297), (24, 0.018730637859322601), (1, 0.017649213765231219), (5, 0.015110463093513715), (6, 0.014760797218810106), (3, 0.013924703760615529), (17, 0.013673381295072362)]\n",
      "----------------------------Rolling Window Time = 54.0----------------------------\n",
      "CPU times: user 31.8 s, sys: 0 ns, total: 31.8 s\n",
      "Wall time: 32.8 s\n",
      "{'min_samples_leaf': 5, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.871666666667\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.42084509031227507), (28, 0.19787874280407342), (25, 0.15210206215171312), (5, 0.03091791268227918), (29, 0.023677610169296343), (24, 0.020567889440332741), (2, 0.018743175738514539), (34, 0.016772453564617699), (30, 0.015343293404092096), (35, 0.01408269157372817)]\n",
      "----------------------------Rolling Window Time = 55.0----------------------------\n",
      "CPU times: user 32.3 s, sys: 4.02 ms, total: 32.3 s\n",
      "Wall time: 32.7 s\n",
      "{'min_samples_leaf': 10, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.878888888889\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(28, 0.37452368451863721), (26, 0.35882594548135321), (25, 0.084152172421094767), (24, 0.028189042878814026), (34, 0.019994174430029311), (5, 0.016788988128733605), (3, 0.016760846575229783), (15, 0.016188627529403678), (35, 0.012403463745370007), (10, 0.011806343900607261)]\n",
      "----------------------------Rolling Window Time = 56.0----------------------------\n",
      "CPU times: user 31.8 s, sys: 18 µs, total: 31.8 s\n",
      "Wall time: 32.3 s\n",
      "{'min_samples_leaf': 10, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 10}\n",
      "CV Best Score = 0.867222222222\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.69704822578267678), (28, 0.12529598824817659), (34, 0.035199515439239729), (24, 0.026101954919436517), (35, 0.021014364981370093), (3, 0.012489976007895058), (6, 0.01082776196979383), (1, 0.0090062389488773081), (5, 0.0081736056079762896), (4, 0.007023614434780107)]\n",
      "----------------------------Rolling Window Time = 57.0----------------------------\n",
      "CPU times: user 32 s, sys: 2 µs, total: 32 s\n",
      "Wall time: 32.4 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.866666666667\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(28, 0.47487939670595553), (26, 0.31946350353671549), (29, 0.056250808115997239), (24, 0.035104717905136383), (10, 0.018937638686800057), (3, 0.017563279979227152), (34, 0.016616504187235613), (5, 0.0097967134278677155), (31, 0.0091927886586437146), (23, 0.0087013737433192621)]\n",
      "----------------------------Rolling Window Time = 58.0----------------------------\n",
      "CPU times: user 32.1 s, sys: 11.9 ms, total: 32.1 s\n",
      "Wall time: 32.8 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 10, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.866666666667\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.65513985739576852), (28, 0.18998000211370186), (24, 0.033372749254508567), (3, 0.019523918527056743), (5, 0.017606956448138429), (1, 0.013556720896257615), (15, 0.010272171742521017), (35, 0.0098124496702704268), (31, 0.0091187412748816588), (34, 0.0080421841973813187)]\n",
      "----------------------------Rolling Window Time = 59.0----------------------------\n",
      "CPU times: user 31.8 s, sys: 4 ms, total: 31.8 s\n",
      "Wall time: 32.4 s\n",
      "{'min_samples_leaf': 15, 'n_estimators': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': None, 'max_depth': 5}\n",
      "CV Best Score = 0.879444444444\n",
      "Accuracy = 1.0\n",
      "Top Ten Importance Features = [(26, 0.59103664744814355), (28, 0.25988782680899075), (24, 0.015169257286916741), (15, 0.01486019908221772), (1, 0.014791335688434753), (3, 0.014115599927420095), (9, 0.012567705430028966), (5, 0.011428108870198369), (35, 0.0097280135634608082), (31, 0.0091745836583366873)]\n"
     ]
    }
   ],
   "source": [
    "latest_min = 60 * 30\n",
    "pred_sec = 5\n",
    "day = 0\n",
    "\n",
    "for i in range(0,300,pred_sec):#9000-latest_min-pred_sec,pred_sec):\n",
    "    \n",
    "    print '----------------------------Rolling Window Time = %s----------------------------'%(i/5.0)\n",
    "    data_train = data_2014_up[day][i:i+latest_min]\n",
    "    X_train = data_train.drop([\"0\"],axis=1)\n",
    "    y_train = data_train['0']\n",
    "    \n",
    "    data_test = data_2014_up[day][i+latest_min:i+latest_min+pred_sec]\n",
    "    X_test = data_test.drop([\"0\"],axis=1)\n",
    "    y_test = data_test['0']\n",
    "    \n",
    "    # hyper-parameter\n",
    "    num_trees = [5,10,15,20]\n",
    "    max_depth = [5,10,15]\n",
    "    criterion = ['entropy']\n",
    "    min_samples_leaf = [5,10,15]\n",
    "    min_samples_split = [2]\n",
    "    max_features = [None]\n",
    "    \n",
    "    # cv\n",
    "    model_grid = {'max_features':max_features,'n_estimators':num_trees,'max_depth':max_depth,\\\n",
    "                  'min_samples_split':min_samples_split,'criterion':criterion,\\\n",
    "                  'min_samples_leaf':min_samples_leaf}\n",
    "    model = RandomForestClassifier(random_state = 0)\n",
    "    Grid = GridSearchCV(model, model_grid, cv = 5)\n",
    "    %time Grid.fit(X_train,y_train) \n",
    "    print Grid.best_params_\n",
    "    print 'CV Best Score = %s'%(Grid.best_score_)\n",
    "    \n",
    "    # training & testing\n",
    "    model.set_params(**Grid.best_params_)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, predictions)\n",
    "    print 'Accuracy = %s'%(acc)\n",
    "    feature_imp = dict(zip([i for i in range(0,64,1)],model.feature_importances_))\n",
    "    print 'Top Ten Importance Features = %s'%(sorted(feature_imp.items(),\\\n",
    "                                                     key = lambda x : x[1] , reverse=True)[0:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "day = 0\n",
    "latest_min = 60 * 30\n",
    "pred_sec = 5\n",
    "\n",
    "data_train = data_2014_up[day][i:i+latest_min]\n",
    "X_train = data_train.drop([\"0\"],axis=1)\n",
    "y_train = data_train['0']\n",
    "data_test = data_2014_up[day][i+latest_min:i+latest_min+pred_sec]\n",
    "X_test = data_test.drop([\"0\"],axis=1)\n",
    "y_test = data_test['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feature_data = X_train\n",
    "Label = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_trees = [5,10,15,20,25]\n",
    "max_depth = [10,15,20.25]\n",
    "criterion = ['entropy']\n",
    "min_samples_leaf = [5,10,15]\n",
    "min_samples_split = [2,3]\n",
    "max_features = [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-50e2ba3cd4b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mGrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, scoring = 'acc')\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time Grid.fit(Feature_data, Label)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mGrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2165\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1173\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1174\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m         \"\"\"\n\u001b[1;32m--> 829\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    571\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 573\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m                 for train, test in cv)\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    606\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[0;32m   1663\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1664\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1665\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    324\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 326\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    606\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    737\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    740\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    348\u001b[0m                                            self.min_impurity_split)\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_grid = {'max_features':max_features,'n_estimators':num_trees,'max_depth':max_depth,\\\n",
    "              'min_samples_split':min_samples_split,'criterion':criterion,\\\n",
    "              'min_samples_leaf':min_samples_leaf}\n",
    "model1 = RandomForestClassifier(random_state = 0)\n",
    "Grid = GridSearchCV(model1, model_grid, cv = 10)#, scoring = 'acc')\n",
    "%time Grid.fit(Feature_data, Label) \n",
    "Grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 10,\n",
       " 'max_features': None,\n",
       " 'min_samples_leaf': 15,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 10}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=10, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=15,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.set_params(**Grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 216 ms, sys: 16 µs, total: 216 ms\n",
      "Wall time: 271 ms\n"
     ]
    }
   ],
   "source": [
    "y = y_train\n",
    "X = X_train\n",
    "%time model1.fit(X, y)\n",
    "predictions = model1.predict(X_test)\n",
    "#model1.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.,  1.,  1.,  1.]), 1800    1.0\n",
       " 1801    1.0\n",
       " 1802    1.0\n",
       " 1803    1.0\n",
       " 1804    1.0\n",
       " Name: 0, dtype: float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Estimator not fitted, call `fit` before `feature_importances_`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-9e4ed7105885>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeature_imp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_imp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bigdatas16/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfeature_importances_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    365\u001b[0m         \"\"\"\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m             raise NotFittedError(\"Estimator not fitted, \"\n\u001b[0m\u001b[0;32m    368\u001b[0m                                  \"call `fit` before `feature_importances_`.\")\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Estimator not fitted, call `fit` before `feature_importances_`."
     ]
    }
   ],
   "source": [
    "feature_imp = dict(zip([i for i in range(0,64,1)],model1.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-4901e61c132a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;34m' = %s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_imp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "print ' = %s'%(sorted(feature_imp.items(),key = lambda x : x[1] , reverse=True)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>preds</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "preds   1.0\n",
       "actual     \n",
       "1.0       5"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test,predictions,rownames = ['actual'],colnames=['preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.25 s, sys: 0 ns, total: 7.25 s\n",
      "Wall time: 7.85 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.75950, std: 0.12004, params: {'min_samples_leaf': 1, 'n_estimators': 10, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': None, 'max_depth': None},\n",
       " mean: 0.75244, std: 0.11471, params: {'min_samples_leaf': 1, 'n_estimators': 10, 'min_samples_split': 5, 'criterion': 'entropy', 'max_features': None, 'max_depth': None},\n",
       " mean: 0.74369, std: 0.16452, params: {'min_samples_leaf': 3, 'n_estimators': 10, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': None, 'max_depth': None},\n",
       " mean: 0.74369, std: 0.16452, params: {'min_samples_leaf': 3, 'n_estimators': 10, 'min_samples_split': 5, 'criterion': 'entropy', 'max_features': None, 'max_depth': None},\n",
       " mean: 0.75741, std: 0.15673, params: {'min_samples_leaf': 5, 'n_estimators': 10, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': None, 'max_depth': None},\n",
       " mean: 0.75741, std: 0.15673, params: {'min_samples_leaf': 5, 'n_estimators': 10, 'min_samples_split': 5, 'criterion': 'entropy', 'max_features': None, 'max_depth': None},\n",
       " mean: 0.77077, std: 0.15319, params: {'min_samples_leaf': 7, 'n_estimators': 10, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': None, 'max_depth': None},\n",
       " mean: 0.77077, std: 0.15319, params: {'min_samples_leaf': 7, 'n_estimators': 10, 'min_samples_split': 5, 'criterion': 'entropy', 'max_features': None, 'max_depth': None}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import (GridSearchCV, RandomizedSearchCV)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Feature_data = data_train.drop([\"0\"],axis=1)\n",
    "Label = data_train['0']\n",
    "\n",
    "num_trees = [10]\n",
    "max_depth = [None]\n",
    "criterion = ['entropy']\n",
    "min_samples_leaf = [1,3,5,7]\n",
    "min_samples_split = [3,5]\n",
    "max_features = [None]\n",
    "\n",
    "model_grid = {'max_features':max_features,'n_estimators':num_trees,'max_depth':max_depth,\\\n",
    "              'min_samples_split':min_samples_split,'criterion':criterion,\\\n",
    "              'min_samples_leaf':min_samples_leaf}\n",
    "\n",
    "model1 = RandomForestClassifier(random_state = 0)\n",
    "Grid = GridSearchCV(model1, model_grid, cv = 5, scoring = 'roc_auc')\n",
    "\n",
    "%time Grid.fit(Feature_data, Label) \n",
    "Grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': None,\n",
       " 'max_features': None,\n",
       " 'min_samples_leaf': 5,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 10}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
